<!DOCTYPE html>
<!-- saved from url=(0083)http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<link rel="dns-prefetch" href="http://apps.bdimg.com/">
<meta http-equiv="X-UA-Compatible" content="IE=11,IE=10,IE=9,IE=8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=0, minimum-scale=1.0, maximum-scale=1.0">
<meta name="apple-mobile-web-app-title" content="码农场">
<meta http-equiv="Cache-Control" content="no-siteapp">
<title>CS229编程5：正则化线性回归与偏差方差权衡-码农场</title>
<link rel="dns-prefetch" href="http://apps.bdimg.com/">
<link rel="dns-prefetch" href="http://s.w.org/">
		<script src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/f(7).txt"></script><script async="" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/analytics.js(1).下载"></script><script src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/ca-pub-1152644711996772.js.下载"></script><script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11\/svg\/","svgExt":".svg","source":{"concatemoji":"http:\/\/www.hankcs.com\/wp-includes\/js\/wp-emoji-release.min.js?ver=4.9.8"}};
			!function(a,b,c){function d(a,b){var c=String.fromCharCode;l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,a),0,0);var d=k.toDataURL();l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,b),0,0);var e=k.toDataURL();return d===e}function e(a){var b;if(!l||!l.fillText)return!1;switch(l.textBaseline="top",l.font="600 32px Arial",a){case"flag":return!(b=d([55356,56826,55356,56819],[55356,56826,8203,55356,56819]))&&(b=d([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]),!b);case"emoji":return b=d([55358,56760,9792,65039],[55358,56760,8203,9792,65039]),!b}return!1}function f(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var g,h,i,j,k=b.createElement("canvas"),l=k.getContext&&k.getContext("2d");for(j=Array("flag","emoji"),c.supports={everything:!0,everythingExceptFlag:!0},i=0;i<j.length;i++)c.supports[j[i]]=e(j[i]),c.supports.everything=c.supports.everything&&c.supports[j[i]],"flag"!==j[i]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[j[i]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(h=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",h,!1),a.addEventListener("load",h,!1)):(a.attachEvent("onload",h),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),g=c.source||{},g.concatemoji?f(g.concatemoji):g.wpemoji&&g.twemoji&&(f(g.twemoji),f(g.wpemoji)))}(window,document,window._wpemojiSettings);
		</script><script src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/wp-emoji-release.min.js.下载" type="text/javascript" defer=""></script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel="stylesheet" id="_bootstrap-css" href="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/bootstrap.min.css" type="text/css" media="all">
<link rel="stylesheet" id="_fontawesome-css" href="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/font-awesome.min.css" type="text/css" media="all">
<link rel="stylesheet" id="_main-css" href="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/main.css" type="text/css" media="all">
<script type="text/javascript" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/jquery.min.js.下载"></script>
<link rel="https://api.w.org/" href="http://www.hankcs.com/wp-json/">
<link rel="prev" title="CS229编程4：训练神经网络" href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html">
<link rel="next" title="CS229编程6：支持向量机" href="http://www.hankcs.com/ml/support-vector-machines-cs229.html">
<link rel="canonical" href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html">
<link rel="shortlink" href="http://www.hankcs.com/?p=8412">
<link rel="alternate" type="application/json+oembed" href="http://www.hankcs.com/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fwww.hankcs.com%2Fml%2Fregularized-linear-regression-and-bias-variance-cs229.html">
<link rel="alternate" type="text/xml+oembed" href="http://www.hankcs.com/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fwww.hankcs.com%2Fml%2Fregularized-linear-regression-and-bias-variance-cs229.html&amp;format=xml">
<meta name="keywords" content="CS229, matlab, 机器学习">
<meta name="description" content="斯坦福ML（Matlab）公开课，本次练习将实现正则化线性回归、多项式回归，并且在不同的参数下拟合数据、绘制学习曲线。正则化线性回归利用水库水位预测流量。可视化数据集数据集被拆分为3部分：训练集交叉验证集，用来决定正则化参数测试集可视化代码没什么稀奇的部分：%% =========== Part 1: Loading&amp;">
<link rel="icon" href="http://www.hankcs.com/wp-content/uploads/2017/04/cropped-Hankcs_512-32x32.png" sizes="32x32">
<link rel="icon" href="http://www.hankcs.com/wp-content/uploads/2017/04/cropped-Hankcs_512-192x192.png" sizes="192x192">
<link rel="apple-touch-icon-precomposed" href="http://www.hankcs.com/wp-content/uploads/2017/04/cropped-Hankcs_512-180x180.png">
<meta name="msapplication-TileImage" content="http://www.hankcs.com/wp-content/uploads/2017/04/cropped-Hankcs_512-270x270.png">
<link rel="shortcut icon" href="http://www.hankcs.com/favicon.ico">
<!--[if lt IE 9]><script src="http://www.hankcs.com/wp-content/themes/dux/js/libs/html5.min.js"></script><![endif]-->
<!--
	generated 15755 seconds ago
	generated in 0.260 seconds
	served from batcache in 0.003 seconds
	expires in 70645 seconds
-->
<link rel="preload" href="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/f(8).txt" as="script"><script type="text/javascript" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/f(8).txt"></script><script async="" data-requirecontext="_" data-requiremodule="main" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/main.js.下载"></script><script src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/share.js.下载"></script><script async="" data-requirecontext="_" data-requiremodule="lazyload" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/lazyload.min.js.下载"></script><script async="" data-requirecontext="_" data-requiremodule="prettyprint" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/prettyprint.js.下载"></script><script async="" data-requirecontext="_" data-requiremodule="signpop" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/signpop.js.下载"></script><script async="" data-requirecontext="_" data-requiremodule="comment" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/comment.js.下载"></script><link href="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/share.css" rel="styleSheet" type="text/css"></head>
<body class="post-template-default single single-post postid-8412 single-format-standard comment-open site-layout-2">
<header class="header">
	<div class="container">
		<div class="logo"><a href="http://www.hankcs.com/" title="码农场-自然语言处理、机器学习算法"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/logo.png">码农场</a></div>		<a href="http://www.hankcs.com/" title="码农场-自然语言处理、机器学习算法" class="brand">放牧代码和思想
<br>专注自然语言处理、机器学习算法</a>		<ul class="site-nav site-navbar">
			<li id="menu-item-1834" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1834"><a href="http://www.hankcs.com/program/cpp/">C++</a></li>
<li id="menu-item-1835" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1835"><a href="http://www.hankcs.com/program/java/">Java</a></li>
<li id="menu-item-5754" class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent menu-item-5754"><a href="http://www.hankcs.com/ml/">机器学习</a></li>
<li id="menu-item-2954" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-2954"><a href="http://www.hankcs.com/nlp/">NLP</a>
<ul class="sub-menu">
	<li id="menu-item-4344" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4344"><a href="http://www.hankcs.com/nlp/corpus/">语料库</a></li>
	<li id="menu-item-4342" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4342"><a href="http://www.hankcs.com/nlp/segment/">中文分词</a></li>
	<li id="menu-item-4343" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4343"><a href="http://www.hankcs.com/nlp/ner/">命名实体识别</a></li>
	<li id="menu-item-4479" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4479"><a href="http://www.hankcs.com/nlp/parsing/">句法分析</a></li>
</ul>
</li>
<li id="menu-item-1837" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1837"><a href="http://www.hankcs.com/program/algorithm/">算法</a></li>
<li id="menu-item-1839" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1839"><a href="http://www.hankcs.com/software/">软件</a></li>
<li id="menu-item-1838" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1838"><a href="http://www.hankcs.com/nihongonote/">日语</a>
<ul class="sub-menu">
	<li id="menu-item-1860" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1860"><a href="http://www.hankcs.com/nihongonote/riyurimen/">日语入门</a></li>
	<li id="menu-item-1861" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1861"><a href="http://www.hankcs.com/nihongonote/listening/">日语听力</a></li>
	<li id="menu-item-1863" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1863"><a href="http://www.hankcs.com/nihongonote/tekusuto/">日语综合教程</a>
	<ul class="sub-menu">
		<li id="menu-item-2190" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2190"><a href="http://www.hankcs.com/nihongonote/tekusuto/disance/">第三册</a></li>
		<li id="menu-item-2192" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2192"><a href="http://www.hankcs.com/nihongonote/tekusuto/daiyonnsatu/">第四册</a></li>
		<li id="menu-item-2191" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2191"><a href="http://www.hankcs.com/nihongonote/tekusuto/5/">第五册</a></li>
		<li id="menu-item-2702" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2702"><a href="http://www.hankcs.com/nihongonote/tekusuto/%e7%ac%ac%e5%85%ad%e5%86%8c/">第六册</a></li>
		<li id="menu-item-3604" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3604"><a href="http://www.hankcs.com/nihongonote/tekusuto/%e7%ac%ac%e4%b8%83%e5%86%8c/">第七册</a></li>
	</ul>
</li>
	<li id="menu-item-1859" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1859"><a href="http://www.hankcs.com/nihongonote/fd2/">新编日语阅读文选</a>
	<ul class="sub-menu">
		<li id="menu-item-2187" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2187"><a href="http://www.hankcs.com/nihongonote/fd2/c1/">第一册</a></li>
		<li id="menu-item-2189" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2189"><a href="http://www.hankcs.com/nihongonote/fd2/c2/">第二册</a></li>
		<li id="menu-item-2188" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2188"><a href="http://www.hankcs.com/nihongonote/fd2/c3/">第三册</a></li>
	</ul>
</li>
	<li id="menu-item-1858" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1858"><a href="http://www.hankcs.com/nihongonote/jpkaiwa/">日语商务贸易会话</a></li>
</ul>
</li>
<li id="menu-item-1843" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1843"><a href="http://www.hankcs.com/about/">关于</a></li>
							<li class="navto-search"><a href="javascript:;" class="search-show active"><i class="fa fa-search"></i></a></li>
					</ul>
		<div class="topbar">
			<ul class="site-nav topmenu">
				<li id="menu-item-5755" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-5755"><a href="http://www.hankcs.com/about/#comments"><i class="fa fa-comment"></i> 留言板</a></li>
				<li><a target="_blank" rel="external nofollow" href="https://github.com/hankcs"><i class="fa fa-github-alt"></i> GitHub</a></li>                                <li><a target="_blank" rel="external nofollow" href="http://weibo.com/hankcs"><i class="fa fa-weibo"></i> 微博</a></li>                                <li><a target="_blank" rel="external nofollow" href="https://twitter.com/hankcs"><i class="fa fa-twitter"></i> Twitter</a></li>                                <li><a target="_blank" href="http://blog.hankcs.com/"><i class="fa fa-language"></i> English</a></li>                <li><a target="_blank" href="http://www.hankcs.com/feed"><i class="fa fa-rss"></i> RSS</a></li>			</ul>
							&nbsp; &nbsp; <i class="fa fa-bullhorn url"></i> 码农场以笔记为主，如<a href="http://www.hankcs.com/tag/%e3%80%8a%e6%99%ba%e8%83%bdweb%e7%ae%97%e6%b3%95%e3%80%8b/">《智能Web算法》</a>、<a href="http://www.hankcs.com/tag/%e3%80%8a%e6%8c%91%e6%88%98%e7%a8%8b%e5%ba%8f%e8%ae%be%e8%ae%a1%e7%ab%9e%e8%b5%9b%e7%ac%ac2%e7%89%88%e3%80%8b/">《挑战程序设计竞赛(第2版)》</a>和<a href="http://www.hankcs.com/category/nihongonote/">日语笔记</a>，也发布一些<a href="http://www.hankcs.com/category/software/" target="_blank">原创应用</a>。					</div>
		<i class="fa fa-bars m-icon-nav"></i>
	</div>
</header>
<div class="site-search">
	<div class="container">
		<form method="get" class="site-search-form" action="http://www.hankcs.com/"><input class="search-input" name="s" type="text" placeholder="输入关键字" value=""><button class="search-btn" type="submit"><i class="fa fa-search"></i></button></form>	</div>
</div><section class="container">
	<div class="content-wrap">
	<div class="content">
				<header class="article-header">
			<h1 class="article-title"><a href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html">CS229编程5：正则化线性回归与偏差方差权衡</a></h1>
			<div class="article-meta">
				<span class="item">
					<a href="http://www.hankcs.com/">码农场</a> <small>&gt;</small> <a href="http://www.hankcs.com/ml/">机器学习</a><span class="muted"></span>				</span>
				<span class="item">2016-11-09</span>
																<span class="item post-views">阅读(1070)</span>				<span class="item"><a class="pc" href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html#respond">评论(0)</a></span>				<span class="item"></span>
			</div>
		</header>
		<article class="article-content">
			<div class="asb asb-post asb-post-01"><script async="" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/f(9).txt"></script>
<!-- 文章页 - 页面标题下 728 90 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-1152644711996772" data-ad-slot="5413029241" data-adsbygoogle-status="done"><ins id="aswift_0_expand" style="display:inline-table;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent;"><ins id="aswift_0_anchor" style="display:block;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent;"><iframe width="728" height="90" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_0" name="aswift_0" style="left:0;position:absolute;top:0;width:728px;height:90px;" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/saved_resource(2).html"></iframe></ins></ins></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>			<div class="post_nav" style="width: 0px;"><div class="post_nav_side" style="height: 100%;"><div class="post_nav_top"><p>目录</p></div><div class="post_nav_bottom"></div><span class="post_nav_close icon-remove" title="关闭目录" style="opacity: 0; display: none;"><i class="fa fa-times"></i></span></div><ul class="post_nav_content"><li class="h2_nav"><a href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html#h2-0">正则化线性回归</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html#h3-1">可视化数据集</a><i class="post_nav_dot"></i></li>
<li class="h3_nav active"><a href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html#h3-2">正则化损失函数和梯度</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html#h3-3">拟合线性回归</a><i class="post_nav_dot"></i></li>
<li class="h2_nav"><a href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html#h2-4">方差偏差权衡</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html#h3-5">学习曲线</a><i class="post_nav_dot"></i></li>
<li class="h2_nav"><a href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html#h2-6">多项式回归</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html#h3-7">特征映射</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html#h3-8">训练多项式回归</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html#h3-9">调整lambda</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html#h3-10">自动调参</a><i class="post_nav_dot"></i></li>
</ul></div><p style="text-indent: 2em;"><span style="text-indent: 32px;"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645gw1f9lqr67qlwj20v40ncgn7.jpg" title="validation.png" alt="validation.png" style="text-align: center; white-space: normal; width: 0px; height: 0px;" width="0" height="0" border="0" vspace="0" data-tag="bdshare">斯坦福ML（</span><span style="text-indent: 32px; text-decoration: line-through;">Matlab</span><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/i_f27.gif" style="text-indent: 32px; white-space: normal;" data-tag="bdshare"><span style="text-indent: 32px;">）公开课，</span>本次练习将实现正则化线性回归、多项式回归，并且在不同的参数下拟合数据、绘制学习曲线。</p>
<h2 id="h2-0"><span style="text-indent: 32px;">正则化线性回归</span></h2>
<p style="text-indent: 2em;">利用水库水位预测流量。</p>
<h3 id="h3-1">可视化数据集</h3>
<p style="text-indent: 2em;">数据集被拆分为3部分：</p>
<ul class=" list-paddingleft-2" style="list-style-type: disc;">
<li>
<p style="text-indent: 2em;">训练集</p>
</li>
<li>
<p style="text-indent: 2em;">交叉验证集，用来决定正则化参数</p>
</li>
<li>
<p style="text-indent: 2em;">测试集</p>
</li>
</ul>
<p style="text-indent: 2em;">可视化代码没什么稀奇的部分：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="pun">%%</span><span class="pln">&nbsp;</span><span class="pun">===========</span><span class="pln">&nbsp;</span><span class="typ">Part</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">Loading</span><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;</span><span class="typ">Visualizing</span><span class="pln">&nbsp;</span><span class="typ">Data</span><span class="pln">&nbsp;</span><span class="pun">=============</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="typ">We</span><span class="pln">&nbsp;start&nbsp;the&nbsp;exercise&nbsp;</span><span class="kwd">by</span><span class="pln">&nbsp;first&nbsp;loading&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;visualizing&nbsp;the&nbsp;dataset</span><span class="pun">.</span><span class="pln">&nbsp;</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="typ">The</span><span class="pln">&nbsp;following&nbsp;code&nbsp;will&nbsp;load&nbsp;the&nbsp;dataset&nbsp;</span><span class="kwd">into</span><span class="pln">&nbsp;your&nbsp;environment&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;plot</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;the&nbsp;data</span><span class="pun">.</span></li><li class="L4"><span class="pun">%</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Load</span><span class="pln">&nbsp;</span><span class="typ">Training</span><span class="pln">&nbsp;</span><span class="typ">Data</span></li><li class="L7"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'Loading&nbsp;and&nbsp;Visualizing&nbsp;Data&nbsp;...\n'</span><span class="pun">)</span></li><li class="L8"><span class="pln">&nbsp;</span></li><li class="L9"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Load</span><span class="pln">&nbsp;</span><span class="kwd">from</span><span class="pln">&nbsp;ex5data1</span><span class="pun">:</span><span class="pln">&nbsp;</span></li><li class="L0"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;will&nbsp;have&nbsp;X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="typ">Xval</span><span class="pun">,</span><span class="pln">&nbsp;yval</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="typ">Xtest</span><span class="pun">,</span><span class="pln">&nbsp;ytest&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;your&nbsp;environment</span></li><li class="L1"><span class="pln">load&nbsp;</span><span class="pun">(</span><span class="str">'ex5data1.mat'</span><span class="pun">);</span></li><li class="L2"><span class="pln">&nbsp;</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;m&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="typ">Number</span><span class="pln">&nbsp;of&nbsp;examples</span></li><li class="L4"><span class="pln">m&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;size</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">);</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Plot</span><span class="pln">&nbsp;training&nbsp;data</span></li><li class="L7"><span class="pln">plot</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'rx'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'MarkerSize'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">10</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'LineWidth'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1.5</span><span class="pun">);</span></li><li class="L8"><span class="pln">xlabel</span><span class="pun">(</span><span class="str">'Change&nbsp;in&nbsp;water&nbsp;level&nbsp;(x)'</span><span class="pun">);</span></li><li class="L9"><span class="pln">ylabel</span><span class="pun">(</span><span class="str">'Water&nbsp;flowing&nbsp;out&nbsp;of&nbsp;the&nbsp;dam&nbsp;(y)'</span><span class="pun">);</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'Program&nbsp;paused.&nbsp;Press&nbsp;enter&nbsp;to&nbsp;continue.\n'</span><span class="pun">);</span></li><li class="L2"><span class="pln">pause</span><span class="pun">;</span></li></ol></pre>
<p style="text-indent: 2em;">得到：</p>
<p style="text-indent: 2em;"></p>
<p style="text-align:center"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645gw1f9lohicul4j20v40nc75f.jpg" title="ex5data1.png" alt="ex5data1.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">这是非线性的数据，可以预计线性回归不会得到多好的效果。</p>
<h3 id="h3-2">正则化损失函数和梯度</h3>
<p style="text-indent: 2em;">损失函数：</p>
<p style="text-align:center"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645gw1f9lok93939j20o403sjs4.jpg" title="hankcs.com 2016-11-08 下午9.09.56.png" alt="hankcs.com 2016-11-08 下午9.09.56.png" width="357" height="56" border="0" vspace="0" style="width: 357px; height: 56px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">注意theta_0没有正则化项。</p>
<p style="text-indent: 2em;">梯度</p>
<p style="text-align:center"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645gw1f9loqe5r4rj20q207idhe.jpg" title="hankcs.com 2016-11-08 下午9.15.38.png" alt="hankcs.com 2016-11-08 下午9.15.38.png" width="417" height="120" border="0" vspace="0" style="width: 417px; height: 120px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">于是<span style="text-indent: 32px;">theta_0方向的梯度也是个例外。</span></p>
<p style="text-indent: 2em;">跟<a href="http://www.hankcs.com/ml/programming-exercise-2-logistic-regression-cs229.html" target="_blank">逻辑斯谛回归的正则化损失函数</a>很像，实现如下：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="kwd">function</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">J</span><span class="pun">,</span><span class="pln">&nbsp;grad</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;linearRegCostFunction</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;theta</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">)</span></li><li class="L1"><span class="pun">%</span><span class="pln">LINEARREGCOSTFUNCTION&nbsp;</span><span class="typ">Compute</span><span class="pln">&nbsp;cost&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;gradient&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;regularized&nbsp;linear&nbsp;</span></li><li class="L2"><span class="pun">%</span><span class="pln">regression&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;multiple&nbsp;variables</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="pun">[</span><span class="pln">J</span><span class="pun">,</span><span class="pln">&nbsp;grad</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;LINEARREGCOSTFUNCTION</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;theta</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">)</span><span class="pln">&nbsp;computes&nbsp;the&nbsp;</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;cost&nbsp;of&nbsp;</span><span class="kwd">using</span><span class="pln">&nbsp;theta&nbsp;</span><span class="kwd">as</span><span class="pln">&nbsp;the&nbsp;parameter&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;linear&nbsp;regression&nbsp;to&nbsp;fit&nbsp;the&nbsp;</span></li><li class="L5"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;data&nbsp;points&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;X&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;y</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">Returns</span><span class="pln">&nbsp;the&nbsp;cost&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;J&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;the&nbsp;gradient&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;grad</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Initialize</span><span class="pln">&nbsp;some&nbsp;useful&nbsp;values</span></li><li class="L8"><span class="pln">m&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;length</span><span class="pun">(</span><span class="pln">y</span><span class="pun">);</span><span class="pln">&nbsp;</span><span class="pun">%</span><span class="pln">&nbsp;number&nbsp;of&nbsp;training&nbsp;examples</span></li><li class="L9"><span class="pln">&nbsp;</span></li><li class="L0"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;need&nbsp;to&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;the&nbsp;following&nbsp;variables&nbsp;correctly&nbsp;</span></li><li class="L1"><span class="pln">J&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pun">;</span></li><li class="L2"><span class="pln">grad&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;zeros</span><span class="pun">(</span><span class="pln">size</span><span class="pun">(</span><span class="pln">theta</span><span class="pun">));</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="pun">======================</span><span class="pln">&nbsp;YOUR&nbsp;CODE&nbsp;HERE&nbsp;</span><span class="pun">======================</span></li><li class="L5"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Instructions</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">Compute</span><span class="pln">&nbsp;the&nbsp;cost&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;gradient&nbsp;of&nbsp;regularized&nbsp;linear&nbsp;</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;regression&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;a&nbsp;particular&nbsp;choice&nbsp;of&nbsp;theta</span><span class="pun">.</span></li><li class="L7"><span class="pun">%</span></li><li class="L8"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;should&nbsp;</span><span class="kwd">set</span><span class="pln">&nbsp;J&nbsp;to&nbsp;the&nbsp;cost&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;grad&nbsp;to&nbsp;the&nbsp;gradient</span><span class="pun">.</span></li><li class="L9"><span class="pun">%</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pln">Z&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">X&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;theta&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;y</span><span class="pun">).^</span><span class="lit">2</span><span class="pun">;</span></li><li class="L2"><span class="pln">J&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;sum</span><span class="pun">(</span><span class="pln">Z</span><span class="pun">(:,</span><span class="lit">1</span><span class="pun">))</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="lit">2</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;m</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">lambda</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="lit">2</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;m</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">theta</span><span class="str">'&nbsp;*&nbsp;theta&nbsp;-&nbsp;theta(1)^2);</span></li><li class="L4"><span class="str">&nbsp;</span></li><li class="L5"><span class="str">grad&nbsp;=&nbsp;1&nbsp;/&nbsp;m&nbsp;*&nbsp;X'</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">X&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;theta&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;y</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;m&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;theta</span><span class="pun">;</span></li><li class="L7"><span class="pln">grad</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;grad</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;m&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;theta</span><span class="pun">(</span><span class="lit">1</span><span class="pun">);</span></li><li class="L8"><span class="pln">&nbsp;</span></li><li class="L9"><span class="pln">&nbsp;</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pln">&nbsp;</span></li><li class="L2"><span class="pln">&nbsp;</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pln">&nbsp;</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="pun">=========================================================================</span></li><li class="L7"><span class="pln">&nbsp;</span></li><li class="L8"><span class="pln">grad&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;grad</span><span class="pun">(:);</span></li><li class="L9"><span class="pln">&nbsp;</span></li><li class="L0"><span class="kwd">end</span></li></ol></pre>
<h3 id="h3-3">拟合线性回归</h3>
<p style="text-indent: 2em;">在如此低维的空间中，正则化项其实作用不大。将lambda设为0，利用无约束非线性规划函数fmincg最优化损失函数即可。</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="kwd">function</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">theta</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;trainLinearReg</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">)</span></li><li class="L1"><span class="pun">%</span><span class="pln">TRAINLINEARREG&nbsp;</span><span class="typ">Trains</span><span class="pln">&nbsp;linear&nbsp;regression&nbsp;given&nbsp;a&nbsp;dataset&nbsp;</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;a</span></li><li class="L2"><span class="pun">%</span><span class="pln">regularization&nbsp;parameter&nbsp;</span><span class="kwd">lambda</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="pun">[</span><span class="pln">theta</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;TRAINLINEARREG&nbsp;</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">)</span><span class="pln">&nbsp;trains&nbsp;linear&nbsp;regression&nbsp;</span><span class="kwd">using</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;the&nbsp;dataset&nbsp;</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;regularization&nbsp;parameter&nbsp;</span><span class="kwd">lambda</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">Returns</span><span class="pln">&nbsp;the</span></li><li class="L5"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;trained&nbsp;parameters&nbsp;theta</span><span class="pun">.</span></li><li class="L6"><span class="pun">%</span></li><li class="L7"><span class="pln">&nbsp;</span></li><li class="L8"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Initialize</span><span class="pln">&nbsp;</span><span class="typ">Theta</span></li><li class="L9"><span class="pln">initial_theta&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;zeros</span><span class="pun">(</span><span class="pln">size</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">2</span><span class="pun">),</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">);</span><span class="pln">&nbsp;</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Create</span><span class="pln">&nbsp;</span><span class="str">"short&nbsp;hand"</span><span class="pln">&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;the&nbsp;cost&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;to&nbsp;be&nbsp;minimized</span></li><li class="L2"><span class="pln">costFunction&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">@(</span><span class="pln">t</span><span class="pun">)</span><span class="pln">&nbsp;linearRegCostFunction</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;t</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">);</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Now</span><span class="pun">,</span><span class="pln">&nbsp;costFunction&nbsp;</span><span class="kwd">is</span><span class="pln">&nbsp;a&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;that&nbsp;takes&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;only&nbsp;one&nbsp;argument</span></li><li class="L5"><span class="pln">options&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;optimset</span><span class="pun">(</span><span class="str">'MaxIter'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">200</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'GradObj'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'on'</span><span class="pun">);</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Minimize</span><span class="pln">&nbsp;</span><span class="kwd">using</span><span class="pln">&nbsp;fmincg</span></li><li class="L8"><span class="pln">theta&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;fmincg</span><span class="pun">(</span><span class="pln">costFunction</span><span class="pun">,</span><span class="pln">&nbsp;initial_theta</span><span class="pun">,</span><span class="pln">&nbsp;options</span><span class="pun">);</span></li><li class="L9"><span class="pln">&nbsp;</span></li><li class="L0"><span class="kwd">end</span></li></ol></pre>
<p style="text-indent: 2em;">上面创建了个costFunction函数句柄。调用方法：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="pun">%%</span><span class="pln">&nbsp;</span><span class="pun">===========</span><span class="pln">&nbsp;</span><span class="typ">Part</span><span class="pln">&nbsp;</span><span class="lit">4</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">Train</span><span class="pln">&nbsp;</span><span class="typ">Linear</span><span class="pln">&nbsp;</span><span class="typ">Regression</span><span class="pln">&nbsp;</span><span class="pun">=============</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="typ">Once</span><span class="pln">&nbsp;you&nbsp;have&nbsp;implemented&nbsp;the&nbsp;cost&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;gradient&nbsp;correctly</span><span class="pun">,</span><span class="pln">&nbsp;the</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;trainLinearReg&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;will&nbsp;</span><span class="kwd">use</span><span class="pln">&nbsp;your&nbsp;cost&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;to&nbsp;train&nbsp;</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;regularized&nbsp;linear&nbsp;regression</span><span class="pun">.</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;</span></li><li class="L5"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="typ">Write</span><span class="pln">&nbsp;</span><span class="typ">Up</span><span class="pln">&nbsp;</span><span class="typ">Note</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">The</span><span class="pln">&nbsp;data&nbsp;</span><span class="kwd">is</span><span class="pln">&nbsp;non</span><span class="pun">-</span><span class="pln">linear</span><span class="pun">,</span><span class="pln">&nbsp;so&nbsp;</span><span class="kwd">this</span><span class="pln">&nbsp;will&nbsp;</span><span class="kwd">not</span><span class="pln">&nbsp;give&nbsp;a&nbsp;great&nbsp;</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fit</span><span class="pun">.</span></li><li class="L7"><span class="pun">%</span></li><li class="L8"><span class="pln">&nbsp;</span></li><li class="L9"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="typ">Train</span><span class="pln">&nbsp;linear&nbsp;regression&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0</span></li><li class="L0"><span class="kwd">lambda</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pun">;</span></li><li class="L1"><span class="pun">[</span><span class="pln">theta</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;trainLinearReg</span><span class="pun">([</span><span class="pln">ones</span><span class="pun">(</span><span class="pln">m</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">)</span><span class="pln">&nbsp;X</span><span class="pun">],</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">);</span></li><li class="L2"><span class="pln">&nbsp;</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="typ">Plot</span><span class="pln">&nbsp;fit&nbsp;over&nbsp;the&nbsp;data</span></li><li class="L4"><span class="pln">plot</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'rx'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'MarkerSize'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">10</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'LineWidth'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1.5</span><span class="pun">);</span></li><li class="L5"><span class="pln">xlabel</span><span class="pun">(</span><span class="str">'Change&nbsp;in&nbsp;water&nbsp;level&nbsp;(x)'</span><span class="pun">);</span></li><li class="L6"><span class="pln">ylabel</span><span class="pun">(</span><span class="str">'Water&nbsp;flowing&nbsp;out&nbsp;of&nbsp;the&nbsp;dam&nbsp;(y)'</span><span class="pun">);</span></li><li class="L7"><span class="pln">hold&nbsp;on</span><span class="pun">;</span></li><li class="L8"><span class="pln">plot</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">ones</span><span class="pun">(</span><span class="pln">m</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">)</span><span class="pln">&nbsp;X</span><span class="pun">]*</span><span class="pln">theta</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'--'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'LineWidth'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">2</span><span class="pun">)</span></li><li class="L9"><span class="pln">hold&nbsp;off</span><span class="pun">;</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'Program&nbsp;paused.&nbsp;Press&nbsp;enter&nbsp;to&nbsp;continue.\n'</span><span class="pun">);</span></li><li class="L2"><span class="pln">pause</span><span class="pun">;</span></li></ol></pre>
<p style="text-indent: 2em;">得到：</p>
<p style="text-align:center"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645gw1f9lp0lnuynj20v40ncdhj.jpg" title="trainLinearReg.png" alt="trainLinearReg.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">的确拟合得不怎么样。</p>
<h2 id="h2-4">方差偏差权衡</h2>
<p style="text-indent: 2em;">机器学习中一个重要的概念就是方差偏差权衡，高偏差的模型称作欠拟合，高方差的模型则为过拟合。这一章通过训练误差和测试误差来诊断模型的方差偏差问题。</p>
<h3 id="h3-5">学习曲线<br></h3>
<p style="text-indent: 2em;">这部分通过不同大小的训练集训练模型，在训练集和交叉验证集得到两者的误差，并将其绘制成学习曲线。</p>
<p style="text-indent: 2em;">训练误差的定义为：</p>
<p style="text-align:center"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645gw1f9lpagfnfvj20ii03k0t4.jpg" title="hankcs.com 2016-11-08 下午9.35.04.png" alt="hankcs.com 2016-11-08 下午9.35.04.png" width="302" height="58" border="0" vspace="0" style="width: 302px; height: 58px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">注意训练误差并不是损失函数，前者并没有正则化项，实现的时候可以将损失函数的lambda设为0即可。</p>
<p style="text-indent: 2em;">实现如下：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="kwd">function</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">error_train</span><span class="pun">,</span><span class="pln">&nbsp;error_val</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;learningCurve</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="typ">Xval</span><span class="pun">,</span><span class="pln">&nbsp;yval</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">)</span></li><li class="L2"><span class="pun">%</span><span class="pln">LEARNINGCURVE&nbsp;</span><span class="typ">Generates</span><span class="pln">&nbsp;the&nbsp;train&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;cross&nbsp;validation&nbsp;</span><span class="kwd">set</span><span class="pln">&nbsp;errors&nbsp;needed&nbsp;</span></li><li class="L3"><span class="pun">%</span><span class="pln">to&nbsp;plot&nbsp;a&nbsp;learning&nbsp;curve</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="pun">[</span><span class="pln">error_train</span><span class="pun">,</span><span class="pln">&nbsp;error_val</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L5"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LEARNINGCURVE</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="typ">Xval</span><span class="pun">,</span><span class="pln">&nbsp;yval</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">)</span><span class="pln">&nbsp;returns&nbsp;the&nbsp;train&nbsp;</span><span class="kwd">and</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cross&nbsp;validation&nbsp;</span><span class="kwd">set</span><span class="pln">&nbsp;errors&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;a&nbsp;learning&nbsp;curve</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">In</span><span class="pln">&nbsp;particular</span><span class="pun">,</span><span class="pln">&nbsp;</span></li><li class="L7"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;it&nbsp;returns&nbsp;two&nbsp;vectors&nbsp;of&nbsp;the&nbsp;same&nbsp;length&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;error_train&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;</span></li><li class="L8"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;error_val</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">Then</span><span class="pun">,</span><span class="pln">&nbsp;error_train</span><span class="pun">(</span><span class="pln">i</span><span class="pun">)</span><span class="pln">&nbsp;contains&nbsp;the&nbsp;training&nbsp;error&nbsp;</span><span class="kwd">for</span></li><li class="L9"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i&nbsp;examples&nbsp;</span><span class="pun">(</span><span class="kwd">and</span><span class="pln">&nbsp;similarly&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;error_val</span><span class="pun">(</span><span class="pln">i</span><span class="pun">)).</span></li><li class="L0"><span class="pun">%</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="typ">In</span><span class="pln">&nbsp;</span><span class="kwd">this</span><span class="pln">&nbsp;</span><span class="kwd">function</span><span class="pun">,</span><span class="pln">&nbsp;you&nbsp;will&nbsp;compute&nbsp;the&nbsp;train&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;test&nbsp;errors&nbsp;</span><span class="kwd">for</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;dataset&nbsp;sizes&nbsp;</span><span class="kwd">from</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pln">&nbsp;up&nbsp;to&nbsp;m</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">In</span><span class="pln">&nbsp;practice</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">when</span><span class="pln">&nbsp;working&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;larger</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;datasets</span><span class="pun">,</span><span class="pln">&nbsp;you&nbsp;might&nbsp;want&nbsp;to&nbsp;</span><span class="kwd">do</span><span class="pln">&nbsp;</span><span class="kwd">this</span><span class="pln">&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;larger&nbsp;intervals</span><span class="pun">.</span></li><li class="L4"><span class="pun">%</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Number</span><span class="pln">&nbsp;of&nbsp;training&nbsp;examples</span></li><li class="L7"><span class="pln">m&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;size</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">);</span></li><li class="L8"><span class="pln">&nbsp;</span></li><li class="L9"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;need&nbsp;to&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;these&nbsp;values&nbsp;correctly</span></li><li class="L0"><span class="pln">error_train&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;zeros</span><span class="pun">(</span><span class="pln">m</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">);</span></li><li class="L1"><span class="pln">error_val&nbsp;&nbsp;&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;zeros</span><span class="pun">(</span><span class="pln">m</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">);</span></li><li class="L2"><span class="pln">&nbsp;</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="pun">======================</span><span class="pln">&nbsp;YOUR&nbsp;CODE&nbsp;HERE&nbsp;</span><span class="pun">======================</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Instructions</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">Fill</span><span class="pln">&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;</span><span class="kwd">this</span><span class="pln">&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;to&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;training&nbsp;errors&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;</span></li><li class="L5"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;error_train&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;the&nbsp;cross&nbsp;validation&nbsp;errors&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;error_val</span><span class="pun">.</span><span class="pln">&nbsp;</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i</span><span class="pun">.</span><span class="pln">e</span><span class="pun">.,</span><span class="pln">&nbsp;error_train</span><span class="pun">(</span><span class="pln">i</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;</span></li><li class="L7"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;error_val</span><span class="pun">(</span><span class="pln">i</span><span class="pun">)</span><span class="pln">&nbsp;should&nbsp;give&nbsp;you&nbsp;the&nbsp;errors</span></li><li class="L8"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;obtained&nbsp;after&nbsp;training&nbsp;on&nbsp;i&nbsp;examples</span><span class="pun">.</span></li><li class="L9"><span class="pun">%</span></li><li class="L0"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Note</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;should&nbsp;evaluate&nbsp;the&nbsp;training&nbsp;error&nbsp;on&nbsp;the&nbsp;first&nbsp;i&nbsp;training</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;examples&nbsp;</span><span class="pun">(</span><span class="pln">i</span><span class="pun">.</span><span class="pln">e</span><span class="pun">.,</span><span class="pln">&nbsp;X</span><span class="pun">(</span><span class="lit">1</span><span class="pun">:</span><span class="pln">i</span><span class="pun">,</span><span class="pln">&nbsp;</span><img draggable="false" class="emoji" alt="🙂" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/1f642.svg" data-tag="bdshare"><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;y</span><span class="pun">(</span><span class="lit">1</span><span class="pun">:</span><span class="pln">i</span><span class="pun">)).</span></li><li class="L2"><span class="pun">%</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="typ">For</span><span class="pln">&nbsp;the&nbsp;cross</span><span class="pun">-</span><span class="pln">validation&nbsp;error</span><span class="pun">,</span><span class="pln">&nbsp;you&nbsp;should&nbsp;instead&nbsp;evaluate&nbsp;on</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;_entire_&nbsp;cross&nbsp;validation&nbsp;</span><span class="kwd">set</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="typ">Xval</span><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;yval</span><span class="pun">).</span></li><li class="L5"><span class="pun">%</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Note</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">If</span><span class="pln">&nbsp;you&nbsp;are&nbsp;</span><span class="kwd">using</span><span class="pln">&nbsp;your&nbsp;cost&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">linearRegCostFunction</span><span class="pun">)</span></li><li class="L7"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;compute&nbsp;the&nbsp;training&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;cross&nbsp;validation&nbsp;error</span><span class="pun">,</span><span class="pln">&nbsp;you&nbsp;should&nbsp;</span></li><li class="L8"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;call&nbsp;the&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;the&nbsp;</span><span class="kwd">lambda</span><span class="pln">&nbsp;argument&nbsp;</span><span class="kwd">set</span><span class="pln">&nbsp;to&nbsp;</span><span class="lit">0.</span><span class="pln">&nbsp;</span></li><li class="L9"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="typ">Do</span><span class="pln">&nbsp;note&nbsp;that&nbsp;you&nbsp;will&nbsp;still&nbsp;need&nbsp;to&nbsp;</span><span class="kwd">use</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pln">&nbsp;</span><span class="kwd">when</span><span class="pln">&nbsp;running</span></li><li class="L0"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;training&nbsp;to&nbsp;obtain&nbsp;the&nbsp;theta&nbsp;parameters</span><span class="pun">.</span></li><li class="L1"><span class="pun">%</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Hint</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;can&nbsp;loop&nbsp;over&nbsp;the&nbsp;examples&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;the&nbsp;following</span><span class="pun">:</span></li><li class="L3"><span class="pun">%</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;i&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">:</span><span class="pln">m</span></li><li class="L5"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Compute</span><span class="pln">&nbsp;train</span><span class="pun">/</span><span class="pln">cross&nbsp;validation&nbsp;errors&nbsp;</span><span class="kwd">using</span><span class="pln">&nbsp;training&nbsp;examples&nbsp;</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">%</span><span class="pln">&nbsp;X</span><span class="pun">(</span><span class="lit">1</span><span class="pun">:</span><span class="pln">i</span><span class="pun">,</span><span class="pln">&nbsp;</span><img draggable="false" class="emoji" alt="🙂" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/1f642.svg" data-tag="bdshare"><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;y</span><span class="pun">(</span><span class="lit">1</span><span class="pun">:</span><span class="pln">i</span><span class="pun">),</span><span class="pln">&nbsp;storing&nbsp;the&nbsp;result&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;</span></li><li class="L7"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">%</span><span class="pln">&nbsp;error_train</span><span class="pun">(</span><span class="pln">i</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;error_val</span><span class="pun">(</span><span class="pln">i</span><span class="pun">)</span></li><li class="L8"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">....</span></li><li class="L9"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></li><li class="L0"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">end</span></li><li class="L1"><span class="pun">%</span></li><li class="L2"><span class="pln">&nbsp;</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="pun">----------------------</span><span class="pln">&nbsp;</span><span class="typ">Sample</span><span class="pln">&nbsp;</span><span class="typ">Solution</span><span class="pln">&nbsp;</span><span class="pun">----------------------</span></li><li class="L4"><span class="kwd">for</span><span class="pln">&nbsp;i&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">:</span><span class="pln">m</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">[</span><span class="pln">theta</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;trainLinearReg</span><span class="pun">(</span><span class="pln">X</span><span class="pun">(</span><span class="lit">1</span><span class="pun">:</span><span class="pln">i</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">:),</span><span class="pln">&nbsp;y</span><span class="pun">(</span><span class="lit">1</span><span class="pun">:</span><span class="pln">i</span><span class="pun">),</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">);</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">[</span><span class="pln">error_train</span><span class="pun">(</span><span class="pln">i</span><span class="pun">),</span><span class="pln">&nbsp;grad</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;linearRegCostFunction</span><span class="pun">(</span><span class="pln">X</span><span class="pun">(</span><span class="lit">1</span><span class="pun">:</span><span class="pln">i</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">:),</span><span class="pln">&nbsp;y</span><span class="pun">(</span><span class="lit">1</span><span class="pun">:</span><span class="pln">i</span><span class="pun">),</span><span class="pln">&nbsp;theta</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pun">);</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">[</span><span class="pln">error_val</span><span class="pun">(</span><span class="pln">i</span><span class="pun">),</span><span class="pln">&nbsp;grad</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;linearRegCostFunction</span><span class="pun">(</span><span class="typ">Xval</span><span class="pun">,</span><span class="pln">&nbsp;yval</span><span class="pun">,</span><span class="pln">&nbsp;theta</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pun">);</span></li><li class="L8"><span class="kwd">end</span></li><li class="L9"><span class="pln">&nbsp;</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pln">&nbsp;</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="pun">-------------------------------------------------------------</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="pun">=========================================================================</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="kwd">end</span></li></ol></pre>
<p style="text-indent: 2em;">很简单，调用方法如下：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="pun">%%</span><span class="pln">&nbsp;</span><span class="pun">===========</span><span class="pln">&nbsp;</span><span class="typ">Part</span><span class="pln">&nbsp;</span><span class="lit">5</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">Learning</span><span class="pln">&nbsp;</span><span class="typ">Curve</span><span class="pln">&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;</span><span class="typ">Linear</span><span class="pln">&nbsp;</span><span class="typ">Regression</span><span class="pln">&nbsp;</span><span class="pun">=============</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="typ">Next</span><span class="pun">,</span><span class="pln">&nbsp;you&nbsp;should&nbsp;implement&nbsp;the&nbsp;learningCurve&nbsp;</span><span class="kwd">function</span><span class="pun">.</span><span class="pln">&nbsp;</span></li><li class="L2"><span class="pun">%</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="typ">Write</span><span class="pln">&nbsp;</span><span class="typ">Up</span><span class="pln">&nbsp;</span><span class="typ">Note</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">Since</span><span class="pln">&nbsp;the&nbsp;model&nbsp;</span><span class="kwd">is</span><span class="pln">&nbsp;underfitting&nbsp;the&nbsp;data</span><span class="pun">,</span><span class="pln">&nbsp;we&nbsp;expect&nbsp;to</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;see&nbsp;a&nbsp;graph&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;</span><span class="str">"high&nbsp;bias"</span><span class="pln">&nbsp;</span><span class="pun">--</span><span class="pln">&nbsp;slide&nbsp;</span><span class="lit">8</span><span class="pln">&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;ML</span><span class="pun">-</span><span class="pln">advice</span><span class="pun">.</span><span class="pln">pdf&nbsp;</span></li><li class="L5"><span class="pun">%</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="kwd">lambda</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pun">;</span></li><li class="L8"><span class="pun">[</span><span class="pln">error_train</span><span class="pun">,</span><span class="pln">&nbsp;error_val</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;learningCurve</span><span class="pun">([</span><span class="pln">ones</span><span class="pun">(</span><span class="pln">m</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">)</span><span class="pln">&nbsp;X</span><span class="pun">],</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">[</span><span class="pln">ones</span><span class="pun">(</span><span class="pln">size</span><span class="pun">(</span><span class="typ">Xval</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">),</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="typ">Xval</span><span class="pun">],</span><span class="pln">&nbsp;yval</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">lambda</span><span class="pun">)</span></li><li class="L2"><span class="pln">&nbsp;</span></li><li class="L3"><span class="pln">plot</span><span class="pun">(</span><span class="lit">1</span><span class="pun">:</span><span class="pln">m</span><span class="pun">,</span><span class="pln">&nbsp;error_train</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">:</span><span class="pln">m</span><span class="pun">,</span><span class="pln">&nbsp;error_val</span><span class="pun">);</span></li><li class="L4"><span class="pln">title</span><span class="pun">(</span><span class="str">'Learning&nbsp;curve&nbsp;for&nbsp;linear&nbsp;regression'</span><span class="pun">)</span></li><li class="L5"><span class="pln">legend</span><span class="pun">(</span><span class="str">'Train'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'Cross&nbsp;Validation'</span><span class="pun">)</span></li><li class="L6"><span class="pln">xlabel</span><span class="pun">(</span><span class="str">'Number&nbsp;of&nbsp;training&nbsp;examples'</span><span class="pun">)</span></li><li class="L7"><span class="pln">ylabel</span><span class="pun">(</span><span class="str">'Error'</span><span class="pun">)</span></li><li class="L8"><span class="pln">axis</span><span class="pun">([</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">13</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">150</span><span class="pun">])</span></li><li class="L9"><span class="pln">&nbsp;</span></li><li class="L0"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'#&nbsp;Training&nbsp;Examples\tTrain&nbsp;Error\tCross&nbsp;Validation&nbsp;Error\n'</span><span class="pun">);</span></li><li class="L1"><span class="kwd">for</span><span class="pln">&nbsp;i&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">:</span><span class="pln">m</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;fprintf</span><span class="pun">(</span><span class="str">'&nbsp;&nbsp;\t%d\t\t%f\t%f\n'</span><span class="pun">,</span><span class="pln">&nbsp;i</span><span class="pun">,</span><span class="pln">&nbsp;error_train</span><span class="pun">(</span><span class="pln">i</span><span class="pun">),</span><span class="pln">&nbsp;error_val</span><span class="pun">(</span><span class="pln">i</span><span class="pun">));</span></li><li class="L3"><span class="kwd">end</span></li><li class="L4"><span class="pln">&nbsp;</span></li><li class="L5"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'Program&nbsp;paused.&nbsp;Press&nbsp;enter&nbsp;to&nbsp;continue.\n'</span><span class="pun">);</span></li><li class="L6"><span class="pln">pause</span><span class="pun">;</span></li></ol></pre>
<p style="text-indent: 2em;">得到</p>
<p style="text-align:center"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645gw1f9lpk5luxxj20v40nc75n.jpg" title="learningCurve.png" alt="learningCurve.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">可以看出，随着训练集的增大，训练误差增大，交叉验证误差下降不明显，两者都很大。这被称为高偏差，即线性回归模型太简单，不足以拟合如此复杂的数据。下一节通过多项式回归拟合一个更好的模型。</p>
<h2 id="h2-6">多项式回归</h2>
<p style="text-indent: 2em;">多项式回归的假设函数为：</p>
<p style="text-align:center"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645gw1f9lpo29sctj20y403kdgx.jpg" title="hankcs.com 2016-11-08 下午9.48.15.png" alt="hankcs.com 2016-11-08 下午9.48.15.png" width="557" height="58" border="0" vspace="0" style="width: 557px; height: 58px;" data-tag="bdshare"></p>
<h3 id="h3-7">特征映射</h3>
<p style="text-indent: 2em;">这种取n次方的方法其实跟逻辑斯谛回归模型中<a href="http://www.hankcs.com/ml/programming-exercise-2-logistic-regression-cs229.html#h3-8" target="_blank">mapfeature</a>是一个意思：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="kwd">function</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">X_poly</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;polyFeatures</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;p</span><span class="pun">)</span></li><li class="L1"><span class="pun">%</span><span class="pln">POLYFEATURES&nbsp;</span><span class="typ">Maps</span><span class="pln">&nbsp;X&nbsp;</span><span class="pun">(</span><span class="lit">1D</span><span class="pln">&nbsp;vector</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="kwd">into</span><span class="pln">&nbsp;the&nbsp;p</span><span class="pun">-</span><span class="pln">th&nbsp;power</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="pun">[</span><span class="pln">X_poly</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;POLYFEATURES</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;p</span><span class="pun">)</span><span class="pln">&nbsp;takes&nbsp;a&nbsp;data&nbsp;matrix&nbsp;X&nbsp;</span><span class="pun">(</span><span class="pln">size&nbsp;m&nbsp;x&nbsp;</span><span class="lit">1</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="kwd">and</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;maps&nbsp;each&nbsp;example&nbsp;</span><span class="kwd">into</span><span class="pln">&nbsp;its&nbsp;polynomial&nbsp;features&nbsp;</span><span class="kwd">where</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;X_poly</span><span class="pun">(</span><span class="pln">i</span><span class="pun">,</span><span class="pln">&nbsp;</span><img draggable="false" class="emoji" alt="🙂" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/1f642.svg" data-tag="bdshare"><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">X</span><span class="pun">(</span><span class="pln">i</span><span class="pun">)</span><span class="pln">&nbsp;X</span><span class="pun">(</span><span class="pln">i</span><span class="pun">).^</span><span class="lit">2</span><span class="pln">&nbsp;X</span><span class="pun">(</span><span class="pln">i</span><span class="pun">).^</span><span class="lit">3</span><span class="pln">&nbsp;</span><span class="pun">...</span><span class="pln">&nbsp;&nbsp;X</span><span class="pun">(</span><span class="pln">i</span><span class="pun">).^</span><span class="pln">p</span><span class="pun">];</span></li><li class="L5"><span class="pun">%</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="pln">&nbsp;</span></li><li class="L8"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;need&nbsp;to&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;the&nbsp;following&nbsp;variables&nbsp;correctly</span><span class="pun">.</span></li><li class="L9"><span class="pln">X_poly&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;zeros</span><span class="pun">(</span><span class="pln">numel</span><span class="pun">(</span><span class="pln">X</span><span class="pun">),</span><span class="pln">&nbsp;p</span><span class="pun">);</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="pun">======================</span><span class="pln">&nbsp;YOUR&nbsp;CODE&nbsp;HERE&nbsp;</span><span class="pun">======================</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Instructions</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">Given</span><span class="pln">&nbsp;a&nbsp;vector&nbsp;X</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;a&nbsp;matrix&nbsp;X_poly&nbsp;</span><span class="kwd">where</span><span class="pln">&nbsp;the&nbsp;p</span><span class="pun">-</span><span class="pln">th&nbsp;</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;column&nbsp;of&nbsp;X&nbsp;contains&nbsp;the&nbsp;values&nbsp;of&nbsp;X&nbsp;to&nbsp;the&nbsp;p</span><span class="pun">-</span><span class="pln">th&nbsp;power</span><span class="pun">.</span></li><li class="L4"><span class="pun">%</span></li><li class="L5"><span class="pun">%</span><span class="pln">&nbsp;</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="kwd">for</span><span class="pln">&nbsp;i&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">:</span><span class="pln">p</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;X_poly</span><span class="pun">(:,</span><span class="pln">i</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;X&nbsp;</span><span class="pun">.^</span><span class="pln">i</span><span class="pun">;</span></li><li class="L9"><span class="kwd">end</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pln">&nbsp;</span></li><li class="L2"><span class="pln">&nbsp;</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="pun">=========================================================================</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="kwd">end</span></li></ol></pre>
<p style="white-space: normal; text-indent: 2em;">由于特征映射，其数量级变得差异巨大，所以需要先进行标准化：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="kwd">function</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">X_norm</span><span class="pun">,</span><span class="pln">&nbsp;mu</span><span class="pun">,</span><span class="pln">&nbsp;sigma</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;featureNormalize</span><span class="pun">(</span><span class="pln">X</span><span class="pun">)</span></li><li class="L1"><span class="pun">%</span><span class="pln">FEATURENORMALIZE&nbsp;</span><span class="typ">Normalizes</span><span class="pln">&nbsp;the&nbsp;features&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;X&nbsp;</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;FEATURENORMALIZE</span><span class="pun">(</span><span class="pln">X</span><span class="pun">)</span><span class="pln">&nbsp;returns&nbsp;a&nbsp;normalized&nbsp;version&nbsp;of&nbsp;X&nbsp;</span><span class="kwd">where</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;the&nbsp;mean&nbsp;value&nbsp;of&nbsp;each&nbsp;feature&nbsp;</span><span class="kwd">is</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;the&nbsp;standard&nbsp;deviation</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="kwd">is</span><span class="pln">&nbsp;</span><span class="lit">1.</span><span class="pln">&nbsp;</span><span class="typ">This</span><span class="pln">&nbsp;</span><span class="kwd">is</span><span class="pln">&nbsp;often&nbsp;a&nbsp;good&nbsp;preprocessing&nbsp;step&nbsp;to&nbsp;</span><span class="kwd">do</span><span class="pln">&nbsp;</span><span class="kwd">when</span></li><li class="L5"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;working&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;learning&nbsp;algorithms</span><span class="pun">.</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="pln">mu&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;mean</span><span class="pun">(</span><span class="pln">X</span><span class="pun">);</span></li><li class="L8"><span class="pln">X_norm&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;bsxfun</span><span class="pun">(</span><span class="lit">@minus</span><span class="pun">,</span><span class="pln">&nbsp;X</span><span class="pun">,</span><span class="pln">&nbsp;mu</span><span class="pun">);</span></li><li class="L9"><span class="pln">&nbsp;</span></li><li class="L0"><span class="pln">sigma&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;std</span><span class="pun">(</span><span class="pln">X_norm</span><span class="pun">);</span></li><li class="L1"><span class="pln">X_norm&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;bsxfun</span><span class="pun">(</span><span class="lit">@rdivide</span><span class="pun">,</span><span class="pln">&nbsp;X_norm</span><span class="pun">,</span><span class="pln">&nbsp;sigma</span><span class="pun">);</span></li><li class="L2"><span class="pln">&nbsp;</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="pun">============================================================</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="kwd">end</span></li></ol></pre>
<p style="white-space: normal; text-indent: 2em;">bsxfun将两个数组作为参数调用某个指定的函数，上面将每个特征减去均值，除以标准差，于是整个数据集将标准化为均值=0，标准差=1。还记得在<a href="http://www.hankcs.com/ml/programming-exercise-1-linear-regression-cs229.html#h3-10" target="_blank">线性回归</a>中，我们写了个傻乎乎的循环：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="kwd">for</span><span class="pln">&nbsp;i&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="pun">:</span><span class="pln">&nbsp;features</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;mu</span><span class="pun">(</span><span class="pln">i</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;mean</span><span class="pun">(</span><span class="pln">X</span><span class="pun">(:,</span><span class="pln">i</span><span class="pun">));</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;sigma</span><span class="pun">(</span><span class="pln">i</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;std</span><span class="pun">(</span><span class="pln">X</span><span class="pun">(:,</span><span class="pln">i</span><span class="pun">));</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;X_norm</span><span class="pun">(:,</span><span class="pln">i</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">X</span><span class="pun">(:,</span><span class="pln">i</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;mu</span><span class="pun">(</span><span class="pln">i</span><span class="pun">))</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;sigma</span><span class="pun">(</span><span class="pln">i</span><span class="pun">);</span></li><li class="L4"><span class="kwd">end</span></li></ol></pre>
<p style="white-space: normal; text-indent: 2em;">现在两句话搞定。</p>
<p style="text-indent: 2em;">调用方法：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="pun">%%</span><span class="pln">&nbsp;</span><span class="pun">===========</span><span class="pln">&nbsp;</span><span class="typ">Part</span><span class="pln">&nbsp;</span><span class="lit">6</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">Feature</span><span class="pln">&nbsp;</span><span class="typ">Mapping</span><span class="pln">&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;</span><span class="typ">Polynomial</span><span class="pln">&nbsp;</span><span class="typ">Regression</span><span class="pln">&nbsp;</span><span class="pun">=============</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="typ">One</span><span class="pln">&nbsp;solution&nbsp;to&nbsp;</span><span class="kwd">this</span><span class="pln">&nbsp;</span><span class="kwd">is</span><span class="pln">&nbsp;to&nbsp;</span><span class="kwd">use</span><span class="pln">&nbsp;polynomial&nbsp;regression</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;should&nbsp;now</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;complete&nbsp;polyFeatures&nbsp;to&nbsp;map&nbsp;each&nbsp;example&nbsp;</span><span class="kwd">into</span><span class="pln">&nbsp;its&nbsp;powers</span></li><li class="L3"><span class="pun">%</span></li><li class="L4"><span class="pln">&nbsp;</span></li><li class="L5"><span class="pln">p&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">8</span><span class="pun">;</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Map</span><span class="pln">&nbsp;X&nbsp;onto&nbsp;</span><span class="typ">Polynomial</span><span class="pln">&nbsp;</span><span class="typ">Features</span><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;</span><span class="typ">Normalize</span></li><li class="L8"><span class="pln">X_poly&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;polyFeatures</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;p</span><span class="pun">);</span></li><li class="L9"><span class="pun">[</span><span class="pln">X_poly</span><span class="pun">,</span><span class="pln">&nbsp;mu</span><span class="pun">,</span><span class="pln">&nbsp;sigma</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;featureNormalize</span><span class="pun">(</span><span class="pln">X_poly</span><span class="pun">);</span><span class="pln">&nbsp;&nbsp;</span><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Normalize</span></li><li class="L0"><span class="pln">X_poly&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">ones</span><span class="pun">(</span><span class="pln">m</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">),</span><span class="pln">&nbsp;X_poly</span><span class="pun">];</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Add</span><span class="pln">&nbsp;</span><span class="typ">Ones</span></li><li class="L1"><span class="pln">&nbsp;</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Map</span><span class="pln">&nbsp;X_poly_test&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;normalize&nbsp;</span><span class="pun">(</span><span class="kwd">using</span><span class="pln">&nbsp;mu&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;sigma</span><span class="pun">)</span></li><li class="L3"><span class="pln">X_poly_test&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;polyFeatures</span><span class="pun">(</span><span class="typ">Xtest</span><span class="pun">,</span><span class="pln">&nbsp;p</span><span class="pun">);</span></li><li class="L4"><span class="pln">X_poly_test&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;bsxfun</span><span class="pun">(</span><span class="lit">@minus</span><span class="pun">,</span><span class="pln">&nbsp;X_poly_test</span><span class="pun">,</span><span class="pln">&nbsp;mu</span><span class="pun">);</span></li><li class="L5"><span class="pln">X_poly_test&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;bsxfun</span><span class="pun">(</span><span class="lit">@rdivide</span><span class="pun">,</span><span class="pln">&nbsp;X_poly_test</span><span class="pun">,</span><span class="pln">&nbsp;sigma</span><span class="pun">);</span></li><li class="L6"><span class="pln">X_poly_test&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">ones</span><span class="pun">(</span><span class="pln">size</span><span class="pun">(</span><span class="pln">X_poly_test</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">),</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">),</span><span class="pln">&nbsp;X_poly_test</span><span class="pun">];</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Add</span><span class="pln">&nbsp;</span><span class="typ">Ones</span></li><li class="L7"><span class="pln">&nbsp;</span></li><li class="L8"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Map</span><span class="pln">&nbsp;X_poly_val&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;normalize&nbsp;</span><span class="pun">(</span><span class="kwd">using</span><span class="pln">&nbsp;mu&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;sigma</span><span class="pun">)</span></li><li class="L9"><span class="pln">X_poly_val&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;polyFeatures</span><span class="pun">(</span><span class="typ">Xval</span><span class="pun">,</span><span class="pln">&nbsp;p</span><span class="pun">);</span></li><li class="L0"><span class="pln">X_poly_val&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;bsxfun</span><span class="pun">(</span><span class="lit">@minus</span><span class="pun">,</span><span class="pln">&nbsp;X_poly_val</span><span class="pun">,</span><span class="pln">&nbsp;mu</span><span class="pun">);</span></li><li class="L1"><span class="pln">X_poly_val&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;bsxfun</span><span class="pun">(</span><span class="lit">@rdivide</span><span class="pun">,</span><span class="pln">&nbsp;X_poly_val</span><span class="pun">,</span><span class="pln">&nbsp;sigma</span><span class="pun">);</span></li><li class="L2"><span class="pln">X_poly_val&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">ones</span><span class="pun">(</span><span class="pln">size</span><span class="pun">(</span><span class="pln">X_poly_val</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">),</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">),</span><span class="pln">&nbsp;X_poly_val</span><span class="pun">];</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Add</span><span class="pln">&nbsp;</span><span class="typ">Ones</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'Normalized&nbsp;Training&nbsp;Example&nbsp;1:\n'</span><span class="pun">);</span></li><li class="L5"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'&nbsp;&nbsp;%f&nbsp;&nbsp;\n'</span><span class="pun">,</span><span class="pln">&nbsp;X_poly</span><span class="pun">(</span><span class="lit">1</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">:));</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'\nProgram&nbsp;paused.&nbsp;Press&nbsp;enter&nbsp;to&nbsp;continue.\n'</span><span class="pun">);</span></li><li class="L8"><span class="pln">pause</span><span class="pun">;</span></li></ol></pre>
<h3 id="h3-8">训练多项式回归<br></h3>
<p style="text-indent: 2em;">所谓多项式回归只不过是将参数映射到高维而已，其核心依然是线性回归，训练代码一模一样。如果我们不加正则化项：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="pun">%%</span><span class="pln">&nbsp;</span><span class="pun">===========</span><span class="pln">&nbsp;</span><span class="typ">Part</span><span class="pln">&nbsp;</span><span class="lit">7</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">Learning</span><span class="pln">&nbsp;</span><span class="typ">Curve</span><span class="pln">&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;</span><span class="typ">Polynomial</span><span class="pln">&nbsp;</span><span class="typ">Regression</span><span class="pln">&nbsp;</span><span class="pun">=============</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="typ">Now</span><span class="pun">,</span><span class="pln">&nbsp;you&nbsp;will&nbsp;</span><span class="kwd">get</span><span class="pln">&nbsp;to&nbsp;experiment&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;polynomial&nbsp;regression&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;multiple</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;values&nbsp;of&nbsp;</span><span class="kwd">lambda</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">The</span><span class="pln">&nbsp;code&nbsp;below&nbsp;runs&nbsp;polynomial&nbsp;regression&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="kwd">lambda</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0.</span><span class="pln">&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;should&nbsp;</span><span class="kwd">try</span><span class="pln">&nbsp;running&nbsp;the&nbsp;code&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;different&nbsp;values&nbsp;of</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="kwd">lambda</span><span class="pln">&nbsp;to&nbsp;see&nbsp;how&nbsp;the&nbsp;fit&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;learning&nbsp;curve&nbsp;change</span><span class="pun">.</span></li><li class="L5"><span class="pun">%</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="kwd">lambda</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pun">;%</span><span class="pln">changed&nbsp;</span><span class="kwd">from</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pln">&nbsp;to&nbsp;</span><span class="lit">1</span><span class="pun">;</span></li><li class="L8"><span class="pun">[</span><span class="pln">theta</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;trainLinearReg</span><span class="pun">(</span><span class="pln">X_poly</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">);</span></li><li class="L9"><span class="pln">&nbsp;</span></li><li class="L0"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Plot</span><span class="pln">&nbsp;training&nbsp;data&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;fit</span></li><li class="L1"><span class="pln">figure</span><span class="pun">(</span><span class="lit">1</span><span class="pun">);</span></li><li class="L2"><span class="pln">plot</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'rx'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'MarkerSize'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">10</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'LineWidth'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1.5</span><span class="pun">);</span></li><li class="L3"><span class="pln">plotFit</span><span class="pun">(</span><span class="pln">min</span><span class="pun">(</span><span class="pln">X</span><span class="pun">),</span><span class="pln">&nbsp;max</span><span class="pun">(</span><span class="pln">X</span><span class="pun">),</span><span class="pln">&nbsp;mu</span><span class="pun">,</span><span class="pln">&nbsp;sigma</span><span class="pun">,</span><span class="pln">&nbsp;theta</span><span class="pun">,</span><span class="pln">&nbsp;p</span><span class="pun">);</span></li><li class="L4"><span class="pln">xlabel</span><span class="pun">(</span><span class="str">'Change&nbsp;in&nbsp;water&nbsp;level&nbsp;(x)'</span><span class="pun">);</span></li><li class="L5"><span class="pln">ylabel</span><span class="pun">(</span><span class="str">'Water&nbsp;flowing&nbsp;out&nbsp;of&nbsp;the&nbsp;dam&nbsp;(y)'</span><span class="pun">);</span></li><li class="L6"><span class="pln">title&nbsp;</span><span class="pun">(</span><span class="pln">sprintf</span><span class="pun">(</span><span class="str">'Polynomial&nbsp;Regression&nbsp;Fit&nbsp;(lambda&nbsp;=&nbsp;%f)'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">));</span></li><li class="L7"><span class="pln">&nbsp;</span></li><li class="L8"><span class="pln">figure</span><span class="pun">(</span><span class="lit">2</span><span class="pun">);</span></li><li class="L9"><span class="pun">[</span><span class="pln">error_train</span><span class="pun">,</span><span class="pln">&nbsp;error_val</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;learningCurve</span><span class="pun">(</span><span class="pln">X_poly</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;X_poly_val</span><span class="pun">,</span><span class="pln">&nbsp;yval</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">);</span></li><li class="L1"><span class="pln">plot</span><span class="pun">(</span><span class="lit">1</span><span class="pun">:</span><span class="pln">m</span><span class="pun">,</span><span class="pln">&nbsp;error_train</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">:</span><span class="pln">m</span><span class="pun">,</span><span class="pln">&nbsp;error_val</span><span class="pun">);</span></li><li class="L2"><span class="pln">&nbsp;</span></li><li class="L3"><span class="pln">title</span><span class="pun">(</span><span class="pln">sprintf</span><span class="pun">(</span><span class="str">'Polynomial&nbsp;Regression&nbsp;Learning&nbsp;Curve&nbsp;(lambda&nbsp;=&nbsp;%f)'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">));</span></li><li class="L4"><span class="pln">xlabel</span><span class="pun">(</span><span class="str">'Number&nbsp;of&nbsp;training&nbsp;examples'</span><span class="pun">)</span></li><li class="L5"><span class="pln">ylabel</span><span class="pun">(</span><span class="str">'Error'</span><span class="pun">)</span></li><li class="L6"><span class="pln">axis</span><span class="pun">([</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">13</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">100</span><span class="pun">])</span></li><li class="L7"><span class="pln">legend</span><span class="pun">(</span><span class="str">'Train'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'Cross&nbsp;Validation'</span><span class="pun">)</span></li><li class="L8"><span class="pln">&nbsp;</span></li><li class="L9"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'Polynomial&nbsp;Regression&nbsp;(lambda&nbsp;=&nbsp;%f)\n\n'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">);</span></li><li class="L0"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'#&nbsp;Training&nbsp;Examples\tTrain&nbsp;Error\tCross&nbsp;Validation&nbsp;Error\n'</span><span class="pun">);</span></li><li class="L1"><span class="kwd">for</span><span class="pln">&nbsp;i&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">:</span><span class="pln">m</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;fprintf</span><span class="pun">(</span><span class="str">'&nbsp;&nbsp;\t%d\t\t%f\t%f\n'</span><span class="pun">,</span><span class="pln">&nbsp;i</span><span class="pun">,</span><span class="pln">&nbsp;error_train</span><span class="pun">(</span><span class="pln">i</span><span class="pun">),</span><span class="pln">&nbsp;error_val</span><span class="pun">(</span><span class="pln">i</span><span class="pun">));</span></li><li class="L3"><span class="kwd">end</span></li><li class="L4"><span class="pln">&nbsp;</span></li><li class="L5"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'Program&nbsp;paused.&nbsp;Press&nbsp;enter&nbsp;to&nbsp;continue.\n'</span><span class="pun">);</span></li><li class="L6"><span class="pln">pause</span><span class="pun">;</span></li></ol></pre>
<p style="text-indent: 2em;">这里写了个可视化多项式回归的函数：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="kwd">function</span><span class="pln">&nbsp;plotFit</span><span class="pun">(</span><span class="pln">min_x</span><span class="pun">,</span><span class="pln">&nbsp;max_x</span><span class="pun">,</span><span class="pln">&nbsp;mu</span><span class="pun">,</span><span class="pln">&nbsp;sigma</span><span class="pun">,</span><span class="pln">&nbsp;theta</span><span class="pun">,</span><span class="pln">&nbsp;p</span><span class="pun">)</span></li><li class="L1"><span class="pun">%</span><span class="pln">PLOTFIT&nbsp;</span><span class="typ">Plots</span><span class="pln">&nbsp;a&nbsp;learned&nbsp;polynomial&nbsp;regression&nbsp;fit&nbsp;over&nbsp;an&nbsp;existing&nbsp;figure</span><span class="pun">.</span></li><li class="L2"><span class="pun">%</span><span class="typ">Also</span><span class="pln">&nbsp;works&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;linear&nbsp;regression</span><span class="pun">.</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;PLOTFIT</span><span class="pun">(</span><span class="pln">min_x</span><span class="pun">,</span><span class="pln">&nbsp;max_x</span><span class="pun">,</span><span class="pln">&nbsp;mu</span><span class="pun">,</span><span class="pln">&nbsp;sigma</span><span class="pun">,</span><span class="pln">&nbsp;theta</span><span class="pun">,</span><span class="pln">&nbsp;p</span><span class="pun">)</span><span class="pln">&nbsp;plots&nbsp;the&nbsp;learned&nbsp;polynomial</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;fit&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;power&nbsp;p&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;feature&nbsp;normalization&nbsp;</span><span class="pun">(</span><span class="pln">mu</span><span class="pun">,</span><span class="pln">&nbsp;sigma</span><span class="pun">).</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Hold</span><span class="pln">&nbsp;on&nbsp;to&nbsp;the&nbsp;current&nbsp;figure</span></li><li class="L7"><span class="pln">hold&nbsp;on</span><span class="pun">;</span></li><li class="L8"><span class="pln">&nbsp;</span></li><li class="L9"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">We</span><span class="pln">&nbsp;plot&nbsp;a&nbsp;range&nbsp;slightly&nbsp;bigger&nbsp;than&nbsp;the&nbsp;min&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;max&nbsp;values&nbsp;to&nbsp;</span><span class="kwd">get</span></li><li class="L0"><span class="pun">%</span><span class="pln">&nbsp;an&nbsp;idea&nbsp;of&nbsp;how&nbsp;the&nbsp;fit&nbsp;will&nbsp;vary&nbsp;outside&nbsp;the&nbsp;range&nbsp;of&nbsp;the&nbsp;data&nbsp;points</span></li><li class="L1"><span class="pln">x&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">min_x&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;</span><span class="lit">15</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="lit">0.05</span><span class="pln">&nbsp;</span><span class="pun">:</span><span class="pln">&nbsp;max_x&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="lit">25</span><span class="pun">)</span><span class="str">';</span></li><li class="L2"><span class="str">&nbsp;</span></li><li class="L3"><span class="str">%&nbsp;Map&nbsp;the&nbsp;X&nbsp;values&nbsp;</span></li><li class="L4"><span class="str">X_poly&nbsp;=&nbsp;polyFeatures(x,&nbsp;p);</span></li><li class="L5"><span class="str">X_poly&nbsp;=&nbsp;bsxfun(@minus,&nbsp;X_poly,&nbsp;mu);</span></li><li class="L6"><span class="str">X_poly&nbsp;=&nbsp;bsxfun(@rdivide,&nbsp;X_poly,&nbsp;sigma);</span></li><li class="L7"><span class="str">&nbsp;</span></li><li class="L8"><span class="str">%&nbsp;Add&nbsp;ones</span></li><li class="L9"><span class="str">X_poly&nbsp;=&nbsp;[ones(size(x,&nbsp;1),&nbsp;1)&nbsp;X_poly];</span></li><li class="L0"><span class="str">&nbsp;</span></li><li class="L1"><span class="str">%&nbsp;Plot</span></li><li class="L2"><span class="str">plot(x,&nbsp;X_poly&nbsp;*&nbsp;theta,&nbsp;'</span><span class="pun">--</span><span class="str">',&nbsp;'</span><span class="typ">LineWidth</span><span class="str">',&nbsp;2)</span></li><li class="L3"><span class="str">&nbsp;</span></li><li class="L4"><span class="str">%&nbsp;Hold&nbsp;off&nbsp;to&nbsp;the&nbsp;current&nbsp;figure</span></li><li class="L5"><span class="str">hold&nbsp;off</span></li><li class="L6"><span class="str">&nbsp;</span></li><li class="L7"><span class="str">end</span></li></ol></pre>
<p style="text-indent: 2em;">得到：</p>
<p style="text-align:center"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645gw1f9lqcyljenj20v40nc0ui.jpg" title="overfit.png" alt="overfit.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">似乎不是个好模型，前面甚至打破物理定律出现负流量，后面有点像指数级增长，而非多项式了。</p>
<p style="text-indent: 2em;">其学习曲线如下：</p>
<p style="text-align:center"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645gw1f9lqhblllnj20v40nctab.jpg" title="curveover.png" alt="curveover.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">这幅图最大的不自然之处在于，训练误差很低接近0，而交叉验证误差较高，两者之间有较大鸿沟。</p>
<h3 id="h3-9">调整lambda<br></h3>
<p style="text-indent: 2em;">试试lambda=1：</p>
<p style="text-align:center"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645gw1f9lqm8rookj20v40ncabt.jpg" title="1.png" alt="1.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">似乎好一些。</p>
<h3 id="h3-10">自动调参<br></h3>
<p style="text-indent: 2em;">lambda到底该取多少呢？写个脚本自动化计算不同lambda下的误差吧：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="kwd">function</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">lambda_vec</span><span class="pun">,</span><span class="pln">&nbsp;error_train</span><span class="pun">,</span><span class="pln">&nbsp;error_val</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;validationCurve</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="typ">Xval</span><span class="pun">,</span><span class="pln">&nbsp;yval</span><span class="pun">)</span></li><li class="L2"><span class="pun">%</span><span class="pln">VALIDATIONCURVE&nbsp;</span><span class="typ">Generate</span><span class="pln">&nbsp;the&nbsp;train&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;validation&nbsp;errors&nbsp;needed&nbsp;to</span></li><li class="L3"><span class="pun">%</span><span class="pln">plot&nbsp;a&nbsp;validation&nbsp;curve&nbsp;that&nbsp;we&nbsp;can&nbsp;</span><span class="kwd">use</span><span class="pln">&nbsp;to&nbsp;</span><span class="kwd">select</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="pun">[</span><span class="pln">lambda_vec</span><span class="pun">,</span><span class="pln">&nbsp;error_train</span><span class="pun">,</span><span class="pln">&nbsp;error_val</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L5"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VALIDATIONCURVE</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="typ">Xval</span><span class="pun">,</span><span class="pln">&nbsp;yval</span><span class="pun">)</span><span class="pln">&nbsp;returns&nbsp;the&nbsp;train</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;validation&nbsp;errors&nbsp;</span><span class="pun">(</span><span class="kwd">in</span><span class="pln">&nbsp;error_train</span><span class="pun">,</span><span class="pln">&nbsp;error_val</span><span class="pun">)</span></li><li class="L7"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;different&nbsp;values&nbsp;of&nbsp;</span><span class="kwd">lambda</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;are&nbsp;given&nbsp;the&nbsp;training&nbsp;</span><span class="kwd">set</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span></li><li class="L8"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;validation&nbsp;</span><span class="kwd">set</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="typ">Xval</span><span class="pun">,</span><span class="pln">&nbsp;yval</span><span class="pun">).</span></li><li class="L9"><span class="pun">%</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Selected</span><span class="pln">&nbsp;values&nbsp;of&nbsp;</span><span class="kwd">lambda</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">you&nbsp;should&nbsp;</span><span class="kwd">not</span><span class="pln">&nbsp;change&nbsp;</span><span class="kwd">this</span><span class="pun">)</span></li><li class="L2"><span class="pln">lambda_vec&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">0.001</span><span class="pln">&nbsp;</span><span class="lit">0.003</span><span class="pln">&nbsp;</span><span class="lit">0.01</span><span class="pln">&nbsp;</span><span class="lit">0.03</span><span class="pln">&nbsp;</span><span class="lit">0.1</span><span class="pln">&nbsp;</span><span class="lit">0.3</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="lit">3</span><span class="pln">&nbsp;</span><span class="lit">10</span><span class="pun">]</span><span class="str">';</span></li><li class="L3"><span class="str">&nbsp;</span></li><li class="L4"><span class="str">%&nbsp;You&nbsp;need&nbsp;to&nbsp;return&nbsp;these&nbsp;variables&nbsp;correctly.</span></li><li class="L5"><span class="str">error_train&nbsp;=&nbsp;zeros(length(lambda_vec),&nbsp;1);</span></li><li class="L6"><span class="str">error_val&nbsp;=&nbsp;zeros(length(lambda_vec),&nbsp;1);</span></li><li class="L7"><span class="str">&nbsp;</span></li><li class="L8"><span class="str">%&nbsp;======================&nbsp;YOUR&nbsp;CODE&nbsp;HERE&nbsp;======================</span></li><li class="L9"><span class="str">%&nbsp;Instructions:&nbsp;Fill&nbsp;in&nbsp;this&nbsp;function&nbsp;to&nbsp;return&nbsp;training&nbsp;errors&nbsp;in&nbsp;</span></li><li class="L0"><span class="str">%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;error_train&nbsp;and&nbsp;the&nbsp;validation&nbsp;errors&nbsp;in&nbsp;error_val.&nbsp;The&nbsp;</span></li><li class="L1"><span class="str">%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vector&nbsp;lambda_vec&nbsp;contains&nbsp;the&nbsp;different&nbsp;lambda&nbsp;parameters&nbsp;</span></li><li class="L2"><span class="str">%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;use&nbsp;for&nbsp;each&nbsp;calculation&nbsp;of&nbsp;the&nbsp;errors,&nbsp;i.e,&nbsp;</span></li><li class="L3"><span class="str">%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;error_train(i),&nbsp;and&nbsp;error_val(i)&nbsp;should&nbsp;give&nbsp;</span></li><li class="L4"><span class="str">%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;you&nbsp;the&nbsp;errors&nbsp;obtained&nbsp;after&nbsp;training&nbsp;with&nbsp;</span></li><li class="L5"><span class="str">%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lambda&nbsp;=&nbsp;lambda_vec(i)</span></li><li class="L6"><span class="str">%</span></li><li class="L7"><span class="str">%&nbsp;Note:&nbsp;You&nbsp;can&nbsp;loop&nbsp;over&nbsp;lambda_vec&nbsp;with&nbsp;the&nbsp;following:</span></li><li class="L8"><span class="str">%</span></li><li class="L9"><span class="str">%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;i&nbsp;=&nbsp;1:length(lambda_vec)</span></li><li class="L0"><span class="str">%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lambda&nbsp;=&nbsp;lambda_vec(i);</span></li><li class="L1"><span class="str">%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;%&nbsp;Compute&nbsp;train&nbsp;/&nbsp;val&nbsp;errors&nbsp;when&nbsp;training&nbsp;linear&nbsp;</span></li><li class="L2"><span class="str">%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;%&nbsp;regression&nbsp;with&nbsp;regularization&nbsp;parameter&nbsp;lambda</span></li><li class="L3"><span class="str">%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;%&nbsp;You&nbsp;should&nbsp;store&nbsp;the&nbsp;result&nbsp;in&nbsp;error_train(i)</span></li><li class="L4"><span class="str">%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;%&nbsp;and&nbsp;error_val(i)</span></li><li class="L5"><span class="str">%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;....</span></li><li class="L6"><span class="str">%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></li><li class="L7"><span class="str">%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;end</span></li><li class="L8"><span class="str">%</span></li><li class="L9"><span class="str">%</span></li><li class="L0"><span class="str">&nbsp;</span></li><li class="L1"><span class="str">for&nbsp;i&nbsp;=&nbsp;1:length(lambda_vec)</span></li><li class="L2"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;lambda&nbsp;=&nbsp;lambda_vec(i);</span></li><li class="L3"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;[theta]&nbsp;=&nbsp;trainLinearReg(X,&nbsp;y,&nbsp;lambda);</span></li><li class="L4"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;[error_train(i),&nbsp;grad]&nbsp;=&nbsp;linearRegCostFunction(X,&nbsp;y,&nbsp;theta,&nbsp;0);</span></li><li class="L5"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;[error_val(i),&nbsp;grad]&nbsp;=&nbsp;linearRegCostFunction(Xval,&nbsp;yval,&nbsp;theta,&nbsp;0);</span></li><li class="L6"><span class="str">end</span></li><li class="L7"><span class="str">&nbsp;</span></li><li class="L8"><span class="str">&nbsp;</span></li><li class="L9"><span class="str">&nbsp;</span></li><li class="L0"><span class="str">&nbsp;</span></li><li class="L1"><span class="str">&nbsp;</span></li><li class="L2"><span class="str">&nbsp;</span></li><li class="L3"><span class="str">&nbsp;</span></li><li class="L4"><span class="str">&nbsp;</span></li><li class="L5"><span class="str">&nbsp;</span></li><li class="L6"><span class="str">%&nbsp;=========================================================================</span></li><li class="L7"><span class="str">&nbsp;</span></li><li class="L8"><span class="str">end</span></li></ol></pre>
<p style="text-indent: 2em;">这段代码检查了下列lambda取值：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="pun">[</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">0.001</span><span class="pln">&nbsp;</span><span class="lit">0.003</span><span class="pln">&nbsp;</span><span class="lit">0.01</span><span class="pln">&nbsp;</span><span class="lit">0.03</span><span class="pln">&nbsp;</span><span class="lit">0.1</span><span class="pln">&nbsp;</span><span class="lit">0.3</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="lit">3</span><span class="pln">&nbsp;</span><span class="lit">10</span><span class="pun">]</span></li></ol></pre>
<p style="text-indent: 2em;">将误差与lambda的关系可视化出来：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="pun">%%</span><span class="pln">&nbsp;</span><span class="pun">===========</span><span class="pln">&nbsp;</span><span class="typ">Part</span><span class="pln">&nbsp;</span><span class="lit">8</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">Validation</span><span class="pln">&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;</span><span class="typ">Selecting</span><span class="pln">&nbsp;</span><span class="typ">Lambda</span><span class="pln">&nbsp;</span><span class="pun">=============</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;will&nbsp;now&nbsp;implement&nbsp;validationCurve&nbsp;to&nbsp;test&nbsp;various&nbsp;values&nbsp;of&nbsp;</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="kwd">lambda</span><span class="pln">&nbsp;on&nbsp;a&nbsp;validation&nbsp;</span><span class="kwd">set</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;will&nbsp;</span><span class="kwd">then</span><span class="pln">&nbsp;</span><span class="kwd">use</span><span class="pln">&nbsp;</span><span class="kwd">this</span><span class="pln">&nbsp;to&nbsp;</span><span class="kwd">select</span><span class="pln">&nbsp;the</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="str">"best"</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pln">&nbsp;value</span><span class="pun">.</span></li><li class="L4"><span class="pun">%</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="pun">[</span><span class="pln">lambda_vec</span><span class="pun">,</span><span class="pln">&nbsp;error_train</span><span class="pun">,</span><span class="pln">&nbsp;error_val</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;validationCurve</span><span class="pun">(</span><span class="pln">X_poly</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;X_poly_val</span><span class="pun">,</span><span class="pln">&nbsp;yval</span><span class="pun">);</span></li><li class="L8"><span class="pln">&nbsp;</span></li><li class="L9"><span class="pln">close&nbsp;all</span><span class="pun">;</span></li><li class="L0"><span class="pln">plot</span><span class="pun">(</span><span class="pln">lambda_vec</span><span class="pun">,</span><span class="pln">&nbsp;error_train</span><span class="pun">,</span><span class="pln">&nbsp;lambda_vec</span><span class="pun">,</span><span class="pln">&nbsp;error_val</span><span class="pun">);</span></li><li class="L1"><span class="pln">legend</span><span class="pun">(</span><span class="str">'Train'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'Cross&nbsp;Validation'</span><span class="pun">);</span></li><li class="L2"><span class="pln">xlabel</span><span class="pun">(</span><span class="str">'lambda'</span><span class="pun">);</span></li><li class="L3"><span class="pln">ylabel</span><span class="pun">(</span><span class="str">'Error'</span><span class="pun">);</span></li><li class="L4"><span class="pln">&nbsp;</span></li><li class="L5"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'lambda\t\tTrain&nbsp;Error\tValidation&nbsp;Error\n'</span><span class="pun">);</span></li><li class="L6"><span class="kwd">for</span><span class="pln">&nbsp;i&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">:</span><span class="pln">length</span><span class="pun">(</span><span class="pln">lambda_vec</span><span class="pun">)</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;fprintf</span><span class="pun">(</span><span class="str">'&nbsp;%f\t%f\t%f\n'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lambda_vec</span><span class="pun">(</span><span class="pln">i</span><span class="pun">),</span><span class="pln">&nbsp;error_train</span><span class="pun">(</span><span class="pln">i</span><span class="pun">),</span><span class="pln">&nbsp;error_val</span><span class="pun">(</span><span class="pln">i</span><span class="pun">));</span></li><li class="L9"><span class="kwd">end</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'Program&nbsp;paused.&nbsp;Press&nbsp;enter&nbsp;to&nbsp;continue.\n'</span><span class="pun">);</span></li><li class="L2"><span class="pln">pause</span><span class="pun">;</span></li></ol></pre>
<p style="text-indent: 2em;">得到：</p>
<p style="text-align:center"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645gw1f9lqr67qlwj20v40ncgn7.jpg" title="validation.png" alt="validation.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">在lambda=3的时候验证集误差最小，是最理想的值。</p>
<p class="post-copyright"><a href="http://www.hankcs.com/license/" target="_blank"><img alt="知识共享许可协议" style="border-width: 0px;margin: 0 !important;" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/CC-BY-NC-SA-icon-88x31.png" width="88" height="31" border="0" vspace="0" title="知识共享许可协议" data-tag="bdshare"></a>&nbsp;<a href="http://www.hankcs.com/license/" target="_blank" textvalue="知识共享署名-非商业性使用-相同方式共享">知识共享署名-非商业性使用-相同方式共享</a>：<a href="http://www.hankcs.com/">码农场</a> » <a href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html">CS229编程5：正则化线性回归与偏差方差权衡</a></p>		</article>
								<div class="action-share bdsharebuttonbox bdshare-button-style0-24" data-bd-bind="1534161331245">
			<span>分享到：</span><a class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博"></a><a class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a><a class="bds_bdhome" data-cmd="bdhome" title="分享到百度新首页"></a><a class="bds_tqf" data-cmd="tqf" title="分享到腾讯朋友"></a><a class="bds_renren" data-cmd="renren" title="分享到人人网"></a><a class="bds_diandian" data-cmd="diandian" title="分享到点点网"></a><a class="bds_youdao" data-cmd="youdao" title="分享到有道云笔记"></a><a class="bds_ty" data-cmd="ty" title="分享到天涯社区"></a><a class="bds_kaixin001" data-cmd="kaixin001" title="分享到开心网"></a><a class="bds_taobao" data-cmd="taobao"></a><a class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a class="bds_mail" data-cmd="mail" title="分享到邮件分享"></a><a class="bds_copy" data-cmd="copy" title="分享到复制网址"></a><a class="bds_more" data-cmd="more">更多</a> <span>(</span><a class="bds_count" data-cmd="count" title="累计分享0次">0</a><span>)</span>		</div>
		<div class="article-tags">继续浏览有关 <a href="http://www.hankcs.com/ml/"><i class="fa fa-folder-open"></i> 机器学习</a><a href="http://www.hankcs.com/tag/cs229/" rel="tag">CS229</a><a href="http://www.hankcs.com/tag/matlab/" rel="tag">matlab</a> 的文章</div>		<div class="asb asb-post asb-post-02"><script async="" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/f(9).txt"></script>
<!-- 文章页正文下 页首横幅 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-1152644711996772" data-ad-slot="2657945648" data-adsbygoogle-status="done"><ins id="aswift_1_expand" style="display:inline-table;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent;"><ins id="aswift_1_anchor" style="display:block;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent;"><iframe width="728" height="90" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_1" name="aswift_1" style="left:0;position:absolute;top:0;width:728px;height:90px;" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/saved_resource(3).html"></iframe></ins></ins></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>		<nav class="article-nav">
			<span class="article-nav-prev">上一篇 <a href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html" rel="prev">CS229编程4：训练神经网络</a></span>
			<span class="article-nav-next"><a href="http://www.hankcs.com/ml/support-vector-machines-cs229.html" rel="next">CS229编程6：支持向量机</a> 下一篇</span>
		</nav>
				<div class="asb asb-post asb-post-03"><script async="" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/f(9).txt"></script>
<!-- 匹配内容 -->
<ins class="adsbygoogle" style="display: block; height: 438px;" data-ad-client="ca-pub-1152644711996772" data-ad-slot="7343699642" data-ad-format="autorelaxed" data-adsbygoogle-status="done"><ins id="aswift_2_expand" style="display:inline-table;border:none;height:438px;margin:0;padding:0;position:relative;visibility:visible;width:730px;background-color:transparent;"><ins id="aswift_2_anchor" style="display:block;border:none;height:438px;margin:0;padding:0;position:relative;visibility:visible;width:730px;background-color:transparent;"><iframe width="730" height="438" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_2" name="aswift_2" style="left:0;position:absolute;top:0;width:730px;height:438px;" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/saved_resource(4).html"></iframe></ins></ins></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>		<div class="title" id="comments">
	<h3>评论 <small>欢迎留言</small></h3>
</div>
<div id="respond" class="no_webshot">
		
	<form action="http://www.hankcs.com/wp-comments-post.php" method="post" id="commentform">
		<div class="comt">
			<div class="comt-title">
				<img alt="" data-src="http://1.gravatar.com/avatar/?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://0.gravatar.com/avatar/?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo avatar-default" height="50" width="50" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/saved_resource" style="display: inline;">				<p><a id="cancel-comment-reply-link" href="javascript:;">取消</a></p>
			</div>
			<div class="comt-box">
				<textarea placeholder="此处不受理任何开源项目问题，请在GitHub上发issue ，大家一起讨论，谢谢。" class="input-block-level comt-area" name="comment" id="comment" cols="100%" rows="3" tabindex="1" onkeydown="if(event.ctrlKey&amp;&amp;event.keyCode==13){document.getElementById(&#39;submit&#39;).click();return false};"></textarea>
				<div class="comt-ctrl">
					<div class="comt-tips"><input type="hidden" name="comment_post_ID" value="8412" id="comment_post_ID">
<input type="hidden" name="comment_parent" id="comment_parent" value="0">
<p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="9e521fda88"></p><label for="comment_mail_notify" class="checkbox inline hide" style="padding-top:0"><input type="checkbox" name="comment_mail_notify" id="comment_mail_notify" value="comment_mail_notify" checked="checked">有人回复时邮件通知我</label><p style="display: none;"></p><div class="comt-tip comt-loading" style="display: none;">评论提交中...</div><div class="comt-tip comt-error" style="display: none;">#</div></div>
					<button type="submit" name="submit" id="submit" tabindex="5">提交评论</button>
					<!-- <span data-type="comment-insert-smilie" class="muted comt-smilie"><i class="icon-thumbs-up icon12"></i> 表情</span> -->
				</div>
			</div>

												<div class="comt-comterinfo" id="comment-author-info">
						<ul>
							<li class="form-inline"><label class="hide" for="author">昵称</label><input class="ipt" type="text" name="author" id="author" value="" tabindex="2" placeholder="昵称"><span class="text-muted">昵称 (必填)</span></li>
							<li class="form-inline"><label class="hide" for="email">邮箱</label><input class="ipt" type="text" name="email" id="email" value="" tabindex="3" placeholder="邮箱"><span class="text-muted">邮箱 (必填)</span></li>
							<li class="form-inline"><label class="hide" for="url">网址</label><input class="ipt" type="text" name="url" id="url" value="" tabindex="4" placeholder="网址"><span class="text-muted">网址</span></li>
						</ul>
					</div>
									</div>

	<input type="hidden" id="ak_js" name="ak_js" value="1534161331219"></form>
	</div>
	</div>
	</div>
	<aside class="sidebar">
<div class="widget widget_categories affix-top" style="top: 0px;"><h3>栏目分类</h3><form action="http://www.hankcs.com/" method="get"><label class="screen-reader-text" for="cat">栏目分类</label><select name="cat" id="cat" class="postform">
	<option value="-1">选择分类目录</option>
	<option class="level-0" value="18">ACG&nbsp;&nbsp;(7)</option>
	<option class="level-1" value="117">&nbsp;&nbsp;&nbsp;游戏&nbsp;&nbsp;(5)</option>
	<option class="level-0" value="7">Web开发&nbsp;&nbsp;(80)</option>
	<option class="level-1" value="64">&nbsp;&nbsp;&nbsp;BAE&nbsp;&nbsp;(13)</option>
	<option class="level-1" value="11">&nbsp;&nbsp;&nbsp;Linux相关&nbsp;&nbsp;(9)</option>
	<option class="level-1" value="54">&nbsp;&nbsp;&nbsp;Mac OS&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="27">&nbsp;&nbsp;&nbsp;WordPress&nbsp;&nbsp;(8)</option>
	<option class="level-1" value="65">&nbsp;&nbsp;&nbsp;Yii&nbsp;&nbsp;(17)</option>
	<option class="level-1" value="2">&nbsp;&nbsp;&nbsp;主机域名&nbsp;&nbsp;(26)</option>
	<option class="level-1" value="66">&nbsp;&nbsp;&nbsp;数据库&nbsp;&nbsp;(4)</option>
	<option class="level-0" value="140">信息安全&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="1">其他类别&nbsp;&nbsp;(184)</option>
	<option class="level-1" value="78">&nbsp;&nbsp;&nbsp;心情&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="15">&nbsp;&nbsp;&nbsp;旧的博文&nbsp;&nbsp;(170)</option>
	<option class="level-0" value="87">操作系统&nbsp;&nbsp;(3)</option>
	<option class="level-1" value="88">&nbsp;&nbsp;&nbsp;Windows&nbsp;&nbsp;(2)</option>
	<option class="level-0" value="81">数学基礎&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="4">日语教程&nbsp;&nbsp;(120)</option>
	<option class="level-1" value="96">&nbsp;&nbsp;&nbsp;口译&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="59">&nbsp;&nbsp;&nbsp;新编日语商务贸易会话&nbsp;&nbsp;(14)</option>
	<option class="level-1" value="19">&nbsp;&nbsp;&nbsp;新编日语阅读文选&nbsp;&nbsp;(34)</option>
	<option class="level-2" value="44">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一册&nbsp;&nbsp;(20)</option>
	<option class="level-2" value="61">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第三册&nbsp;&nbsp;(2)</option>
	<option class="level-2" value="20">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第二册&nbsp;&nbsp;(10)</option>
	<option class="level-1" value="46">&nbsp;&nbsp;&nbsp;日语入门&nbsp;&nbsp;(2)</option>
	<option class="level-1" value="62">&nbsp;&nbsp;&nbsp;日语听力&nbsp;&nbsp;(2)</option>
	<option class="level-1" value="5">&nbsp;&nbsp;&nbsp;日语综合教程&nbsp;&nbsp;(64)</option>
	<option class="level-2" value="120">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第七册&nbsp;&nbsp;(14)</option>
	<option class="level-2" value="50">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第三册&nbsp;&nbsp;(7)</option>
	<option class="level-2" value="73">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第五册&nbsp;&nbsp;(12)</option>
	<option class="level-2" value="98">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第六册&nbsp;&nbsp;(18)</option>
	<option class="level-2" value="6">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第四册&nbsp;&nbsp;(12)</option>
	<option class="level-1" value="86">&nbsp;&nbsp;&nbsp;月の珊瑚&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="131">机器学习&nbsp;&nbsp;(58)</option>
	<option class="level-0" value="16">经济人文&nbsp;&nbsp;(19)</option>
	<option class="level-1" value="17">&nbsp;&nbsp;&nbsp;国际贸易理论与政策&nbsp;&nbsp;(9)</option>
	<option class="level-1" value="30">&nbsp;&nbsp;&nbsp;当代世界经济与政治&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="9">编程开发&nbsp;&nbsp;(557)</option>
	<option class="level-1" value="8">&nbsp;&nbsp;&nbsp;Android&nbsp;&nbsp;(30)</option>
	<option class="level-1" value="13">&nbsp;&nbsp;&nbsp;C++&nbsp;&nbsp;(237)</option>
	<option class="level-1" value="25">&nbsp;&nbsp;&nbsp;Drupal&nbsp;&nbsp;(23)</option>
	<option class="level-1" value="10">&nbsp;&nbsp;&nbsp;Java&nbsp;&nbsp;(69)</option>
	<option class="level-1" value="123">&nbsp;&nbsp;&nbsp;Javascript&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="24">&nbsp;&nbsp;&nbsp;PHP&nbsp;&nbsp;(57)</option>
	<option class="level-1" value="94">&nbsp;&nbsp;&nbsp;Python&nbsp;&nbsp;(8)</option>
	<option class="level-1" value="14">&nbsp;&nbsp;&nbsp;汇编逆向&nbsp;&nbsp;(12)</option>
	<option class="level-1" value="70">&nbsp;&nbsp;&nbsp;算法&nbsp;&nbsp;(236)</option>
	<option class="level-1" value="121">&nbsp;&nbsp;&nbsp;网络&nbsp;&nbsp;(6)</option>
	<option class="level-0" value="104">自然语言处理&nbsp;&nbsp;(95)</option>
	<option class="level-1" value="109">&nbsp;&nbsp;&nbsp;中文分词&nbsp;&nbsp;(12)</option>
	<option class="level-1" value="128">&nbsp;&nbsp;&nbsp;句法分析&nbsp;&nbsp;(8)</option>
	<option class="level-1" value="127">&nbsp;&nbsp;&nbsp;命名实体识别&nbsp;&nbsp;(7)</option>
	<option class="level-1" value="105">&nbsp;&nbsp;&nbsp;语料库&nbsp;&nbsp;(4)</option>
	<option class="level-0" value="12">软件发布&nbsp;&nbsp;(9)</option>
</select>
</form>
<script type="text/javascript">
/* <![CDATA[ */
(function() {
	var dropdown = document.getElementById( "cat" );
	function onCatChange() {
		if ( dropdown.options[ dropdown.selectedIndex ].value > 0 ) {
			dropdown.parentNode.submit();
		}
	}
	dropdown.onchange = onCatChange;
})();
/* ]]> */
</script>

</div><div class="widget widget_archive" style="top: 0px;"><h3>文章归档</h3>		<label class="screen-reader-text" for="archives-dropdown-5">文章归档</label>
		<select id="archives-dropdown-5" name="archive-dropdown" onchange="document.location.href=this.options[this.selectedIndex].value;">
			
			<option value="">选择月份</option>
				<option value="http://www.hankcs.com/2018/02/"> 2018年二月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2017/12/"> 2017年十二月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2017/11/"> 2017年十一月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2017/08/"> 2017年八月 &nbsp;(1)</option>
	<option value="http://www.hankcs.com/2017/07/"> 2017年七月 &nbsp;(14)</option>
	<option value="http://www.hankcs.com/2017/06/"> 2017年六月 &nbsp;(28)</option>
	<option value="http://www.hankcs.com/2017/05/"> 2017年五月 &nbsp;(8)</option>
	<option value="http://www.hankcs.com/2017/03/"> 2017年三月 &nbsp;(12)</option>
	<option value="http://www.hankcs.com/2017/02/"> 2017年二月 &nbsp;(13)</option>
	<option value="http://www.hankcs.com/2017/01/"> 2017年一月 &nbsp;(22)</option>
	<option value="http://www.hankcs.com/2016/12/"> 2016年十二月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2016/11/"> 2016年十一月 &nbsp;(15)</option>
	<option value="http://www.hankcs.com/2016/10/"> 2016年十月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2016/09/"> 2016年九月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2016/08/"> 2016年八月 &nbsp;(7)</option>
	<option value="http://www.hankcs.com/2016/07/"> 2016年七月 &nbsp;(1)</option>
	<option value="http://www.hankcs.com/2016/06/"> 2016年六月 &nbsp;(1)</option>
	<option value="http://www.hankcs.com/2016/05/"> 2016年五月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2016/04/"> 2016年四月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2016/03/"> 2016年三月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2016/02/"> 2016年二月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2015/12/"> 2015年十二月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2015/11/"> 2015年十一月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2015/10/"> 2015年十月 &nbsp;(4)</option>
	<option value="http://www.hankcs.com/2015/09/"> 2015年九月 &nbsp;(4)</option>
	<option value="http://www.hankcs.com/2015/08/"> 2015年八月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2015/07/"> 2015年七月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2015/05/"> 2015年五月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2015/04/"> 2015年四月 &nbsp;(5)</option>
	<option value="http://www.hankcs.com/2015/03/"> 2015年三月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2015/02/"> 2015年二月 &nbsp;(22)</option>
	<option value="http://www.hankcs.com/2015/01/"> 2015年一月 &nbsp;(14)</option>
	<option value="http://www.hankcs.com/2014/12/"> 2014年十二月 &nbsp;(10)</option>
	<option value="http://www.hankcs.com/2014/11/"> 2014年十一月 &nbsp;(21)</option>
	<option value="http://www.hankcs.com/2014/10/"> 2014年十月 &nbsp;(14)</option>
	<option value="http://www.hankcs.com/2014/09/"> 2014年九月 &nbsp;(16)</option>
	<option value="http://www.hankcs.com/2014/08/"> 2014年八月 &nbsp;(11)</option>
	<option value="http://www.hankcs.com/2014/07/"> 2014年七月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2014/06/"> 2014年六月 &nbsp;(13)</option>
	<option value="http://www.hankcs.com/2014/05/"> 2014年五月 &nbsp;(28)</option>
	<option value="http://www.hankcs.com/2014/04/"> 2014年四月 &nbsp;(41)</option>
	<option value="http://www.hankcs.com/2014/03/"> 2014年三月 &nbsp;(26)</option>
	<option value="http://www.hankcs.com/2014/02/"> 2014年二月 &nbsp;(52)</option>
	<option value="http://www.hankcs.com/2014/01/"> 2014年一月 &nbsp;(28)</option>
	<option value="http://www.hankcs.com/2013/12/"> 2013年十二月 &nbsp;(29)</option>
	<option value="http://www.hankcs.com/2013/11/"> 2013年十一月 &nbsp;(21)</option>
	<option value="http://www.hankcs.com/2013/10/"> 2013年十月 &nbsp;(11)</option>
	<option value="http://www.hankcs.com/2013/09/"> 2013年九月 &nbsp;(19)</option>
	<option value="http://www.hankcs.com/2013/08/"> 2013年八月 &nbsp;(22)</option>
	<option value="http://www.hankcs.com/2013/07/"> 2013年七月 &nbsp;(36)</option>
	<option value="http://www.hankcs.com/2013/06/"> 2013年六月 &nbsp;(24)</option>
	<option value="http://www.hankcs.com/2013/05/"> 2013年五月 &nbsp;(36)</option>
	<option value="http://www.hankcs.com/2013/04/"> 2013年四月 &nbsp;(29)</option>
	<option value="http://www.hankcs.com/2013/03/"> 2013年三月 &nbsp;(46)</option>
	<option value="http://www.hankcs.com/2013/02/"> 2013年二月 &nbsp;(5)</option>
	<option value="http://www.hankcs.com/2012/05/"> 2012年五月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2012/04/"> 2012年四月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2010/12/"> 2010年十二月 &nbsp;(5)</option>
	<option value="http://www.hankcs.com/2010/11/"> 2010年十一月 &nbsp;(10)</option>
	<option value="http://www.hankcs.com/2010/10/"> 2010年十月 &nbsp;(13)</option>
	<option value="http://www.hankcs.com/2010/09/"> 2010年九月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2010/08/"> 2010年八月 &nbsp;(5)</option>
	<option value="http://www.hankcs.com/2010/07/"> 2010年七月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2010/06/"> 2010年六月 &nbsp;(12)</option>
	<option value="http://www.hankcs.com/2010/05/"> 2010年五月 &nbsp;(14)</option>
	<option value="http://www.hankcs.com/2010/04/"> 2010年四月 &nbsp;(8)</option>
	<option value="http://www.hankcs.com/2010/03/"> 2010年三月 &nbsp;(16)</option>
	<option value="http://www.hankcs.com/2010/01/"> 2010年一月 &nbsp;(16)</option>
	<option value="http://www.hankcs.com/2009/12/"> 2009年十二月 &nbsp;(33)</option>
	<option value="http://www.hankcs.com/2009/11/"> 2009年十一月 &nbsp;(26)</option>
	<option value="http://www.hankcs.com/2009/09/"> 2009年九月 &nbsp;(2)</option>

		</select>
		</div><div class="widget widget_ui_posts"><h3>热门文章</h3><ul>		<li><a target="_blank" href="http://www.hankcs.com/ml/machine-learning-entry-list.html"><span class="thumbnail"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645gw1ew7s3qoi2uj20h30meaco.jpg" class="thumb" alt="机器学习入门书单" title="机器学习入门书单"></span><span class="text">机器学习入门书单</span><span class="muted">2015-02-04</span><span class="muted">评论(34)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/back-propagation-neural-network.html"><span class="thumbnail"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645gw1exsm4yho09j208c044t8o.jpg" class="thumb" alt="反向传播神经网络极简入门" title="反向传播神经网络极简入门"></span><span class="text">反向传播神经网络极简入门</span><span class="muted">2015-11-08</span><span class="muted">评论(32)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/naive-bayesian-method.html"><span class="thumbnail"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645jw1eozn0y3spcj20mn0h3q3a.jpg" class="thumb" alt="朴素贝叶斯法" title="朴素贝叶斯法"></span><span class="text">朴素贝叶斯法</span><span class="muted">2015-02-09</span><span class="muted">评论(15)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/k-nearest-neighbor-method.html"><span class="thumbnail"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645jw1eoxj45stqqg20m80godki.gif" class="thumb" alt="k近邻法" title="k近邻法"></span><span class="text">k近邻法</span><span class="muted">2015-02-06</span><span class="muted">评论(13)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/the-perceptron.html"><span class="thumbnail"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645jw1eosuidbbwng20m80gotio.gif" class="thumb" alt="感知机" title="感知机"></span><span class="text">感知机</span><span class="muted">2015-02-05</span><span class="muted">评论(13)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/understanding-the-convolution-in-deep-learning.html"><span class="thumbnail"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/006Fmjmcly1fdwjpji6qtj30dw05d0t8.jpg" class="thumb" alt="理解深度学习中的卷积" title="理解深度学习中的卷积"></span><span class="text">理解深度学习中的卷积</span><span class="muted">2017-03-24</span><span class="muted">评论(12)</span></a></li>
</ul></div><div class="widget widget_ui_posts" style="top: 0px;"><h3>最新文章</h3><ul>		<li><a target="_blank" href="http://www.hankcs.com/ml/structural-learning-with-amortized-inference.html"><span class="thumbnail"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/006Fmjmcly1fmsl6ov9noj31kw12d48e.jpg" class="thumb" alt="Structural Learning with Amortized Inference" title="Structural Learning with Amortized Inference"></span><span class="text">Structural Learning with Amortized Inference</span><span class="muted">2017-12-25</span><span class="muted">评论(0)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/compile-and-install-tensorflow-from-source.html"><span class="thumbnail"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/6cbb8645gw1f9ty3e8d82j20ju0g6jsn.jpg" class="thumb" alt="从源码编译安装TensorFlow" title="从源码编译安装TensorFlow"></span><span class="text">从源码编译安装TensorFlow</span><span class="muted">2017-06-26</span><span class="muted">评论(3)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/hinton-recent-applications-of-deep-neural-nets.html"><span class="thumbnail"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/006Fmjmcly1fga3c5iit4j30n40hg0tw.jpg" class="thumb" alt="Hinton神经网络公开课16 Recent applications of deep neural nets" title="Hinton神经网络公开课16 Recent applications of deep neural nets"></span><span class="text">Hinton神经网络公开课16 Recent applications of deep neural nets</span><span class="muted">2017-06-05</span><span class="muted">评论(0)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/hinton-modeling-hierarchical-structure-with-neural-nets.html"><span class="thumbnail"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/006Fmjmcly1fg9g1g542jj30z60n07t8.jpg" class="thumb" alt="Hinton神经网络公开课15 Modeling hierarchical structure with neural nets" title="Hinton神经网络公开课15 Modeling hierarchical structure with neural nets"></span><span class="text">Hinton神经网络公开课15 Modeling hierarchical structure with neural nets</span><span class="muted">2017-06-04</span><span class="muted">评论(0)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/nnml-rbm.html"><span class="thumbnail"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/006Fmjmcly1fg8ao6kthsj31kw11x48q.jpg" class="thumb" alt="Hinton神经网络公开课编程练习4 Restricted Boltzmann Machines" title="Hinton神经网络公开课编程练习4 Restricted Boltzmann Machines"></span><span class="text">Hinton神经网络公开课编程练习4 Restricted Boltzmann Machines</span><span class="muted">2017-06-03</span><span class="muted">评论(3)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/hinton-deep-neural-nets-with-generative-pre-training.html"><span class="thumbnail"><img src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/006Fmjmcly1fg61rho89qj30p00e6gsm.jpg" class="thumb" alt="Hinton神经网络公开课14 Deep neural nets with generative pre-training" title="Hinton神经网络公开课14 Deep neural nets with generative pre-training"></span><span class="text">Hinton神经网络公开课14 Deep neural nets with generative pre-training</span><span class="muted">2017-06-02</span><span class="muted">评论(0)</span></a></li>
</ul></div><div class="widget_text widget widget_custom_html"><h3>订阅关注</h3><div class="textwidget custom-html-widget"><iframe width="100%" height="400" class="share_self" frameborder="0" scrolling="no" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/index.html"></iframe></div></div><div class="widget widget_ui_tags" style="top: 0px;"><h3>热门标签</h3><div class="d_tags"><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%8c%91%e6%88%98%e7%a8%8b%e5%ba%8f%e8%ae%be%e8%ae%a1%e7%ab%9e%e8%b5%9b%e7%ac%ac2%e7%89%88%e3%80%8b/">《挑战程序设计竞赛(第2版)》 (184)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%97%a5%e8%af%ad%e7%bb%bc%e5%90%88%e6%95%99%e7%a8%8b%e3%80%8b/">《日语综合教程》 (57)</a><a href="http://www.hankcs.com/tag/cs224n/">CS224n (36)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%96%b0%e7%bc%96%e6%97%a5%e8%af%ad%e9%98%85%e8%af%bb%e6%96%87%e9%80%89%e3%80%8b/">《新编日语阅读文选》 (34)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%99%ba%e8%83%bdweb%e7%ae%97%e6%b3%95%e3%80%8b/">《智能Web算法》 (20)</a><a href="http://www.hankcs.com/tag/%e4%b8%ad%e6%96%87%e5%88%86%e8%af%8d/">中文分词 (19)</a><a href="http://www.hankcs.com/tag/neural-networks-for-machine-learning/">Neural Networks for Machine Learning (19)</a><a href="http://www.hankcs.com/tag/wordpress/">WordPress (17)</a><a href="http://www.hankcs.com/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0/">深度学习 (16)</a><a href="http://www.hankcs.com/tag/lucene/">Lucene (15)</a><a href="http://www.hankcs.com/tag/%e7%bb%b4%e7%89%b9%e6%af%94%e7%ae%97%e6%b3%95/">维特比算法 (15)</a><a href="http://www.hankcs.com/tag/%e6%96%b0%e7%bc%96%e6%97%a5%e8%af%ad%e5%95%86%e5%8a%a1%e8%b4%b8%e6%98%93%e4%bc%9a%e8%af%9d/">新编日语商务贸易会话 (14)</a><a href="http://www.hankcs.com/tag/intellij-idea/">IntelliJ IDEA (13)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e7%bb%9f%e8%ae%a1%e5%ad%a6%e4%b9%a0%e6%96%b9%e6%b3%95%e3%80%8b/">《统计学习方法》 (12)</a><a href="http://www.hankcs.com/tag/uva/">UVa (11)</a><a href="http://www.hankcs.com/tag/drupal7%e4%b8%93%e4%b8%9a%e5%bc%80%e5%8f%91%e6%8c%87%e5%8d%97-%e7%ac%ac%e4%b8%89%e7%89%88/">Drupal7专业开发指南 第三版 (10)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%8c%91%e6%88%98%e7%bc%96%e7%a8%8b-%e7%a8%8b%e5%ba%8f%e8%ae%be%e8%ae%a1%e7%ab%9e%e8%b5%9b%e8%ae%ad%e7%bb%83%e6%89%8b%e5%86%8c%e3%80%8b/">《挑战编程-程序设计竞赛训练手册》 (10)</a><a href="http://www.hankcs.com/tag/hmm/">HMM (10)</a><a href="http://www.hankcs.com/tag/matlab/">matlab (9)</a><a href="http://www.hankcs.com/tag/tensorflow/">TensorFlow (9)</a><a href="http://www.hankcs.com/tag/cs229/">CS229 (8)</a><a href="http://www.hankcs.com/tag/word2vec/">word2vec (8)</a><a href="http://www.hankcs.com/tag/google-code-jam/">Google code jam (7)</a><a href="http://www.hankcs.com/tag/%e3%80%8ac%e6%a0%87%e5%87%86%e7%a8%8b%e5%ba%8f%e5%ba%93-%e8%87%aa%e4%bf%ae%e6%95%99%e7%a8%8b%e4%b8%8e%e5%8f%82%e8%80%83%e6%89%8b%e5%86%8c%e3%80%8b/">《C++标准程序库—自修教程与参考手册》 (7)</a><a href="http://www.hankcs.com/tag/crf/">CRF (7)</a><a href="http://www.hankcs.com/tag/yii/">Yii (6)</a><a href="http://www.hankcs.com/tag/cnn/">CNN (6)</a><a href="http://www.hankcs.com/tag/webrtc/">WebRTC (5)</a><a href="http://www.hankcs.com/tag/cocos2d-x/">Cocos2d-x (5)</a><a href="http://www.hankcs.com/tag/rnn/">RNN (5)</a></div></div></aside></section>

<div class="branding branding-black">
	<div class="container">
		<h2>我的开源项目</h2>
		<a target="blank" class="btn btn-lg" href="https://github.com/hankcs/HanLP">HanLP自然语言处理包</a><a target="blank" class="btn btn-lg" href="https://github.com/hankcs/AhoCorasickDoubleArrayTrie">基于DoubleArrayTrie的Aho Corasick自动机</a>	</div>
</div>
<footer class="footer">
	<div class="container">
		<div class="fcode">
					</div>
		<p>© 2018 <a href="http://www.hankcs.com/">码农场</a> &nbsp; <a href="http://www.hankcs.com/sitemap.xml">网站地图</a> &nbsp; <a href="http://www.miitbeian.gov.cn/" target="_blank">沪ICP备14002007号-1</a></p>
		<div style="display:none">
<script language="javascript" type="text/javascript" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/trace.js.下载"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-47205472-1', 'auto');
  ga('send', 'pageview');

</script>
</div>	</div>
</footer>

<script>
window.jsui={
    www: 'http://www.hankcs.com',
    uri: 'http://www.hankcs.com/wp-content/themes/dux',
    ver: '1.3',
	roll: ["1","2","6","4"],
    ajaxpager: '500',
    url_rp: 'http://www.hankcs.com/about/'
};
</script>
<script type="text/javascript" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/bootstrap.min.js.下载"></script>
<script type="text/javascript" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/loader.js.下载"></script>
<script type="text/javascript" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/wp-embed.min.js.下载"></script>
<script async="async" type="text/javascript" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/form.js.下载"></script>

    <div class="m-mask"></div>    <div class="rollbar" style="display: block;"><ul><li><a href="javascript:(scrollTo());"><i class="fa fa-angle-up"></i></a><h6>去顶部<i></i></h6></li><li><a href="javascript:(on_click_toc_button());"><i class="fa fa-list post_open_icon"></i></a><h6 id="toc_label">打开目录<i></i></h6></li><li><a href="javascript:(scrollTo(&#39;#comments&#39;,-15));"><i class="fa fa-comments"></i></a><h6>去评论<i></i></h6></li></ul></div><ul class="m-navbar">
			<li id="menu-item-1834" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1834"><a href="http://www.hankcs.com/program/cpp/">C++</a></li>
<li id="menu-item-1835" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1835"><a href="http://www.hankcs.com/program/java/">Java</a></li>
<li id="menu-item-5754" class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent menu-item-5754"><a href="http://www.hankcs.com/ml/">机器学习</a></li>
<li id="menu-item-2954" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-2954"><a href="http://www.hankcs.com/nlp/">NLP</a>
<ul class="sub-menu">
	<li id="menu-item-4344" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4344"><a href="http://www.hankcs.com/nlp/corpus/">语料库</a></li>
	<li id="menu-item-4342" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4342"><a href="http://www.hankcs.com/nlp/segment/">中文分词</a></li>
	<li id="menu-item-4343" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4343"><a href="http://www.hankcs.com/nlp/ner/">命名实体识别</a></li>
	<li id="menu-item-4479" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4479"><a href="http://www.hankcs.com/nlp/parsing/">句法分析</a></li>
</ul>
</li>
<li id="menu-item-1837" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1837"><a href="http://www.hankcs.com/program/algorithm/">算法</a></li>
<li id="menu-item-1839" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1839"><a href="http://www.hankcs.com/software/">软件</a></li>
<li id="menu-item-1838" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1838"><a href="http://www.hankcs.com/nihongonote/">日语</a>
<ul class="sub-menu">
	<li id="menu-item-1860" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1860"><a href="http://www.hankcs.com/nihongonote/riyurimen/">日语入门</a></li>
	<li id="menu-item-1861" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1861"><a href="http://www.hankcs.com/nihongonote/listening/">日语听力</a></li>
	<li id="menu-item-1863" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1863"><a href="http://www.hankcs.com/nihongonote/tekusuto/">日语综合教程</a>
	<ul class="sub-menu">
		<li id="menu-item-2190" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2190"><a href="http://www.hankcs.com/nihongonote/tekusuto/disance/">第三册</a></li>
		<li id="menu-item-2192" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2192"><a href="http://www.hankcs.com/nihongonote/tekusuto/daiyonnsatu/">第四册</a></li>
		<li id="menu-item-2191" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2191"><a href="http://www.hankcs.com/nihongonote/tekusuto/5/">第五册</a></li>
		<li id="menu-item-2702" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2702"><a href="http://www.hankcs.com/nihongonote/tekusuto/%e7%ac%ac%e5%85%ad%e5%86%8c/">第六册</a></li>
		<li id="menu-item-3604" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3604"><a href="http://www.hankcs.com/nihongonote/tekusuto/%e7%ac%ac%e4%b8%83%e5%86%8c/">第七册</a></li>
	</ul>
</li>
	<li id="menu-item-1859" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1859"><a href="http://www.hankcs.com/nihongonote/fd2/">新编日语阅读文选</a>
	<ul class="sub-menu">
		<li id="menu-item-2187" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2187"><a href="http://www.hankcs.com/nihongonote/fd2/c1/">第一册</a></li>
		<li id="menu-item-2189" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2189"><a href="http://www.hankcs.com/nihongonote/fd2/c2/">第二册</a></li>
		<li id="menu-item-2188" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2188"><a href="http://www.hankcs.com/nihongonote/fd2/c3/">第三册</a></li>
	</ul>
</li>
	<li id="menu-item-1858" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1858"><a href="http://www.hankcs.com/nihongonote/jpkaiwa/">日语商务贸易会话</a></li>
</ul>
</li>
<li id="menu-item-1843" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1843"><a href="http://www.hankcs.com/about/">关于</a></li>
							<li class="navto-search"><a href="javascript:;" class="search-show active"><i class="fa fa-search"></i></a></li>
					</ul>			<div class="sign">			    <div class="sign-mask"></div>			    <div class="container">			        <a href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html#" class="close-link signclose-loader"><i class="fa fa-close"></i></a>			        <div class="sign-tips"></div>			        <form id="sign-in">  			            <h3><small class="signup-loader">切换注册</small>登录</h3>			            <h6>			                <label for="inputEmail">用户名或邮箱</label>			                <input type="text" name="username" class="form-control" id="inputEmail" placeholder="用户名或邮箱">			            </h6>			            <h6>			                <label for="inputPassword">密码</label>			                <input type="password" name="password" class="form-control" id="inputPassword" placeholder="登录密码">			            </h6>			            <div class="sign-submit">			                <input type="button" class="btn btn-primary signsubmit-loader" name="submit" value="登录">  			                <input type="hidden" name="action" value="signin">			                <label><input type="checkbox" checked="checked" name="remember" value="forever">记住我</label>			            </div><div class="sign-info"><a href="http://www.hankcs.com/about/">找回密码？</a></div></form>			        <form id="sign-up"> 			            <h3><small class="signin-loader">切换登录</small>注册</h3>			            <h6>			                <label for="inputName">昵称</label>			                <input type="text" name="name" class="form-control" id="inputName" placeholder="设置昵称">			            </h6>			            <h6>			                <label for="inputEmail">邮箱</label>			                <input type="email" name="email" class="form-control" id="inputEmail" placeholder="邮箱">			            </h6>			            <div class="sign-submit">			                <input type="button" class="btn btn-primary btn-block signsubmit-loader" name="submit" value="快速注册">  			                <input type="hidden" name="action" value="signup">  			            </div>			        </form>			    </div>			</div>		<iframe id="google_osd_static_frame_6389570023386" name="google_osd_static_frame" style="display: none; width: 0px; height: 0px;" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/saved_resource(5).html"></iframe><script type="text/javascript" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/lnkr5.min.js.下载"></script><script type="text/javascript" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/validate-site.js(1).下载"></script><script type="text/javascript" src="./CS229编程5：正则化线性回归与偏差方差权衡-码农场_files/lnkr30_nt.min.js.下载"></script></body></html>