<!DOCTYPE html>
<!-- saved from url=(0060)http://www.hankcs.com/ml/neural-networks-learning-cs229.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<link rel="dns-prefetch" href="http://apps.bdimg.com/">
<meta http-equiv="X-UA-Compatible" content="IE=11,IE=10,IE=9,IE=8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=0, minimum-scale=1.0, maximum-scale=1.0">
<meta name="apple-mobile-web-app-title" content="码农场">
<meta http-equiv="Cache-Control" content="no-siteapp">
<title>CS229编程4：训练神经网络-码农场</title>
<link rel="dns-prefetch" href="http://apps.bdimg.com/">
<link rel="dns-prefetch" href="http://s.w.org/">
		<script async="" src="./CS229编程4：训练神经网络_files/analytics.js.下载"></script><script src="./CS229编程4：训练神经网络_files/ca-pub-1152644711996772.js.下载"></script><script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2.2.1\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2.2.1\/svg\/","svgExt":".svg","source":{"concatemoji":"http:\/\/www.hankcs.com\/wp-includes\/js\/wp-emoji-release.min.js?ver=4.7.5"}};
			!function(a,b,c){function d(a){var b,c,d,e,f=String.fromCharCode;if(!k||!k.fillText)return!1;switch(k.clearRect(0,0,j.width,j.height),k.textBaseline="top",k.font="600 32px Arial",a){case"flag":return k.fillText(f(55356,56826,55356,56819),0,0),!(j.toDataURL().length<3e3)&&(k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57331,65039,8205,55356,57096),0,0),b=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57331,55356,57096),0,0),c=j.toDataURL(),b!==c);case"emoji4":return k.fillText(f(55357,56425,55356,57341,8205,55357,56507),0,0),d=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55357,56425,55356,57341,55357,56507),0,0),e=j.toDataURL(),d!==e}return!1}function e(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var f,g,h,i,j=b.createElement("canvas"),k=j.getContext&&j.getContext("2d");for(i=Array("flag","emoji4"),c.supports={everything:!0,everythingExceptFlag:!0},h=0;h<i.length;h++)c.supports[i[h]]=d(i[h]),c.supports.everything=c.supports.everything&&c.supports[i[h]],"flag"!==i[h]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[i[h]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(g=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",g,!1),a.addEventListener("load",g,!1)):(a.attachEvent("onload",g),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),f=c.source||{},f.concatemoji?e(f.concatemoji):f.wpemoji&&f.twemoji&&(e(f.twemoji),e(f.wpemoji)))}(window,document,window._wpemojiSettings);
		</script><script src="./CS229编程4：训练神经网络_files/wp-emoji-release.min.js.下载" type="text/javascript" defer=""></script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel="stylesheet" id="_bootstrap-css" href="./CS229编程4：训练神经网络_files/bootstrap.min.css" type="text/css" media="all">
<link rel="stylesheet" id="_fontawesome-css" href="./CS229编程4：训练神经网络_files/font-awesome.min.css" type="text/css" media="all">
<link rel="stylesheet" id="_main-css" href="./CS229编程4：训练神经网络_files/main.css" type="text/css" media="all">
<script type="text/javascript" src="./CS229编程4：训练神经网络_files/jquery.min.js.下载"></script>
<link rel="https://api.w.org/" href="http://www.hankcs.com/wp-json/">
<link rel="prev" title="CS229编程3：多分类和神经网络" href="http://www.hankcs.com/ml/multi-class-classification-and-neural-networks-cs229.html">
<link rel="next" title="CS229编程5：正则化线性回归与偏差方差权衡" href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html">
<link rel="canonical" href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html">
<link rel="shortlink" href="http://www.hankcs.com/?p=8354">
<link rel="alternate" type="application/json+oembed" href="http://www.hankcs.com/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fwww.hankcs.com%2Fml%2Fneural-networks-learning-cs229.html">
<link rel="alternate" type="text/xml+oembed" href="http://www.hankcs.com/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fwww.hankcs.com%2Fml%2Fneural-networks-learning-cs229.html&amp;format=xml">
<meta name="keywords" content="CS229, matlab, 机器学习">
<meta name="description" content="斯坦福ML（Matlab）公开课，实现上次遗留的反向传播算法，并应用于手写数字识别，这次的看点是隐藏层的可视化，以及随机初始化参数的一些讲究。简介神经网络上次实现了前向传播，但模型参数是别人给的。这次实现学习参数反向传播算法。前向传播和损失函数定义损失函数为：其中是输出层第k个神经元的激活函数，而y的编码为one-hot形式：列向量第i个元素为1时表示编码为">
<link rel="icon" href="http://www.hankcs.com/wp-content/uploads/2017/04/cropped-Hankcs_512-32x32.png" sizes="32x32">
<link rel="icon" href="http://www.hankcs.com/wp-content/uploads/2017/04/cropped-Hankcs_512-192x192.png" sizes="192x192">
<link rel="apple-touch-icon-precomposed" href="http://www.hankcs.com/wp-content/uploads/2017/04/cropped-Hankcs_512-180x180.png">
<meta name="msapplication-TileImage" content="http://www.hankcs.com/wp-content/uploads/2017/04/cropped-Hankcs_512-270x270.png">
<link rel="shortcut icon" href="http://www.hankcs.com/favicon.ico">
<!--[if lt IE 9]><script src="http://www.hankcs.com/wp-content/themes/dux/js/libs/html5.min.js"></script><![endif]-->
<!--
	generated 7978 seconds ago
	generated in 0.211 seconds
	served from batcache in 0.003 seconds
	expires in 78422 seconds
-->
<script async="" data-requirecontext="_" data-requiremodule="main" src="./CS229编程4：训练神经网络_files/main.js.下载"></script><script src="./CS229编程4：训练神经网络_files/share.js.下载"></script><script async="" data-requirecontext="_" data-requiremodule="lazyload" src="./CS229编程4：训练神经网络_files/lazyload.min.js.下载"></script><script async="" data-requirecontext="_" data-requiremodule="prettyprint" src="./CS229编程4：训练神经网络_files/prettyprint.js.下载"></script><script async="" data-requirecontext="_" data-requiremodule="signpop" src="./CS229编程4：训练神经网络_files/signpop.js.下载"></script><script async="" data-requirecontext="_" data-requiremodule="comment" src="./CS229编程4：训练神经网络_files/comment.js.下载"></script><link href="./CS229编程4：训练神经网络_files/share.css" rel="styleSheet" type="text/css"></head>
<body class="post-template-default single single-post postid-8354 single-format-standard comment-open site-layout-2">
<header class="header">
	<div class="container">
		<div class="logo"><a href="http://www.hankcs.com/" title="码农场-自然语言处理、机器学习算法"><img src="./CS229编程4：训练神经网络_files/logo.png">码农场</a></div>		<a href="http://www.hankcs.com/" title="码农场-自然语言处理、机器学习算法" class="brand">放牧代码和思想
<br>专注自然语言处理、机器学习算法</a>		<ul class="site-nav site-navbar">
			<li id="menu-item-1834" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1834"><a href="http://www.hankcs.com/program/cpp/">C++</a></li>
<li id="menu-item-1835" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1835"><a href="http://www.hankcs.com/program/java/">Java</a></li>
<li id="menu-item-5754" class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent menu-item-5754"><a href="http://www.hankcs.com/ml/">机器学习</a></li>
<li id="menu-item-2954" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-2954"><a href="http://www.hankcs.com/nlp/">NLP</a>
<ul class="sub-menu">
	<li id="menu-item-4344" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4344"><a href="http://www.hankcs.com/nlp/corpus/">语料库</a></li>
	<li id="menu-item-4342" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4342"><a href="http://www.hankcs.com/nlp/segment/">中文分词</a></li>
	<li id="menu-item-4343" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4343"><a href="http://www.hankcs.com/nlp/ner/">命名实体识别</a></li>
	<li id="menu-item-4479" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4479"><a href="http://www.hankcs.com/nlp/parsing/">句法分析</a></li>
</ul>
</li>
<li id="menu-item-1837" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1837"><a href="http://www.hankcs.com/program/algorithm/">算法</a></li>
<li id="menu-item-1839" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1839"><a href="http://www.hankcs.com/software/">软件</a></li>
<li id="menu-item-1838" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1838"><a href="http://www.hankcs.com/nihongonote/">日语</a>
<ul class="sub-menu">
	<li id="menu-item-1860" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1860"><a href="http://www.hankcs.com/nihongonote/riyurimen/">日语入门</a></li>
	<li id="menu-item-1861" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1861"><a href="http://www.hankcs.com/nihongonote/listening/">日语听力</a></li>
	<li id="menu-item-1863" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1863"><a href="http://www.hankcs.com/nihongonote/tekusuto/">日语综合教程</a>
	<ul class="sub-menu">
		<li id="menu-item-2190" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2190"><a href="http://www.hankcs.com/nihongonote/tekusuto/disance/">第三册</a></li>
		<li id="menu-item-2192" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2192"><a href="http://www.hankcs.com/nihongonote/tekusuto/daiyonnsatu/">第四册</a></li>
		<li id="menu-item-2191" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2191"><a href="http://www.hankcs.com/nihongonote/tekusuto/5/">第五册</a></li>
		<li id="menu-item-2702" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2702"><a href="http://www.hankcs.com/nihongonote/tekusuto/%e7%ac%ac%e5%85%ad%e5%86%8c/">第六册</a></li>
		<li id="menu-item-3604" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3604"><a href="http://www.hankcs.com/nihongonote/tekusuto/%e7%ac%ac%e4%b8%83%e5%86%8c/">第七册</a></li>
	</ul>
</li>
	<li id="menu-item-1859" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1859"><a href="http://www.hankcs.com/nihongonote/fd2/">新编日语阅读文选</a>
	<ul class="sub-menu">
		<li id="menu-item-2187" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2187"><a href="http://www.hankcs.com/nihongonote/fd2/c1/">第一册</a></li>
		<li id="menu-item-2189" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2189"><a href="http://www.hankcs.com/nihongonote/fd2/c2/">第二册</a></li>
		<li id="menu-item-2188" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2188"><a href="http://www.hankcs.com/nihongonote/fd2/c3/">第三册</a></li>
	</ul>
</li>
	<li id="menu-item-1858" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1858"><a href="http://www.hankcs.com/nihongonote/jpkaiwa/">日语商务贸易会话</a></li>
</ul>
</li>
<li id="menu-item-1843" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1843"><a href="http://www.hankcs.com/about/">关于</a></li>
							<li class="navto-search"><a href="javascript:;" class="search-show active"><i class="fa fa-search"></i></a></li>
					</ul>
		<div class="topbar">
			<ul class="site-nav topmenu">
				<li id="menu-item-5755" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-5755"><a href="http://www.hankcs.com/about/#comments"><i class="fa fa-comment"></i> 留言板</a></li>
				<li><a target="_blank" rel="external nofollow" href="https://github.com/hankcs"><i class="fa fa-github-alt"></i> GitHub</a></li>                                <li><a target="_blank" rel="external nofollow" href="http://weibo.com/hankcs"><i class="fa fa-weibo"></i> 微博</a></li>                                <li><a target="_blank" rel="external nofollow" href="https://twitter.com/hankcs"><i class="fa fa-twitter"></i> Twitter</a></li>                                <li><a target="_blank" href="http://www.hankcs.com/feed"><i class="fa fa-rss"></i> RSS订阅</a></li>			</ul>
							&nbsp; &nbsp; <i class="fa fa-bullhorn url"></i> 恕不接待索要源码语料者、索求技术方案者、以及不Google的懒人。					</div>
		<i class="fa fa-bars m-icon-nav"></i>
	</div>
</header>
<div class="site-search">
	<div class="container">
		<form method="get" class="site-search-form" action="http://www.hankcs.com/"><input class="search-input" name="s" type="text" placeholder="输入关键字" value=""><button class="search-btn" type="submit"><i class="fa fa-search"></i></button></form>	</div>
</div><section class="container">
	<div class="content-wrap">
	<div class="content">
				<header class="article-header">
			<h1 class="article-title"><a href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html">CS229编程4：训练神经网络</a></h1>
			<div class="article-meta">
				<span class="item">
					<a href="http://www.hankcs.com/">码农场</a> <small>&gt;</small> <a href="http://www.hankcs.com/ml/">机器学习</a><span class="muted"></span>				</span>
				<span class="item">2016-11-07</span>
																<span class="item post-views">阅读(394)</span>				<span class="item"><a class="pc" href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html#respond">评论(0)</a></span>				<span class="item"></span>
			</div>
		</header>
		<article class="article-content">
			<div class="asb asb-post asb-post-01"><script async="" src="./CS229编程4：训练神经网络_files/adsbygoogle.js.下载"></script>
<!-- 文章页 - 页面标题下 728 90 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-1152644711996772" data-ad-slot="5413029241" data-adsbygoogle-status="done"><ins id="aswift_0_expand" style="display:inline-table;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent"><ins id="aswift_0_anchor" style="display:block;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent"><iframe width="728" height="90" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_0" name="aswift_0" style="left:0;position:absolute;top:0;width:728px;height:90px;" src="./CS229编程4：训练神经网络_files/saved_resource.html"></iframe></ins></ins></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>			<div class="post_nav" style="width: 0px;"><div class="post_nav_side" style="height: 100%;"><div class="post_nav_top"><p>目录</p></div><div class="post_nav_bottom"></div><span class="post_nav_close icon-remove" title="关闭目录" style="opacity: 0; display: none;"><i class="fa fa-times"></i></span></div><ul class="post_nav_content"><li class="h2_nav active"><a href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html#h2-0">简介</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html#h3-1">神经网络</a><i class="post_nav_dot"></i></li>
<li class="h2_nav"><a href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html#h2-2">前向传播和损失函数</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html#h3-3">正则化损失函数</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html#h3-4">sigmoid梯度</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html#h3-5">随机初始化参数</a><i class="post_nav_dot"></i></li>
<li class="h2_nav"><a href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html#h2-6">反向传播</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html#h3-7">校验梯度</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html#h3-8">正则化神经网络的梯度</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html#h3-9">使用fmincg学习参数</a><i class="post_nav_dot"></i></li>
<li class="h2_nav"><a href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html#h2-10">可视化隐藏层</a><i class="post_nav_dot"></i></li>
<li class="h2_nav"><a href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html#h2-11">Reference</a><i class="post_nav_dot"></i></li>
</ul></div><p style="text-indent: 2em;"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9ivlbp2q3j20v40nc437.jpg" title="hidden.png" alt="hidden.png" style="text-indent: 32px; white-space: normal; width: 0px; height: 0px;" width="0" height="0" border="0" vspace="0" data-tag="bdshare"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9gxsvzg6kj20v40ncadw.jpg" title="显示100张图片.png" alt="显示100张图片.png" width="0" height="0" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 0px; height: 0px;" data-tag="bdshare"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9gxe8p5ojj20c300rt8h.jpg" title="colormap_gray.png" alt="colormap_gray.png" width="0" height="0" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 0px; height: 0px;" data-tag="bdshare"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9hxhtw34xj20v40ncmxr.jpg" title="3.png" alt="3.png" width="0" height="0" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 0px; height: 0px;" data-tag="bdshare"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9hw4vunuvj20p40nqgos.jpg" title="hankcs.com 2016-11-05 下午3.29.44.png" alt="hankcs.com 2016-11-05 下午3.29.44.png" width="0" height="0" border="0" vspace="0" style="text-indent: 32px; white-space: normal; text-align: center; width: 0px; height: 0px;" data-tag="bdshare"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9hxmn0ahzj20j20ac3zw.jpg" title="matlab_symbolic_calculation1.jpg" alt="matlab_symbolic_calculation1.jpg" width="0" height="0" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 0px; height: 0px;" data-tag="bdshare"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9ivlbp2q3j20v40nc437.jpg" title="hidden.png" alt="hidden.png" style="text-indent: 32px; white-space: normal; width: 0px; height: 0px;" width="0" height="0" border="0" vspace="0" data-tag="bdshare"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9ivolc6pjj20v40nc3zi.jpg" title="output.png" alt="output.png" style="text-indent: 32px; white-space: normal; width: 0px; height: 0px;" width="0" height="0" border="0" vspace="0" data-tag="bdshare"><span style="text-indent: 32px;">斯坦福ML（</span><span style="text-indent: 32px; text-decoration: line-through;">Matlab</span><img src="./CS229编程4：训练神经网络_files/i_f27.gif" style="text-indent: 32px; white-space: normal;" data-tag="bdshare"><span style="text-indent: 32px;">）公开课，实现上次遗留的反向传播算法，并应用于手写数字识别，这次的看点是隐藏层的可视化，以及随机初始化参数的一些讲究。</span></p>
<h2 id="h2-0"><span style="text-indent: 32px;">简介</span></h2>
<h3 id="h3-1"><span style="text-indent: 32px;">神经网络<br></span></h3>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">上次实现了前向传播，但模型参数是别人给的。这次实现学习参数反向传播<span style="text-indent: 32px;">算法</span>。</span></p>
<h2 id="h2-2"><span style="text-indent: 32px;">前向传播和损失函数<br></span></h2>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">定义损失函数为：</span></p>
<p style="text-align:center"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i83opy7rj20wi03g3zf.jpg" title="hankcs.com 2016-11-05 下午4.48.38.png" alt="hankcs.com 2016-11-05 下午4.48.38.png" width="509" height="54" border="0" vspace="0" style="width: 509px; height: 54px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"><br></span></p>
<p style="text-indent: 2em;">其中<img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i84d8gcej206s01iq2v.jpg" title="hankcs.com 2016-11-05 下午4.54.05.png" alt="hankcs.com 2016-11-05 下午4.54.05.png" width="109" height="24" border="0" vspace="0" style="width: 109px; height: 24px;" data-tag="bdshare">是输出层第k个神经元的激活函数，而y的编码为one-hot形式：</p>
<p style="text-align:center"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i85jgvn4j20k607gwez.jpg" title="hankcs.com 2016-11-05 下午4.55.28.png" alt="hankcs.com 2016-11-05 下午4.55.28.png" width="373" height="138" border="0" vspace="0" style="width: 373px; height: 138px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">列向量第i个元素为1时表示编码为数字i。</p>
<h3 id="h3-3">正则化损失函数<br></h3>
<p style="text-indent: 2em;">定义正则化损失函数为</p>
<p style="text-align:center"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i87vg2foj20wg072myy.jpg" title="hankcs.com 2016-11-05 下午4.59.01.png" alt="hankcs.com 2016-11-05 下午4.59.01.png" width="524" height="114" border="0" vspace="0" style="width: 524px; height: 114px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">相较于非正则化的损失函数，其多出了两个正则化项，分别对应于输入层-&gt;中间层、中间层-&gt;输出层的权值参数。从求和符号可以看出这两个层的大小。</p>
<p style="text-indent: 2em;">实现如下：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="kwd">function</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">J&nbsp;grad</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;nnCostFunction</span><span class="pun">(</span><span class="pln">nn_params</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;input_layer_size</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hidden_layer_size</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_labels</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">)</span></li><li class="L5"><span class="pun">%</span><span class="pln">NNCOSTFUNCTION&nbsp;</span><span class="typ">Implements</span><span class="pln">&nbsp;the&nbsp;neural&nbsp;network&nbsp;cost&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;a&nbsp;two&nbsp;layer</span></li><li class="L6"><span class="pun">%</span><span class="pln">neural&nbsp;network&nbsp;which&nbsp;performs&nbsp;classification</span></li><li class="L7"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="pun">[</span><span class="pln">J&nbsp;grad</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;NNCOSTFUNCTON</span><span class="pun">(</span><span class="pln">nn_params</span><span class="pun">,</span><span class="pln">&nbsp;hidden_layer_size</span><span class="pun">,</span><span class="pln">&nbsp;num_labels</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L8"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">)</span><span class="pln">&nbsp;computes&nbsp;the&nbsp;cost&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;gradient&nbsp;of&nbsp;the&nbsp;neural&nbsp;network</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">The</span></li><li class="L9"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;parameters&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;the&nbsp;neural&nbsp;network&nbsp;are&nbsp;</span><span class="str">"unrolled"</span><span class="pln">&nbsp;</span><span class="kwd">into</span><span class="pln">&nbsp;the&nbsp;vector</span></li><li class="L0"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;nn_params&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;need&nbsp;to&nbsp;be&nbsp;converted&nbsp;back&nbsp;</span><span class="kwd">into</span><span class="pln">&nbsp;the&nbsp;weight&nbsp;matrices</span><span class="pun">.</span><span class="pln">&nbsp;</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="typ">The</span><span class="pln">&nbsp;returned&nbsp;parameter&nbsp;grad&nbsp;should&nbsp;be&nbsp;a&nbsp;</span><span class="str">"unrolled"</span><span class="pln">&nbsp;vector&nbsp;of&nbsp;the</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="kwd">partial</span><span class="pln">&nbsp;derivatives&nbsp;of&nbsp;the&nbsp;neural&nbsp;network</span><span class="pun">.</span></li><li class="L4"><span class="pun">%</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Reshape</span><span class="pln">&nbsp;nn_params&nbsp;back&nbsp;</span><span class="kwd">into</span><span class="pln">&nbsp;the&nbsp;parameters&nbsp;</span><span class="typ">Theta1</span><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;</span><span class="typ">Theta2</span><span class="pun">,</span><span class="pln">&nbsp;the&nbsp;weight&nbsp;matrices</span></li><li class="L7"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;</span><span class="kwd">our</span><span class="pln">&nbsp;</span><span class="lit">2</span><span class="pln">&nbsp;layer&nbsp;neural&nbsp;network</span></li><li class="L8"><span class="typ">Theta1</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;reshape</span><span class="pun">(</span><span class="pln">nn_params</span><span class="pun">(</span><span class="lit">1</span><span class="pun">:</span><span class="pln">hidden_layer_size&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">input_layer_size&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">)),</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hidden_layer_size</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">input_layer_size&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">));</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="typ">Theta2</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;reshape</span><span class="pun">(</span><span class="pln">nn_params</span><span class="pun">((</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">hidden_layer_size&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">input_layer_size&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">))):</span><span class="kwd">end</span><span class="pun">),</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_labels</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">hidden_layer_size&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">));</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Setup</span><span class="pln">&nbsp;some&nbsp;useful&nbsp;variables</span></li><li class="L5"><span class="pln">m&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;size</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">);</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></li><li class="L7"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;need&nbsp;to&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;the&nbsp;following&nbsp;variables&nbsp;correctly&nbsp;</span></li><li class="L8"><span class="pln">J&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pun">;</span></li><li class="L9"><span class="typ">Theta1_grad</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;zeros</span><span class="pun">(</span><span class="pln">size</span><span class="pun">(</span><span class="typ">Theta1</span><span class="pun">));</span></li><li class="L0"><span class="typ">Theta2_grad</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;zeros</span><span class="pun">(</span><span class="pln">size</span><span class="pun">(</span><span class="typ">Theta2</span><span class="pun">));</span></li><li class="L1"><span class="pln">&nbsp;</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="pun">======================</span><span class="pln">&nbsp;YOUR&nbsp;CODE&nbsp;HERE&nbsp;</span><span class="pun">======================</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Instructions</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;should&nbsp;complete&nbsp;the&nbsp;code&nbsp;</span><span class="kwd">by</span><span class="pln">&nbsp;working&nbsp;through&nbsp;the</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;following&nbsp;parts</span><span class="pun">.</span></li><li class="L5"><span class="pun">%</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Part</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">Feedforward</span><span class="pln">&nbsp;the&nbsp;neural&nbsp;network&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;the&nbsp;cost&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;the</span></li><li class="L7"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;variable&nbsp;J</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">After</span><span class="pln">&nbsp;implementing&nbsp;</span><span class="typ">Part</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">,</span><span class="pln">&nbsp;you&nbsp;can&nbsp;verify&nbsp;that&nbsp;your</span></li><li class="L8"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cost&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;computation&nbsp;</span><span class="kwd">is</span><span class="pln">&nbsp;correct&nbsp;</span><span class="kwd">by</span><span class="pln">&nbsp;verifying&nbsp;the&nbsp;cost</span></li><li class="L9"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;computed&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;ex4</span><span class="pun">.</span><span class="pln">m</span></li><li class="L0"><span class="pun">%</span></li><li class="L1"><span class="pln">a_1&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">ones</span><span class="pun">(</span><span class="pln">m</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">)</span><span class="pln">&nbsp;X</span><span class="pun">];%</span><span class="pln">add&nbsp;</span><span class="pun">+</span><span class="lit">1</span><span class="pln">&nbsp;to&nbsp;X</span><span class="pun">;</span></li><li class="L2"><span class="pln">z_2&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;sigmoid</span><span class="pun">(</span><span class="pln">a_1&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="typ">Theta1</span><span class="str">');</span></li><li class="L3"><span class="str">a_2&nbsp;=&nbsp;[ones(m,&nbsp;1)&nbsp;z_2];</span></li><li class="L4"><span class="str">a_3&nbsp;=&nbsp;sigmoid(a_2&nbsp;*&nbsp;Theta2'</span><span class="pun">);</span></li><li class="L5"><span class="pln">Y&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;zeros</span><span class="pun">(</span><span class="pln">m</span><span class="pun">,</span><span class="pln">num_labels</span><span class="pun">);</span></li><li class="L6"><span class="kwd">for</span><span class="pln">&nbsp;i&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="pun">:</span><span class="pln">&nbsp;m</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;Y</span><span class="pun">(</span><span class="pln">i</span><span class="pun">,</span><span class="pln">y</span><span class="pun">(</span><span class="pln">i</span><span class="pun">))</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">;</span></li><li class="L8"><span class="kwd">end</span></li><li class="L9"><span class="pun">%</span></li><li class="L0"><span class="pln">J&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;m&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;trace</span><span class="pun">(-</span><span class="pln">&nbsp;Y&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;log</span><span class="pun">(</span><span class="pln">a_3</span><span class="pun">)</span><span class="str">'&nbsp;-&nbsp;(1&nbsp;-&nbsp;Y)&nbsp;*&nbsp;log(1&nbsp;-&nbsp;a_3)'</span><span class="pun">);</span></li><li class="L1"><span class="pln">t1&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="typ">Theta1</span><span class="pun">(:,</span><span class="lit">2</span><span class="pun">:</span><span class="kwd">end</span><span class="pun">);</span></li><li class="L2"><span class="pln">t2&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="typ">Theta2</span><span class="pun">(:,</span><span class="lit">2</span><span class="pun">:</span><span class="kwd">end</span><span class="pun">);</span></li><li class="L3"><span class="pln">regularize&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;</span><span class="lit">2</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;m&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">trace</span><span class="pun">(</span><span class="pln">t1&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;t1</span><span class="str">')&nbsp;+&nbsp;trace(t2&nbsp;*&nbsp;t2'</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">);</span></li><li class="L4"><span class="pln">J&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;J&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;regularize</span><span class="pun">;</span></li></ol></pre>
<p style="text-indent: 2em;">nn_params其实是个列向量：</p>
<p style="text-align:center"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i8cd9ukgj20ba0f6ab3.jpg" title="hankcs.com 2016-11-05 下午5.13.06.png" alt="hankcs.com 2016-11-05 下午5.13.06.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">包含了两个权值矩阵的全部元素（也许这么做是为了效率考虑），所以需要reshape还原成两个矩阵。</p>
<p style="text-indent: 2em;">然后完成前向传播：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="pln">a_1&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">ones</span><span class="pun">(</span><span class="pln">m</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">)</span><span class="pln">&nbsp;X</span><span class="pun">];%</span><span class="pln">add&nbsp;</span><span class="pun">+</span><span class="lit">1</span><span class="pln">&nbsp;to&nbsp;X</span><span class="pun">;</span></li><li class="L1"><span class="pln">z_2&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;sigmoid</span><span class="pun">(</span><span class="pln">a_1&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="typ">Theta1</span><span class="str">');</span></li><li class="L2"><span class="str">a_2&nbsp;=&nbsp;[ones(m,&nbsp;1)&nbsp;z_2];</span></li><li class="L3"><span class="str">a_3&nbsp;=&nbsp;sigmoid(a_2&nbsp;*&nbsp;Theta2'</span><span class="pun">);</span></li></ol></pre>
<p style="text-indent: 2em;">完成label的one-hot编码：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="pln">Y&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;zeros</span><span class="pun">(</span><span class="pln">m</span><span class="pun">,</span><span class="pln">num_labels</span><span class="pun">);</span></li><li class="L1"><span class="kwd">for</span><span class="pln">&nbsp;i&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="pun">:</span><span class="pln">&nbsp;m</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;Y</span><span class="pun">(</span><span class="pln">i</span><span class="pun">,</span><span class="pln">y</span><span class="pun">(</span><span class="pln">i</span><span class="pun">))</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">;</span></li><li class="L3"><span class="kwd">end</span></li></ol></pre>
<p style="text-indent: 2em;">然后计算非正规化的部分：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="pln">J&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;m&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;trace</span><span class="pun">(-</span><span class="pln">&nbsp;Y&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;log</span><span class="pun">(</span><span class="pln">a_3</span><span class="pun">)</span><span class="str">'&nbsp;-&nbsp;(1&nbsp;-&nbsp;Y)&nbsp;*&nbsp;log(1&nbsp;-&nbsp;a_3)'</span><span class="pun">);</span></li></ol></pre>
<p style="text-indent: 2em;">这里的trace表示计算一个方阵的对角线元素之和。由于Y和a都是m*K的矩阵，所以括号里的结果是一个m*m的方阵，然而根据</p>
<p style="text-align:center"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i87vg2foj20wg072myy.jpg" title="hankcs.com 2016-11-05 下午4.59.01.png" alt="hankcs.com 2016-11-05 下午4.59.01.png" width="524" height="114" border="0" vspace="0" style="text-align: center; white-space: normal; width: 524px; height: 114px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">求和方括号一共有m*K项。但是对y这个K*1的列向量来讲，只有一个元素是1，其他都是0，所以最终求和只有m项。究竟是方阵中哪m个元素呢？有效的只有i相等的元素，也就是对角线元素。其他元素都是无效的，是为了代码简洁而浪费的牺牲品。同样的牺牲品还有下面的</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="pln">trace</span><span class="pun">(</span><span class="pln">t1&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;t1</span><span class="str">')</span></li></ol></pre>
<p style="text-indent: 2em;">明明只想计算t1元素的平方和的，为了优雅非要使用矩阵运算。</p>
<p style="text-indent: 2em;">不得不说学术界永远视简洁优雅高于效率内存，好看至上。</p>
<h3 id="h3-4">sigmoid梯度<br></h3>
<p style="text-indent: 2em;">对激活函数<img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i8r40yzij20di02aq31.jpg" title="hankcs.com 2016-11-05 下午10.46.15.png" alt="hankcs.com 2016-11-05 下午10.46.15.png" width="249" height="42" border="0" vspace="0" style="width: 249px; height: 42px;" data-tag="bdshare">求导：</p>
<p style="text-align:center"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i8ql28yvj20ek02qgls.jpg" title="hankcs.com 2016-11-05 下午10.45.44.png" alt="hankcs.com 2016-11-05 下午10.45.44.png" width="257" height="48" border="0" vspace="0" style="width: 257px; height: 48px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">实现如下：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="kwd">function</span><span class="pln">&nbsp;g&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;sigmoidGradient</span><span class="pun">(</span><span class="pln">z</span><span class="pun">)</span></li><li class="L1"><span class="pun">%</span><span class="pln">SIGMOIDGRADIENT&nbsp;returns&nbsp;the&nbsp;gradient&nbsp;of&nbsp;the&nbsp;sigmoid&nbsp;</span><span class="kwd">function</span></li><li class="L2"><span class="pun">%</span><span class="pln">evaluated&nbsp;at&nbsp;z</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;g&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;SIGMOIDGRADIENT</span><span class="pun">(</span><span class="pln">z</span><span class="pun">)</span><span class="pln">&nbsp;computes&nbsp;the&nbsp;gradient&nbsp;of&nbsp;the&nbsp;sigmoid&nbsp;</span><span class="kwd">function</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;evaluated&nbsp;at&nbsp;z</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">This</span><span class="pln">&nbsp;should&nbsp;work&nbsp;regardless&nbsp;</span><span class="kwd">if</span><span class="pln">&nbsp;z&nbsp;</span><span class="kwd">is</span><span class="pln">&nbsp;a&nbsp;matrix&nbsp;</span><span class="kwd">or</span><span class="pln">&nbsp;a</span></li><li class="L5"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;vector</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">In</span><span class="pln">&nbsp;particular</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">if</span><span class="pln">&nbsp;z&nbsp;</span><span class="kwd">is</span><span class="pln">&nbsp;a&nbsp;vector&nbsp;</span><span class="kwd">or</span><span class="pln">&nbsp;matrix</span><span class="pun">,</span><span class="pln">&nbsp;you&nbsp;should&nbsp;</span><span class="kwd">return</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;the&nbsp;gradient&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;each&nbsp;element</span><span class="pun">.</span></li><li class="L7"><span class="pln">&nbsp;</span></li><li class="L8"><span class="pln">g&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;zeros</span><span class="pun">(</span><span class="pln">size</span><span class="pun">(</span><span class="pln">z</span><span class="pun">));</span></li><li class="L9"><span class="pln">&nbsp;</span></li><li class="L0"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="pun">======================</span><span class="pln">&nbsp;YOUR&nbsp;CODE&nbsp;HERE&nbsp;</span><span class="pun">======================</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Instructions</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">Compute</span><span class="pln">&nbsp;the&nbsp;gradient&nbsp;of&nbsp;the&nbsp;sigmoid&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;evaluated&nbsp;at</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;each&nbsp;value&nbsp;of&nbsp;z&nbsp;</span><span class="pun">(</span><span class="pln">z&nbsp;can&nbsp;be&nbsp;a&nbsp;matrix</span><span class="pun">,</span><span class="pln">&nbsp;vector&nbsp;</span><span class="kwd">or</span><span class="pln">&nbsp;scalar</span><span class="pun">).</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pln">g&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;sigmoid</span><span class="pun">(</span><span class="pln">z</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">.*</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;sigmoid</span><span class="pun">(</span><span class="pln">z</span><span class="pun">));</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="pun">=============================================================</span></li><li class="L7"><span class="kwd">end</span></li></ol></pre>
<h3 id="h3-5">随机初始化参数</h3>
<p style="text-indent: 2em;">要将参数进行随机初始化，而不是全部置为0 。如果所有参数都用相同的值作为初始值，那么所有隐藏层单元最终会得到与输入值有关的、相同的函数（也就是说，对于所有 i，<img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9ir1xg1izj201100s3y9.jpg" title="82d79561e2994ccba3e4fe2cc4d527e5.png" alt="82d79561e2994ccba3e4fe2cc4d527e5.png" width="27" height="20" border="0" vspace="0" style="width: 27px; height: 20px;" data-tag="bdshare">都会取相同的值，那么对于任何输入 x都会有<img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9ir2nm5v7j205800qjr6.jpg" title="3aa55fedae234afd387a314144cd6b32.png" alt="3aa55fedae234afd387a314144cd6b32.png" width="159" height="22" border="0" vspace="0" style="width: 159px; height: 22px;" data-tag="bdshare">）。随机初始化的目的是使对称失效（symmetry breaking）。</p>
<p style="text-indent: 2em;">随机初始化可不是单纯地取随机数那么简单，里面大有学问。<span style="text-indent: 2em;">好的随机初始化算法可以加快训练速度，提高训练效果。其中一种经典的方法是，取一个常数</span><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i8u0j1ttj201w014a9u.jpg" title="hankcs.com 2016-11-05 下午7.03.49.png" alt="hankcs.com 2016-11-05 下午7.03.49.png" width="34" height="20" border="0" vspace="0" style="text-indent: 2em; width: 34px; height: 20px;" data-tag="bdshare"><span style="text-indent: 2em;">，随机生成</span><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i8u8vsn8j205e01kwec.jpg" title="hankcs.com 2016-11-05 下午7.04.17.png" alt="hankcs.com 2016-11-05 下午7.04.17.png" width="90" height="26" border="0" vspace="0" style="text-indent: 2em; width: 90px; height: 26px;" data-tag="bdshare"><span style="text-indent: 2em;">之间的均匀分布。</span></p>
<p style="text-indent: 2em;">而常数<img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i8u0j1ttj201w014a9u.jpg" title="hankcs.com 2016-11-05 下午7.03.49.png" alt="hankcs.com 2016-11-05 下午7.03.49.png" width="34" height="20" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 34px; height: 20px;" data-tag="bdshare">的选取也有学问，经验法则是取</p>
<p style="text-align:center"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i8vd34vhj20j001imxc.jpg" title="hankcs.com 2016-11-05 下午7.05.20.png" alt="hankcs.com 2016-11-05 下午7.05.20.png" width="431" height="34" border="0" vspace="0" style="width: 431px; height: 34px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">上面的in和out分别代表某一层的入度和出度，可以看出，讲究的话每一层的<img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i8u0j1ttj201w014a9u.jpg" title="hankcs.com 2016-11-05 下午7.03.49.png" alt="hankcs.com 2016-11-05 下午7.03.49.png" width="34" height="20" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 34px; height: 20px;" data-tag="bdshare">都可能是不同的。</p>
<h2 id="h2-6">反向传播<br></h2>
<p style="text-indent: 2em;">反向传播算法从输出层反向朝着输入层传播的是什么？是误差（包括<a href="http://ufldl.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95" target="_blank" rel="external nofollow">斯坦福wiki</a>在内的一些资料也译作残差）。</p>
<p style="text-indent: 2em;">1、定义输入层的值<img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i8xvrzahj201i01e3ya.jpg" title="hankcs.com 2016-11-05 下午7.19.42.png" alt="hankcs.com 2016-11-05 下午7.19.42.png" width="25" height="23" border="0" vspace="0" style="width: 25px; height: 23px;" data-tag="bdshare">为特征，利用前向传播计算每一层的输入和输出<img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i8zn8gewj208q01edft.jpg" title="hankcs.com 2016-11-05 下午7.21.36.png" alt="hankcs.com 2016-11-05 下午7.21.36.png" width="158" height="25" border="0" vspace="0" style="width: 158px; height: 25px;" data-tag="bdshare">。注意输入层和隐藏层都必须设置一个+1偏置神经元。</p>
<p style="text-indent: 2em;">2、对输出层的每个神经元k，其误差是</p>
<p style="text-align:center"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i90zrvfwj208e01ujrc.jpg" title="hankcs.com 2016-11-05 下午7.23.46.png" alt="hankcs.com 2016-11-05 下午7.23.46.png" width="137" height="30" border="0" vspace="0" style="width: 137px; height: 30px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">其推导方式如下：</p>
<p style="text-align:center"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9ir7uhqvuj20dh04aaah.jpg" title="0b057858cd01020adb2c41cd8a586049.png" alt="0b057858cd01020adb2c41cd8a586049.png" width="422" height="134" border="0" vspace="0" style="width: 422px; height: 134px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">这里多了个f'，是因为额外进行了一次“sigmoid激活”，而我们直接输出了z。</p>
<p style="text-indent: 2em;">3、隐藏层反向传播公式</p>
<p style="text-align:center"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9i94tct6mj20cs0200su.jpg" title="hankcs.com 2016-11-05 下午7.34.31.png" alt="hankcs.com 2016-11-05 下午7.34.31.png" width="192" height="30" border="0" vspace="0" style="width: 192px; height: 30px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">这个式子的推导如下：</p>
<p style="text-align:center"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9irdlwwxxj20kp0abwfx.jpg" title="701c8dc8dbd71013c6a4110a1cb4f6f7.png" alt="701c8dc8dbd71013c6a4110a1cb4f6f7.png" width="602" height="300" border="0" vspace="0" style="width: 602px; height: 300px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">将上式中的<img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9iregjs8pj201e00i0nb.jpg" title="4f6cfb751715090b0493154e4b912097.png" alt="4f6cfb751715090b0493154e4b912097.png" data-tag="bdshare">与<img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9irez8na7j200g00d0e4.jpg" title="5b7a0657fdea25f29866c8e1d6e884ac.png" alt="5b7a0657fdea25f29866c8e1d6e884ac.png" data-tag="bdshare">的关系替换为l-1与l的关系，就可以得到：</p>
<p style="text-align:center"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9irfnqnfyj206z01p3ye.jpg" title="20f9979d6a46e7bca83f217bdfead4f0.png" alt="20f9979d6a46e7bca83f217bdfead4f0.png" width="222" height="54" border="0" vspace="0" style="width: 222px; height: 54px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">以上逐次从后向前求导的过程即为“反向传播”的本意所在。</p>
<p style="text-indent: 2em;">4、上式对参数求导，得到偏导数：</p>
<p style="text-align:center"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9irlz97loj20eu03gjre.jpg" title="hankcs.com 2016-11-06 上午8.38.45.png" alt="hankcs.com 2016-11-06 上午8.38.45.png" width="233" height="54" border="0" vspace="0" style="width: 233px; height: 54px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">我们把偏置b也放进了W矩阵，所以上式等同于：</p>
<p style="text-align:center"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9irn740rpj205a01mjr9.jpg" title="hankcs.com 2016-11-06 上午8.39.36.png" alt="hankcs.com 2016-11-06 上午8.39.36.png" width="91" height="28" border="0" vspace="0" style="width: 91px; height: 28px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;"></p>
<p style="text-indent: 2em;">考虑到一共有m个训练实例，上式是其总体残差，平均残差为其1/m：</p>
<p style="text-align:center"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9irrh2eh7j20cs03sjrj.jpg" title="hankcs.com 2016-11-06 上午8.44.00.png" alt="hankcs.com 2016-11-06 上午8.44.00.png" width="223" height="66" border="0" vspace="0" style="width: 223px; height: 66px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">实现如下</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Part</span><span class="pln">&nbsp;</span><span class="lit">2</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">Implement</span><span class="pln">&nbsp;the&nbsp;backpropagation&nbsp;algorithm&nbsp;to&nbsp;compute&nbsp;the&nbsp;gradients</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="typ">Theta1_grad</span><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;</span><span class="typ">Theta2_grad</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;should&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;the&nbsp;</span><span class="kwd">partial</span><span class="pln">&nbsp;derivatives&nbsp;of</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;cost&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;respect&nbsp;to&nbsp;</span><span class="typ">Theta1</span><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;</span><span class="typ">Theta2</span><span class="pln">&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;</span><span class="typ">Theta1_grad</span><span class="pln">&nbsp;</span><span class="kwd">and</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="typ">Theta2_grad</span><span class="pun">,</span><span class="pln">&nbsp;respectively</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">After</span><span class="pln">&nbsp;implementing&nbsp;</span><span class="typ">Part</span><span class="pln">&nbsp;</span><span class="lit">2</span><span class="pun">,</span><span class="pln">&nbsp;you&nbsp;can&nbsp;check</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;your&nbsp;implementation&nbsp;</span><span class="kwd">is</span><span class="pln">&nbsp;correct&nbsp;</span><span class="kwd">by</span><span class="pln">&nbsp;running&nbsp;checkNNGradients</span></li><li class="L5"><span class="pun">%</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="typ">Note</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">The</span><span class="pln">&nbsp;vector&nbsp;y&nbsp;passed&nbsp;</span><span class="kwd">into</span><span class="pln">&nbsp;the&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;</span><span class="kwd">is</span><span class="pln">&nbsp;a&nbsp;vector&nbsp;of&nbsp;labels</span></li><li class="L7"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;containing&nbsp;values&nbsp;</span><span class="kwd">from</span><span class="pln">&nbsp;</span><span class="lit">1.</span><span class="pun">.</span><span class="pln">K</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;need&nbsp;to&nbsp;map&nbsp;</span><span class="kwd">this</span><span class="pln">&nbsp;vector&nbsp;</span><span class="kwd">into</span><span class="pln">&nbsp;a&nbsp;</span></li><li class="L8"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;binary&nbsp;vector&nbsp;of&nbsp;</span><span class="lit">1</span><span class="str">'s&nbsp;and&nbsp;0'</span><span class="pln">s&nbsp;to&nbsp;be&nbsp;used&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;the&nbsp;neural&nbsp;network</span></li><li class="L9"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cost&nbsp;</span><span class="kwd">function</span><span class="pun">.</span></li><li class="L0"><span class="pun">%</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="typ">Hint</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">We</span><span class="pln">&nbsp;recommend&nbsp;implementing&nbsp;backpropagation&nbsp;</span><span class="kwd">using</span><span class="pln">&nbsp;a&nbsp;</span><span class="kwd">for</span><span class="pun">-</span><span class="pln">loop</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;over&nbsp;the&nbsp;training&nbsp;examples&nbsp;</span><span class="kwd">if</span><span class="pln">&nbsp;you&nbsp;are&nbsp;implementing&nbsp;it&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;the&nbsp;</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;time</span><span class="pun">.</span></li><li class="L4"><span class="pun">%</span></li><li class="L5"><span class="pln">delta_3&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;a_3&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;Y</span><span class="pun">;</span></li><li class="L6"><span class="pun">%</span><span class="pln">delta_2&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;delta_3&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="typ">Theta2</span><span class="pun">;</span></li><li class="L7"><span class="pun">%</span><span class="pln">delta_2&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;delta_2</span><span class="pun">(:,</span><span class="lit">2</span><span class="pun">:</span><span class="kwd">end</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">.*</span><span class="pln">&nbsp;sigmoidGradient</span><span class="pun">(</span><span class="pln">z_2</span><span class="pun">);</span></li><li class="L8"><span class="pln">delta_2&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;delta_3&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="typ">Theta2</span><span class="pln">&nbsp;</span><span class="pun">.*</span><span class="pln">&nbsp;a_2&nbsp;</span><span class="pun">.*</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;a_2</span><span class="pun">);</span></li><li class="L9"><span class="pln">delta_2&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;delta_2</span><span class="pun">(:,</span><span class="lit">2</span><span class="pun">:</span><span class="kwd">end</span><span class="pun">);</span></li><li class="L0"><span class="typ">Theta1_grad</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;delta_2</span><span class="str">'&nbsp;*&nbsp;a_1&nbsp;/&nbsp;m;</span></li><li class="L1"><span class="str">Theta2_grad&nbsp;=&nbsp;delta_3'</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;a_2&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;m</span><span class="pun">;</span></li></ol></pre>
<p style="text-indent: 2em;">推导很复杂，代码倒是很简单，不过寥寥数行。</p>
<h3 id="h3-7">校验梯度</h3>
<p style="text-indent: 2em;">怎么知道代码到底有没有写错呢？把参数reshape到一个长长的向量中，每次只轻微改变其中一个元素，应该只有该元素对应的<span style="text-indent: 32px;">偏导数会轻微变化。</span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">假设我们实现的梯度函数为<img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9isi9lyywj202g01it8i.jpg" title="hankcs.com 2016-11-06 上午9.09.50.png" alt="hankcs.com 2016-11-06 上午9.09.50.png" width="39" height="24" border="0" vspace="0" style="width: 39px; height: 24px;" data-tag="bdshare">，理想情况它应该输出<img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9isini866j203c01w3yc.jpg" title="hankcs.com 2016-11-06 上午9.10.15.png" alt="hankcs.com 2016-11-06 上午9.10.15.png" width="67" height="38" border="0" vspace="0" style="width: 67px; height: 38px;" data-tag="bdshare">。定义</span></p>
<p style="text-align:center"><span style="text-indent: 32px;"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9isj91ayoj20lk08qdga.jpg" title="hankcs.com 2016-11-06 上午9.10.46.png" alt="hankcs.com 2016-11-06 上午9.10.46.png" width="405" height="164" border="0" vspace="0" style="width: 405px; height: 164px;" data-tag="bdshare"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">两者与theta相比都只有第i个元素有少量变化，如果<img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9isi9lyywj202g01it8i.jpg" title="hankcs.com 2016-11-06 上午9.09.50.png" alt="hankcs.com 2016-11-06 上午9.09.50.png" width="39" height="24" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 39px; height: 24px;" data-tag="bdshare">真的没错的话，那么可以期待</span></p>
<p style="text-align:center"><span style="text-indent: 32px;"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9iso3mf5sj20bw0323yl.jpg" title="hankcs.com 2016-11-06 上午9.15.23.png" alt="hankcs.com 2016-11-06 上午9.15.23.png" width="195" height="50" border="0" vspace="0" style="width: 195px; height: 50px;" data-tag="bdshare"></span></p>
<p style="text-indent: 2em;">实现如下，等式右边：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="kwd">function</span><span class="pln">&nbsp;numgrad&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;computeNumericalGradient</span><span class="pun">(</span><span class="pln">J</span><span class="pun">,</span><span class="pln">&nbsp;theta</span><span class="pun">)</span></li><li class="L1"><span class="pun">%</span><span class="pln">COMPUTENUMERICALGRADIENT&nbsp;</span><span class="typ">Computes</span><span class="pln">&nbsp;the&nbsp;gradient&nbsp;</span><span class="kwd">using</span><span class="pln">&nbsp;</span><span class="str">"finite&nbsp;differences"</span></li><li class="L2"><span class="pun">%</span><span class="kwd">and</span><span class="pln">&nbsp;gives&nbsp;us&nbsp;a&nbsp;numerical&nbsp;estimate&nbsp;of&nbsp;the&nbsp;gradient</span><span class="pun">.</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;numgrad&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;COMPUTENUMERICALGRADIENT</span><span class="pun">(</span><span class="pln">J</span><span class="pun">,</span><span class="pln">&nbsp;theta</span><span class="pun">)</span><span class="pln">&nbsp;computes&nbsp;the&nbsp;numerical</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;gradient&nbsp;of&nbsp;the&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;J&nbsp;around&nbsp;theta</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">Calling</span><span class="pln">&nbsp;y&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;J</span><span class="pun">(</span><span class="pln">theta</span><span class="pun">)</span><span class="pln">&nbsp;should</span></li><li class="L5"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;the&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;value&nbsp;at&nbsp;theta</span><span class="pun">.</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Notes</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">The</span><span class="pln">&nbsp;following&nbsp;code&nbsp;</span><span class="kwd">implements</span><span class="pln">&nbsp;numerical&nbsp;gradient&nbsp;checking</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;</span></li><li class="L8"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;returns&nbsp;the&nbsp;numerical&nbsp;gradient</span><span class="pun">.</span><span class="typ">It</span><span class="pln">&nbsp;sets&nbsp;numgrad</span><span class="pun">(</span><span class="pln">i</span><span class="pun">)</span><span class="pln">&nbsp;to&nbsp;</span><span class="pun">(</span><span class="pln">a&nbsp;numerical&nbsp;</span></li><li class="L9"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;approximation&nbsp;of</span><span class="pun">)</span><span class="pln">&nbsp;the&nbsp;</span><span class="kwd">partial</span><span class="pln">&nbsp;derivative&nbsp;of&nbsp;J&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;respect&nbsp;to&nbsp;the&nbsp;</span></li><li class="L0"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i</span><span class="pun">-</span><span class="pln">th&nbsp;input&nbsp;argument</span><span class="pun">,</span><span class="pln">&nbsp;evaluated&nbsp;at&nbsp;theta</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">i</span><span class="pun">.</span><span class="pln">e</span><span class="pun">.,</span><span class="pln">&nbsp;numgrad</span><span class="pun">(</span><span class="pln">i</span><span class="pun">)</span><span class="pln">&nbsp;should&nbsp;</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;the&nbsp;</span><span class="pun">(</span><span class="pln">approximately</span><span class="pun">)</span><span class="pln">&nbsp;the&nbsp;</span><span class="kwd">partial</span><span class="pln">&nbsp;derivative&nbsp;of&nbsp;J&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;respect&nbsp;</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;theta</span><span class="pun">(</span><span class="pln">i</span><span class="pun">).)</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></li><li class="L4"><span class="pln">&nbsp;</span></li><li class="L5"><span class="pln">numgrad&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;zeros</span><span class="pun">(</span><span class="pln">size</span><span class="pun">(</span><span class="pln">theta</span><span class="pun">));</span></li><li class="L6"><span class="pln">perturb&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;zeros</span><span class="pun">(</span><span class="pln">size</span><span class="pun">(</span><span class="pln">theta</span><span class="pun">));</span></li><li class="L7"><span class="pln">e&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1e-4</span><span class="pun">;</span></li><li class="L8"><span class="kwd">for</span><span class="pln">&nbsp;p&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">:</span><span class="pln">numel</span><span class="pun">(</span><span class="pln">theta</span><span class="pun">)</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Set</span><span class="pln">&nbsp;perturbation&nbsp;vector</span></li><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;perturb</span><span class="pun">(</span><span class="pln">p</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;e</span><span class="pun">;</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;loss1&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;J</span><span class="pun">(</span><span class="pln">theta&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;perturb</span><span class="pun">);</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;loss2&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;J</span><span class="pun">(</span><span class="pln">theta&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;perturb</span><span class="pun">);</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Compute</span><span class="pln">&nbsp;</span><span class="typ">Numerical</span><span class="pln">&nbsp;</span><span class="typ">Gradient</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;numgrad</span><span class="pun">(</span><span class="pln">p</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">loss2&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;loss1</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="lit">2</span><span class="pun">*</span><span class="pln">e</span><span class="pun">);</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;perturb</span><span class="pun">(</span><span class="pln">p</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pun">;</span></li><li class="L6"><span class="kwd">end</span></li><li class="L7"><span class="pln">&nbsp;</span></li><li class="L8"><span class="kwd">end</span></li></ol></pre>
<h3 id="h3-8"><span style="text-indent: 32px;"></span>正则化神经网络的梯度<br></h3>
<p style="text-indent: 2em;">在原梯度上加上正则化项的梯度即可，对于j=0代表bias，不做改动，否则：</p>
<p style="text-align:center"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9ityig1rij20n403qdgd.jpg" title="hankcs.com 2016-11-06 上午9.59.43.png" alt="hankcs.com 2016-11-06 上午9.59.43.png" width="335" height="54" border="0" vspace="0" style="width: 335px; height: 54px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">这部分代码对应于：</span></p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Part</span><span class="pln">&nbsp;</span><span class="lit">3</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">Implement</span><span class="pln">&nbsp;regularization&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;the&nbsp;cost&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;gradients</span><span class="pun">.</span></li><li class="L1"><span class="pun">%</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="typ">Hint</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;can&nbsp;implement&nbsp;</span><span class="kwd">this</span><span class="pln">&nbsp;around&nbsp;the&nbsp;code&nbsp;</span><span class="kwd">for</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;backpropagation</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">That</span><span class="pln">&nbsp;</span><span class="kwd">is</span><span class="pun">,</span><span class="pln">&nbsp;you&nbsp;can&nbsp;compute&nbsp;the&nbsp;gradients&nbsp;</span><span class="kwd">for</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;regularization&nbsp;separately&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;</span><span class="kwd">then</span><span class="pln">&nbsp;add&nbsp;them&nbsp;to&nbsp;</span><span class="typ">Theta1_grad</span></li><li class="L5"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;</span><span class="typ">Theta2_grad</span><span class="pln">&nbsp;</span><span class="kwd">from</span><span class="pln">&nbsp;</span><span class="typ">Part</span><span class="pln">&nbsp;</span><span class="lit">2.</span></li><li class="L6"><span class="pun">%</span></li><li class="L7"><span class="pln">t1&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">zeros</span><span class="pun">(</span><span class="pln">size</span><span class="pun">(</span><span class="pln">t1</span><span class="pun">,</span><span class="lit">1</span><span class="pun">),</span><span class="lit">1</span><span class="pun">)</span><span class="pln">&nbsp;t1</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;m</span><span class="pun">;</span></li><li class="L8"><span class="pln">t2&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">zeros</span><span class="pun">(</span><span class="pln">size</span><span class="pun">(</span><span class="pln">t2</span><span class="pun">,</span><span class="lit">1</span><span class="pun">),</span><span class="lit">1</span><span class="pun">)</span><span class="pln">&nbsp;t2</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;m</span><span class="pun">;</span></li><li class="L9"><span class="typ">Theta1_grad</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="typ">Theta1_grad</span><span class="pln">&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;t1</span><span class="pun">;</span></li><li class="L0"><span class="typ">Theta2_grad</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="typ">Theta2_grad</span><span class="pln">&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;t2</span><span class="pun">;</span></li></ol></pre>
<h3 id="h3-9"><span style="text-indent: 32px;">使用fmincg学习参数</span></h3>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">没什么可说的，fmincg在上次练习中也用过。</span></p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="pun">%%</span><span class="pln">&nbsp;</span><span class="pun">===================</span><span class="pln">&nbsp;</span><span class="typ">Part</span><span class="pln">&nbsp;</span><span class="lit">8</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">Training</span><span class="pln">&nbsp;NN&nbsp;</span><span class="pun">===================</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;have&nbsp;now&nbsp;implemented&nbsp;all&nbsp;the&nbsp;code&nbsp;necessary&nbsp;to&nbsp;train&nbsp;a&nbsp;neural&nbsp;</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;network</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">To</span><span class="pln">&nbsp;train&nbsp;your&nbsp;neural&nbsp;network</span><span class="pun">,</span><span class="pln">&nbsp;we&nbsp;will&nbsp;now&nbsp;</span><span class="kwd">use</span><span class="pln">&nbsp;</span><span class="str">"fmincg"</span><span class="pun">,</span><span class="pln">&nbsp;which</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="kwd">is</span><span class="pln">&nbsp;a&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;which&nbsp;works&nbsp;similarly&nbsp;to&nbsp;</span><span class="str">"fminunc"</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">Recall</span><span class="pln">&nbsp;that&nbsp;these</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;advanced&nbsp;optimizers&nbsp;are&nbsp;able&nbsp;to&nbsp;train&nbsp;</span><span class="kwd">our</span><span class="pln">&nbsp;cost&nbsp;functions&nbsp;efficiently&nbsp;</span><span class="kwd">as</span></li><li class="L5"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="kwd">long</span><span class="pln">&nbsp;</span><span class="kwd">as</span><span class="pln">&nbsp;we&nbsp;provide&nbsp;them&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;the&nbsp;gradient&nbsp;computations</span><span class="pun">.</span></li><li class="L6"><span class="pun">%</span></li><li class="L7"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'\nTraining&nbsp;Neural&nbsp;Network...&nbsp;\n'</span><span class="pun">)</span></li><li class="L8"><span class="pln">&nbsp;</span></li><li class="L9"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="typ">After</span><span class="pln">&nbsp;you&nbsp;have&nbsp;completed&nbsp;the&nbsp;assignment</span><span class="pun">,</span><span class="pln">&nbsp;change&nbsp;the&nbsp;</span><span class="typ">MaxIter</span><span class="pln">&nbsp;to&nbsp;a&nbsp;larger</span></li><li class="L0"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;value&nbsp;to&nbsp;see&nbsp;how&nbsp;more&nbsp;training&nbsp;helps</span><span class="pun">.</span></li><li class="L1"><span class="pln">options&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;optimset</span><span class="pun">(</span><span class="str">'MaxIter'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">50</span><span class="pun">);</span></li><li class="L2"><span class="pln">&nbsp;</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;should&nbsp;also&nbsp;</span><span class="kwd">try</span><span class="pln">&nbsp;different&nbsp;values&nbsp;of&nbsp;</span><span class="kwd">lambda</span></li><li class="L4"><span class="kwd">lambda</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">;</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Create</span><span class="pln">&nbsp;</span><span class="str">"short&nbsp;hand"</span><span class="pln">&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;the&nbsp;cost&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;to&nbsp;be&nbsp;minimized</span></li><li class="L7"><span class="pln">costFunction&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">@(</span><span class="pln">p</span><span class="pun">)</span><span class="pln">&nbsp;nnCostFunction</span><span class="pun">(</span><span class="pln">p</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;input_layer_size</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hidden_layer_size</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_labels</span><span class="pun">,</span><span class="pln">&nbsp;X</span><span class="pun">,</span><span class="pln">&nbsp;y</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">lambda</span><span class="pun">);</span></li><li class="L1"><span class="pln">&nbsp;</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Now</span><span class="pun">,</span><span class="pln">&nbsp;costFunction&nbsp;</span><span class="kwd">is</span><span class="pln">&nbsp;a&nbsp;</span><span class="kwd">function</span><span class="pln">&nbsp;that&nbsp;takes&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;only&nbsp;one&nbsp;argument&nbsp;</span><span class="pun">(</span><span class="pln">the</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;neural&nbsp;network&nbsp;parameters</span><span class="pun">)</span></li><li class="L4"><span class="pun">[</span><span class="pln">nn_params</span><span class="pun">,</span><span class="pln">&nbsp;cost</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;fmincg</span><span class="pun">(</span><span class="pln">costFunction</span><span class="pun">,</span><span class="pln">&nbsp;initial_nn_params</span><span class="pun">,</span><span class="pln">&nbsp;options</span><span class="pun">);</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Obtain</span><span class="pln">&nbsp;</span><span class="typ">Theta1</span><span class="pln">&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;</span><span class="typ">Theta2</span><span class="pln">&nbsp;back&nbsp;</span><span class="kwd">from</span><span class="pln">&nbsp;nn_params</span></li><li class="L7"><span class="typ">Theta1</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;reshape</span><span class="pun">(</span><span class="pln">nn_params</span><span class="pun">(</span><span class="lit">1</span><span class="pun">:</span><span class="pln">hidden_layer_size&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">input_layer_size&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">)),</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hidden_layer_size</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">input_layer_size&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">));</span></li><li class="L9"><span class="pln">&nbsp;</span></li><li class="L0"><span class="typ">Theta2</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;reshape</span><span class="pun">(</span><span class="pln">nn_params</span><span class="pun">((</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">hidden_layer_size&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">input_layer_size&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">))):</span><span class="kwd">end</span><span class="pun">),</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_labels</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">hidden_layer_size&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">));</span></li><li class="L2"><span class="pln">&nbsp;</span></li><li class="L3"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'Program&nbsp;paused.&nbsp;Press&nbsp;enter&nbsp;to&nbsp;continue.\n'</span><span class="pun">);</span></li><li class="L4"><span class="pln">pause</span><span class="pun">;</span></li></ol></pre>
<h2 id="h2-10"><span style="text-indent: 32px;"></span>可视化隐藏层<br></h2>
<p style="text-indent: 2em;">这是我最喜欢最期待的一部分，从来没见过这种玩法。</p>
<p style="text-indent: 2em;">某个隐藏层神经元究竟在捕捉什么信息？这个问题等效于，什么样的输入x会使它激活（输出近似1，越大越好）。通过找到这样的x，然后将其可视化出来即可。由a=g(z)，z=theta*x知，<span style="text-indent: 32px;">theta*x也要越大越好。事实上这个超平面是无限的，必须得有个约束，比如<img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9iv5l7t0zj203e01aa9v.jpg" title="hankcs.com 2016-11-06 上午10.41.25.png" alt="hankcs.com 2016-11-06 上午10.41.25.png" width="69" height="26" border="0" vspace="0" style="width: 69px; height: 26px;" data-tag="bdshare">。据说加了约束后，取x=<span style="text-indent: 32px;">theta总能得到最大值（具体为什么还不知道）。</span></span></p>
<p style="text-indent: 2em;">回忆下隐藏层的theta1每一行其实是一个401维的列向量，去掉bias项后为400维，代表一个20*20的图片，将这个图片显示出来即可。隐藏层一共25个节点，所以一共25张图片。</p>
<p style="text-indent: 2em;">实现如下：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="kwd">function</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">h</span><span class="pun">,</span><span class="pln">&nbsp;display_array</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;displayData</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;example_width</span><span class="pun">)</span></li><li class="L1"><span class="pun">%</span><span class="pln">DISPLAYDATA&nbsp;</span><span class="typ">Display</span><span class="pln">&nbsp;</span><span class="lit">2D</span><span class="pln">&nbsp;data&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;a&nbsp;nice&nbsp;grid</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="pun">[</span><span class="pln">h</span><span class="pun">,</span><span class="pln">&nbsp;display_array</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;DISPLAYDATA</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;example_width</span><span class="pun">)</span><span class="pln">&nbsp;displays&nbsp;</span><span class="lit">2D</span><span class="pln">&nbsp;data</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;stored&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;X&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;a&nbsp;nice&nbsp;grid</span><span class="pun">.</span><span class="pln">&nbsp;</span><span class="typ">It</span><span class="pln">&nbsp;returns&nbsp;the&nbsp;figure&nbsp;handle&nbsp;h&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;the&nbsp;</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;&nbsp;displayed&nbsp;array&nbsp;</span><span class="kwd">if</span><span class="pln">&nbsp;requested</span><span class="pun">.</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Set</span><span class="pln">&nbsp;example_width&nbsp;automatically&nbsp;</span><span class="kwd">if</span><span class="pln">&nbsp;</span><span class="kwd">not</span><span class="pln">&nbsp;passed&nbsp;</span><span class="kwd">in</span></li><li class="L7"><span class="kwd">if</span><span class="pln">&nbsp;</span><span class="pun">~</span><span class="pln">exist</span><span class="pun">(</span><span class="str">'example_width'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'var'</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">||</span><span class="pln">&nbsp;isempty</span><span class="pun">(</span><span class="pln">example_width</span><span class="pun">)</span><span class="pln">&nbsp;</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;example_width&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;round</span><span class="pun">(</span><span class="pln">sqrt</span><span class="pun">(</span><span class="pln">size</span><span class="pun">(</span><span class="pln">X</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">2</span><span class="pun">)));</span></li><li class="L9"><span class="kwd">end</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Gray</span><span class="pln">&nbsp;</span><span class="typ">Image</span></li><li class="L2"><span class="pln">colormap</span><span class="pun">(</span><span class="pln">gray</span><span class="pun">);</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Compute</span><span class="pln">&nbsp;rows</span><span class="pun">,</span><span class="pln">&nbsp;cols</span></li><li class="L5"><span class="pun">[</span><span class="pln">m&nbsp;n</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;size</span><span class="pun">(</span><span class="pln">X</span><span class="pun">);</span></li><li class="L6"><span class="pln">example_height&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">n&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;example_width</span><span class="pun">);</span></li><li class="L7"><span class="pln">&nbsp;</span></li><li class="L8"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Compute</span><span class="pln">&nbsp;number&nbsp;of&nbsp;items&nbsp;to&nbsp;display</span></li><li class="L9"><span class="pln">display_rows&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;floor</span><span class="pun">(</span><span class="pln">sqrt</span><span class="pun">(</span><span class="pln">m</span><span class="pun">));</span></li><li class="L0"><span class="pln">display_cols&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;ceil</span><span class="pun">(</span><span class="pln">m&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;display_rows</span><span class="pun">);</span></li><li class="L1"><span class="pln">&nbsp;</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Between</span><span class="pln">&nbsp;images&nbsp;padding</span></li><li class="L3"><span class="pln">pad&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">;</span></li><li class="L4"><span class="pln">&nbsp;</span></li><li class="L5"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Setup</span><span class="pln">&nbsp;blank&nbsp;display</span></li><li class="L6"><span class="pln">display_array&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;ones</span><span class="pun">(</span><span class="pln">pad&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;display_rows&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">example_height&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;pad</span><span class="pun">),</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pad&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;display_cols&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">example_width&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;pad</span><span class="pun">));</span></li><li class="L8"><span class="pln">&nbsp;</span></li><li class="L9"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Copy</span><span class="pln">&nbsp;each&nbsp;example&nbsp;</span><span class="kwd">into</span><span class="pln">&nbsp;a&nbsp;patch&nbsp;on&nbsp;the&nbsp;display&nbsp;array</span></li><li class="L0"><span class="pln">curr_ex&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">;</span></li><li class="L1"><span class="kwd">for</span><span class="pln">&nbsp;j&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">:</span><span class="pln">display_rows</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;i&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">:</span><span class="pln">display_cols</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">if</span><span class="pln">&nbsp;curr_ex&nbsp;</span><span class="pun">&gt;</span><span class="pln">&nbsp;m</span><span class="pun">,</span><span class="pln">&nbsp;</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">break</span><span class="pun">;</span><span class="pln">&nbsp;</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">end</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Copy</span><span class="pln">&nbsp;the&nbsp;patch</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Get</span><span class="pln">&nbsp;the&nbsp;max&nbsp;value&nbsp;of&nbsp;the&nbsp;patch</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_val&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;max</span><span class="pun">(</span><span class="pln">abs</span><span class="pun">(</span><span class="pln">X</span><span class="pun">(</span><span class="pln">curr_ex</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">:)));</span></li><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;display_array</span><span class="pun">(</span><span class="pln">pad&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">j&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">example_height&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;pad</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="lit">1</span><span class="pun">:</span><span class="pln">example_height</span><span class="pun">),</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pad&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">i&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">example_width&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;pad</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="lit">1</span><span class="pun">:</span><span class="pln">example_width</span><span class="pun">))</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">...</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reshape</span><span class="pun">(</span><span class="pln">X</span><span class="pun">(</span><span class="pln">curr_ex</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">:),</span><span class="pln">&nbsp;example_height</span><span class="pun">,</span><span class="pln">&nbsp;example_width</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;max_val</span><span class="pun">;</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;curr_ex&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;curr_ex&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">;</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">end</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">if</span><span class="pln">&nbsp;curr_ex&nbsp;</span><span class="pun">&gt;</span><span class="pln">&nbsp;m</span><span class="pun">,</span><span class="pln">&nbsp;</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">break</span><span class="pun">;</span><span class="pln">&nbsp;</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">end</span></li><li class="L8"><span class="kwd">end</span></li><li class="L9"><span class="pln">&nbsp;</span></li><li class="L0"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Display</span><span class="pln">&nbsp;</span><span class="typ">Image</span></li><li class="L1"><span class="pln">h&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;imagesc</span><span class="pun">(</span><span class="pln">display_array</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="pun">[-</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">]);</span></li><li class="L2"><span class="pln">&nbsp;</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;</span><span class="typ">Do</span><span class="pln">&nbsp;</span><span class="kwd">not</span><span class="pln">&nbsp;show&nbsp;axis</span></li><li class="L4"><span class="pln">axis&nbsp;image&nbsp;off</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="pln">drawnow</span><span class="pun">;</span></li><li class="L7"><span class="pln">&nbsp;</span></li><li class="L8"><span class="kwd">end</span></li></ol></pre>
<p style="text-indent: 2em;">其实代码跟上次可视化手写数字图片的并没有多大不同，只不过传入的参数X变成了theta。</p>
<p style="text-indent: 2em;">调用代码如下：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="pun">%%</span><span class="pln">&nbsp;</span><span class="pun">=================</span><span class="pln">&nbsp;</span><span class="typ">Part</span><span class="pln">&nbsp;</span><span class="lit">9</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="typ">Visualize</span><span class="pln">&nbsp;</span><span class="typ">Weights</span><span class="pln">&nbsp;</span><span class="pun">=================</span></li><li class="L1"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;</span><span class="typ">You</span><span class="pln">&nbsp;can&nbsp;now&nbsp;</span><span class="str">"visualize"</span><span class="pln">&nbsp;what&nbsp;the&nbsp;neural&nbsp;network&nbsp;</span><span class="kwd">is</span><span class="pln">&nbsp;learning&nbsp;</span><span class="kwd">by</span><span class="pln">&nbsp;</span></li><li class="L2"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;displaying&nbsp;the&nbsp;hidden&nbsp;units&nbsp;to&nbsp;see&nbsp;what&nbsp;features&nbsp;they&nbsp;are&nbsp;capturing&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;</span></li><li class="L3"><span class="pun">%</span><span class="pln">&nbsp;&nbsp;the&nbsp;data</span><span class="pun">.</span></li><li class="L4"><span class="pln">&nbsp;</span></li><li class="L5"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'\nVisualizing&nbsp;Neural&nbsp;Network...&nbsp;\n'</span><span class="pun">)</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="pln">displayData</span><span class="pun">(</span><span class="typ">Theta1</span><span class="pun">(:,</span><span class="pln">&nbsp;</span><span class="lit">2</span><span class="pun">:</span><span class="kwd">end</span><span class="pun">));</span></li><li class="L8"><span class="pln">&nbsp;</span></li><li class="L9"><span class="pln">fprintf</span><span class="pun">(</span><span class="str">'\nProgram&nbsp;paused.&nbsp;Press&nbsp;enter&nbsp;to&nbsp;continue.\n'</span><span class="pun">);</span></li><li class="L0"><span class="pln">pause</span><span class="pun">;</span></li></ol></pre>
<p style="text-indent: 2em;">输出：</p>
<p style="text-indent: 2em;"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9ivlbp2q3j20v40nc437.jpg" title="hidden.png" alt="hidden.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">的确多多少少可以看出每个节点在关注白色笔画的某种模式，而不关心图片边缘的像素。</p>
<p style="text-indent: 2em;">利用同样的思路可以把输出层的权值可视化出来：</p>
<pre class="prettyprint lang-as3 linenums"><ol class="linenums"><li class="L0"><span class="pln">figure</span><span class="pun">;</span></li><li class="L1"><span class="pln">displayData</span><span class="pun">(</span><span class="typ">Theta2</span><span class="pun">(:,</span><span class="pln">&nbsp;</span><span class="lit">2</span><span class="pun">:</span><span class="kwd">end</span><span class="pun">));</span></li></ol></pre>
<p style="text-indent: 2em;">得到</p>
<p style="text-indent: 2em;"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f9ivolc6pjj20v40nc3zi.jpg" title="output.png" alt="output.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">这10张图片对应着从1到0这10个数字，属于更抽象的表示，所以可视化其实意义不大，什么也看不出来。</p>
<h2 id="h2-11">Reference</h2>
<p style="text-indent: 2em;"><a href="http://www.hankcs.com/ml/back-propagation-neural-network.html" _src="http://www.hankcs.com/ml/back-propagation-neural-network.html" target="_blank" rel="external nofollow">http://www.hankcs.com/ml/back-propagation-neural-network.html</a></p>
<p style="text-indent: 2em;"><a href="http://ufldl.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95" _src="http://ufldl.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95" target="_blank" rel="external nofollow">http://ufldl.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95</a></p>
<p style="text-indent: 2em;"></p>
<p class="post-copyright"><a href="http://www.hankcs.com/license/" target="_blank"><img alt="知识共享许可协议" style="border-width: 0px;margin: 0 !important;" src="./CS229编程4：训练神经网络_files/88x31.png" width="88" height="31" border="0" vspace="0" title="知识共享许可协议" data-tag="bdshare"></a>&nbsp;<a href="http://www.hankcs.com/license/" target="_blank" textvalue="知识共享署名-非商业性使用-相同方式共享">知识共享署名-非商业性使用-相同方式共享</a>：<a href="http://www.hankcs.com/">码农场</a> » <a href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html">CS229编程4：训练神经网络</a></p>		</article>
								<div class="action-share bdsharebuttonbox bdshare-button-style0-24" data-bd-bind="1495386748381">
			<span>分享到：</span><a class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博"></a><a class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a><a class="bds_bdhome" data-cmd="bdhome" title="分享到百度新首页"></a><a class="bds_tqf" data-cmd="tqf" title="分享到腾讯朋友"></a><a class="bds_renren" data-cmd="renren" title="分享到人人网"></a><a class="bds_diandian" data-cmd="diandian" title="分享到点点网"></a><a class="bds_youdao" data-cmd="youdao" title="分享到有道云笔记"></a><a class="bds_ty" data-cmd="ty" title="分享到天涯社区"></a><a class="bds_kaixin001" data-cmd="kaixin001" title="分享到开心网"></a><a class="bds_taobao" data-cmd="taobao"></a><a class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a class="bds_mail" data-cmd="mail" title="分享到邮件分享"></a><a class="bds_copy" data-cmd="copy" title="分享到复制网址"></a><a class="bds_more" data-cmd="more">更多</a> <span>(</span><a class="bds_count" data-cmd="count" title="累计分享2次">2</a><span>)</span>		</div>
		<div class="article-tags">继续浏览有关 <a href="http://www.hankcs.com/ml/"><i class="fa fa-folder-open"></i> 机器学习</a><a href="http://www.hankcs.com/tag/cs229/" rel="tag">CS229</a><a href="http://www.hankcs.com/tag/matlab/" rel="tag">matlab</a> 的文章</div>		<div class="asb asb-post asb-post-02"><script async="" src="./CS229编程4：训练神经网络_files/adsbygoogle.js.下载"></script>
<!-- 文章页正文下 页首横幅 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-1152644711996772" data-ad-slot="2657945648" data-adsbygoogle-status="done"><ins id="aswift_1_expand" style="display:inline-table;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent"><ins id="aswift_1_anchor" style="display:block;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent"><iframe width="728" height="90" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_1" name="aswift_1" style="left:0;position:absolute;top:0;width:728px;height:90px;" src="./CS229编程4：训练神经网络_files/saved_resource(1).html"></iframe></ins></ins></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>		<nav class="article-nav">
			<span class="article-nav-prev">上一篇 <a href="http://www.hankcs.com/ml/multi-class-classification-and-neural-networks-cs229.html" rel="prev">CS229编程3：多分类和神经网络</a></span>
			<span class="article-nav-next"><a href="http://www.hankcs.com/ml/regularized-linear-regression-and-bias-variance-cs229.html" rel="next">CS229编程5：正则化线性回归与偏差方差权衡</a> 下一篇</span>
		</nav>
				<div class="asb asb-post asb-post-03"><script async="" src="./CS229编程4：训练神经网络_files/adsbygoogle.js.下载"></script>
<!-- 匹配内容 -->
<ins class="adsbygoogle" style="display: block; height: 466px;" data-ad-client="ca-pub-1152644711996772" data-ad-slot="7343699642" data-ad-format="autorelaxed" data-adsbygoogle-status="done"><ins id="aswift_2_expand" style="display:inline-table;border:none;height:466px;margin:0;padding:0;position:relative;visibility:visible;width:778px;background-color:transparent"><ins id="aswift_2_anchor" style="display:block;border:none;height:466px;margin:0;padding:0;position:relative;visibility:visible;width:778px;background-color:transparent"><iframe width="778" height="466" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_2" name="aswift_2" style="left:0;position:absolute;top:0;width:778px;height:466px;" src="./CS229编程4：训练神经网络_files/saved_resource(2).html"></iframe></ins></ins></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>		<div class="title" id="comments">
	<h3>评论 <small>欢迎留言</small></h3>
</div>
<div id="respond" class="no_webshot">
		
	<form action="http://www.hankcs.com/wp-comments-post.php" method="post" id="commentform">
		<div class="comt">
			<div class="comt-title">
				<img alt="" data-src="http://0.gravatar.com/avatar/?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://1.gravatar.com/avatar/?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo avatar-default" height="50" width="50" src="./CS229编程4：训练神经网络_files/avatar-default.png">				<p><a id="cancel-comment-reply-link" href="javascript:;">取消</a></p>
			</div>
			<div class="comt-box">
				<textarea placeholder="此处不受理任何开源项目问题，请在GitHub上发issue ，大家一起讨论，谢谢。" class="input-block-level comt-area" name="comment" id="comment" cols="100%" rows="3" tabindex="1" onkeydown="if(event.ctrlKey&amp;&amp;event.keyCode==13){document.getElementById(&#39;submit&#39;).click();return false};"></textarea>
				<div class="comt-ctrl">
					<div class="comt-tips"><input type="hidden" name="comment_post_ID" value="8354" id="comment_post_ID">
<input type="hidden" name="comment_parent" id="comment_parent" value="0">
<p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="6efe71f561"></p><label for="comment_mail_notify" class="checkbox inline hide" style="padding-top:0"><input type="checkbox" name="comment_mail_notify" id="comment_mail_notify" value="comment_mail_notify" checked="checked">有人回复时邮件通知我</label><p style="display: none;"></p><div class="comt-tip comt-loading" style="display: none;">评论提交中...</div><div class="comt-tip comt-error" style="display: none;">#</div></div>
					<button type="submit" name="submit" id="submit" tabindex="5">提交评论</button>
					<!-- <span data-type="comment-insert-smilie" class="muted comt-smilie"><i class="icon-thumbs-up icon12"></i> 表情</span> -->
				</div>
			</div>

												<div class="comt-comterinfo" id="comment-author-info">
						<ul>
							<li class="form-inline"><label class="hide" for="author">昵称</label><input class="ipt" type="text" name="author" id="author" value="" tabindex="2" placeholder="昵称"><span class="text-muted">昵称 (必填)</span></li>
							<li class="form-inline"><label class="hide" for="email">邮箱</label><input class="ipt" type="text" name="email" id="email" value="" tabindex="3" placeholder="邮箱"><span class="text-muted">邮箱 (必填)</span></li>
							<li class="form-inline"><label class="hide" for="url">网址</label><input class="ipt" type="text" name="url" id="url" value="" tabindex="4" placeholder="网址"><span class="text-muted">网址</span></li>
						</ul>
					</div>
									</div>

	<input type="hidden" id="ak_js" name="ak_js" value="1495386747793"></form>
	</div>
	</div>
	</div>
	<aside class="sidebar">
<div class="widget widget_categories affix-top" style="top: 0px;"><h3>栏目分类</h3><label class="screen-reader-text" for="cat">栏目分类</label><select name="cat" id="cat" class="postform">
	<option value="-1">选择分类目录</option>
	<option class="level-0" value="18">ACG&nbsp;&nbsp;(7)</option>
	<option class="level-1" value="117">&nbsp;&nbsp;&nbsp;游戏&nbsp;&nbsp;(5)</option>
	<option class="level-0" value="7">Web开发&nbsp;&nbsp;(80)</option>
	<option class="level-1" value="64">&nbsp;&nbsp;&nbsp;BAE&nbsp;&nbsp;(13)</option>
	<option class="level-1" value="11">&nbsp;&nbsp;&nbsp;Linux相关&nbsp;&nbsp;(9)</option>
	<option class="level-1" value="54">&nbsp;&nbsp;&nbsp;Mac OS&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="27">&nbsp;&nbsp;&nbsp;WordPress&nbsp;&nbsp;(8)</option>
	<option class="level-1" value="65">&nbsp;&nbsp;&nbsp;Yii&nbsp;&nbsp;(17)</option>
	<option class="level-1" value="2">&nbsp;&nbsp;&nbsp;主机域名&nbsp;&nbsp;(26)</option>
	<option class="level-1" value="66">&nbsp;&nbsp;&nbsp;数据库&nbsp;&nbsp;(4)</option>
	<option class="level-0" value="140">信息安全&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="1">其他类别&nbsp;&nbsp;(184)</option>
	<option class="level-1" value="78">&nbsp;&nbsp;&nbsp;心情&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="15">&nbsp;&nbsp;&nbsp;旧的博文&nbsp;&nbsp;(170)</option>
	<option class="level-0" value="87">操作系统&nbsp;&nbsp;(3)</option>
	<option class="level-1" value="88">&nbsp;&nbsp;&nbsp;Windows&nbsp;&nbsp;(2)</option>
	<option class="level-0" value="81">数学基礎&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="4">日语教程&nbsp;&nbsp;(120)</option>
	<option class="level-1" value="96">&nbsp;&nbsp;&nbsp;口译&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="59">&nbsp;&nbsp;&nbsp;新编日语商务贸易会话&nbsp;&nbsp;(14)</option>
	<option class="level-1" value="19">&nbsp;&nbsp;&nbsp;新编日语阅读文选&nbsp;&nbsp;(34)</option>
	<option class="level-2" value="44">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一册&nbsp;&nbsp;(20)</option>
	<option class="level-2" value="61">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第三册&nbsp;&nbsp;(2)</option>
	<option class="level-2" value="20">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第二册&nbsp;&nbsp;(10)</option>
	<option class="level-1" value="46">&nbsp;&nbsp;&nbsp;日语入门&nbsp;&nbsp;(2)</option>
	<option class="level-1" value="62">&nbsp;&nbsp;&nbsp;日语听力&nbsp;&nbsp;(2)</option>
	<option class="level-1" value="5">&nbsp;&nbsp;&nbsp;日语综合教程&nbsp;&nbsp;(64)</option>
	<option class="level-2" value="120">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第七册&nbsp;&nbsp;(14)</option>
	<option class="level-2" value="50">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第三册&nbsp;&nbsp;(7)</option>
	<option class="level-2" value="73">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第五册&nbsp;&nbsp;(12)</option>
	<option class="level-2" value="98">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第六册&nbsp;&nbsp;(18)</option>
	<option class="level-2" value="6">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第四册&nbsp;&nbsp;(12)</option>
	<option class="level-1" value="86">&nbsp;&nbsp;&nbsp;月の珊瑚&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="131">机器学习&nbsp;&nbsp;(46)</option>
	<option class="level-0" value="16">经济人文&nbsp;&nbsp;(19)</option>
	<option class="level-1" value="17">&nbsp;&nbsp;&nbsp;国际贸易理论与政策&nbsp;&nbsp;(9)</option>
	<option class="level-1" value="30">&nbsp;&nbsp;&nbsp;当代世界经济与政治&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="9">编程开发&nbsp;&nbsp;(555)</option>
	<option class="level-1" value="8">&nbsp;&nbsp;&nbsp;Android&nbsp;&nbsp;(30)</option>
	<option class="level-1" value="13">&nbsp;&nbsp;&nbsp;C++&nbsp;&nbsp;(236)</option>
	<option class="level-1" value="25">&nbsp;&nbsp;&nbsp;Drupal&nbsp;&nbsp;(23)</option>
	<option class="level-1" value="10">&nbsp;&nbsp;&nbsp;Java&nbsp;&nbsp;(69)</option>
	<option class="level-1" value="123">&nbsp;&nbsp;&nbsp;Javascript&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="24">&nbsp;&nbsp;&nbsp;PHP&nbsp;&nbsp;(57)</option>
	<option class="level-1" value="94">&nbsp;&nbsp;&nbsp;Python&nbsp;&nbsp;(8)</option>
	<option class="level-1" value="14">&nbsp;&nbsp;&nbsp;汇编逆向&nbsp;&nbsp;(12)</option>
	<option class="level-1" value="70">&nbsp;&nbsp;&nbsp;算法&nbsp;&nbsp;(235)</option>
	<option class="level-1" value="121">&nbsp;&nbsp;&nbsp;网络&nbsp;&nbsp;(6)</option>
	<option class="level-0" value="104">自然语言处理&nbsp;&nbsp;(54)</option>
	<option class="level-1" value="109">&nbsp;&nbsp;&nbsp;中文分词&nbsp;&nbsp;(10)</option>
	<option class="level-1" value="128">&nbsp;&nbsp;&nbsp;句法分析&nbsp;&nbsp;(5)</option>
	<option class="level-1" value="127">&nbsp;&nbsp;&nbsp;命名实体识别&nbsp;&nbsp;(6)</option>
	<option class="level-1" value="105">&nbsp;&nbsp;&nbsp;语料库&nbsp;&nbsp;(4)</option>
	<option class="level-0" value="12">软件发布&nbsp;&nbsp;(9)</option>
</select>

<script type="text/javascript">
/* <![CDATA[ */
(function() {
	var dropdown = document.getElementById( "cat" );
	function onCatChange() {
		if ( dropdown.options[ dropdown.selectedIndex ].value > 0 ) {
			location.href = "http://www.hankcs.com/?cat=" + dropdown.options[ dropdown.selectedIndex ].value;
		}
	}
	dropdown.onchange = onCatChange;
})();
/* ]]> */
</script>

</div><div class="widget widget_archive" style="top: 0px;"><h3>文章归档</h3>		<label class="screen-reader-text" for="archives-dropdown-5">文章归档</label>
		<select id="archives-dropdown-5" name="archive-dropdown" onchange="document.location.href=this.options[this.selectedIndex].value;">
			
			<option value="">选择月份</option>
				<option value="http://www.hankcs.com/2017/05/"> 2017年五月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2017/03/"> 2017年三月 &nbsp;(12)</option>
	<option value="http://www.hankcs.com/2017/02/"> 2017年二月 &nbsp;(13)</option>
	<option value="http://www.hankcs.com/2017/01/"> 2017年一月 &nbsp;(22)</option>
	<option value="http://www.hankcs.com/2016/12/"> 2016年十二月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2016/11/"> 2016年十一月 &nbsp;(15)</option>
	<option value="http://www.hankcs.com/2016/10/"> 2016年十月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2016/09/"> 2016年九月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2016/08/"> 2016年八月 &nbsp;(7)</option>
	<option value="http://www.hankcs.com/2016/07/"> 2016年七月 &nbsp;(1)</option>
	<option value="http://www.hankcs.com/2016/06/"> 2016年六月 &nbsp;(1)</option>
	<option value="http://www.hankcs.com/2016/05/"> 2016年五月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2016/04/"> 2016年四月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2016/03/"> 2016年三月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2016/02/"> 2016年二月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2015/12/"> 2015年十二月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2015/11/"> 2015年十一月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2015/10/"> 2015年十月 &nbsp;(4)</option>
	<option value="http://www.hankcs.com/2015/09/"> 2015年九月 &nbsp;(4)</option>
	<option value="http://www.hankcs.com/2015/08/"> 2015年八月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2015/07/"> 2015年七月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2015/05/"> 2015年五月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2015/04/"> 2015年四月 &nbsp;(5)</option>
	<option value="http://www.hankcs.com/2015/03/"> 2015年三月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2015/02/"> 2015年二月 &nbsp;(22)</option>
	<option value="http://www.hankcs.com/2015/01/"> 2015年一月 &nbsp;(14)</option>
	<option value="http://www.hankcs.com/2014/12/"> 2014年十二月 &nbsp;(10)</option>
	<option value="http://www.hankcs.com/2014/11/"> 2014年十一月 &nbsp;(21)</option>
	<option value="http://www.hankcs.com/2014/10/"> 2014年十月 &nbsp;(14)</option>
	<option value="http://www.hankcs.com/2014/09/"> 2014年九月 &nbsp;(16)</option>
	<option value="http://www.hankcs.com/2014/08/"> 2014年八月 &nbsp;(11)</option>
	<option value="http://www.hankcs.com/2014/07/"> 2014年七月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2014/06/"> 2014年六月 &nbsp;(13)</option>
	<option value="http://www.hankcs.com/2014/05/"> 2014年五月 &nbsp;(28)</option>
	<option value="http://www.hankcs.com/2014/04/"> 2014年四月 &nbsp;(41)</option>
	<option value="http://www.hankcs.com/2014/03/"> 2014年三月 &nbsp;(26)</option>
	<option value="http://www.hankcs.com/2014/02/"> 2014年二月 &nbsp;(52)</option>
	<option value="http://www.hankcs.com/2014/01/"> 2014年一月 &nbsp;(28)</option>
	<option value="http://www.hankcs.com/2013/12/"> 2013年十二月 &nbsp;(29)</option>
	<option value="http://www.hankcs.com/2013/11/"> 2013年十一月 &nbsp;(21)</option>
	<option value="http://www.hankcs.com/2013/10/"> 2013年十月 &nbsp;(11)</option>
	<option value="http://www.hankcs.com/2013/09/"> 2013年九月 &nbsp;(19)</option>
	<option value="http://www.hankcs.com/2013/08/"> 2013年八月 &nbsp;(22)</option>
	<option value="http://www.hankcs.com/2013/07/"> 2013年七月 &nbsp;(36)</option>
	<option value="http://www.hankcs.com/2013/06/"> 2013年六月 &nbsp;(24)</option>
	<option value="http://www.hankcs.com/2013/05/"> 2013年五月 &nbsp;(36)</option>
	<option value="http://www.hankcs.com/2013/04/"> 2013年四月 &nbsp;(29)</option>
	<option value="http://www.hankcs.com/2013/03/"> 2013年三月 &nbsp;(46)</option>
	<option value="http://www.hankcs.com/2013/02/"> 2013年二月 &nbsp;(5)</option>
	<option value="http://www.hankcs.com/2012/05/"> 2012年五月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2012/04/"> 2012年四月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2010/12/"> 2010年十二月 &nbsp;(5)</option>
	<option value="http://www.hankcs.com/2010/11/"> 2010年十一月 &nbsp;(10)</option>
	<option value="http://www.hankcs.com/2010/10/"> 2010年十月 &nbsp;(13)</option>
	<option value="http://www.hankcs.com/2010/09/"> 2010年九月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2010/08/"> 2010年八月 &nbsp;(5)</option>
	<option value="http://www.hankcs.com/2010/07/"> 2010年七月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2010/06/"> 2010年六月 &nbsp;(12)</option>
	<option value="http://www.hankcs.com/2010/05/"> 2010年五月 &nbsp;(14)</option>
	<option value="http://www.hankcs.com/2010/04/"> 2010年四月 &nbsp;(8)</option>
	<option value="http://www.hankcs.com/2010/03/"> 2010年三月 &nbsp;(16)</option>
	<option value="http://www.hankcs.com/2010/01/"> 2010年一月 &nbsp;(16)</option>
	<option value="http://www.hankcs.com/2009/12/"> 2009年十二月 &nbsp;(33)</option>
	<option value="http://www.hankcs.com/2009/11/"> 2009年十一月 &nbsp;(26)</option>
	<option value="http://www.hankcs.com/2009/09/"> 2009年九月 &nbsp;(2)</option>

		</select>
		</div><div class="widget widget_ui_posts"><h3>热门文章</h3><ul>		<li><a target="_blank" href="http://www.hankcs.com/ml/machine-learning-entry-list.html"><span class="thumbnail"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1ew7s3qoi2uj20h30meaco.jpg" class="thumb" alt="机器学习入门书单" title="机器学习入门书单"></span><span class="text">机器学习入门书单</span><span class="muted">2015-02-04</span><span class="muted">评论(26)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/back-propagation-neural-network.html"><span class="thumbnail"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1exsm4yho09j208c044t8o.jpg" class="thumb" alt="反向传播神经网络极简入门" title="反向传播神经网络极简入门"></span><span class="text">反向传播神经网络极简入门</span><span class="muted">2015-11-08</span><span class="muted">评论(25)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/k-nearest-neighbor-method.html"><span class="thumbnail"><img src="./CS229编程4：训练神经网络_files/6cbb8645jw1eoxj45stqqg20m80godki.gif" class="thumb" alt="k近邻法" title="k近邻法"></span><span class="text">k近邻法</span><span class="muted">2015-02-06</span><span class="muted">评论(11)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/naive-bayesian-method.html"><span class="thumbnail"><img src="./CS229编程4：训练神经网络_files/6cbb8645jw1eozn0y3spcj20mn0h3q3a.jpg" class="thumb" alt="朴素贝叶斯法" title="朴素贝叶斯法"></span><span class="text">朴素贝叶斯法</span><span class="muted">2015-02-09</span><span class="muted">评论(8)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/crf-code-analysis.html"><span class="thumbnail"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f72dgz2npkj21hc0uhwrp.jpg" class="thumb" alt="CRF++代码分析" title="CRF++代码分析"></span><span class="text">CRF++代码分析</span><span class="muted">2016-08-22</span><span class="muted">评论(8)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/em-algorithm-and-its-generalization.html"><span class="thumbnail"><img src="./CS229编程4：训练神经网络_files/6cbb8645gw1f4b95mzejvj20p60t0qab.jpg" class="thumb" alt="EM算法及其推广" title="EM算法及其推广"></span><span class="text">EM算法及其推广</span><span class="muted">2016-05-30</span><span class="muted">评论(7)</span></a></li>
</ul></div><div class="widget widget_ui_posts" style="top: 0px;"><h3>最新文章</h3><ul>		<li><a target="_blank" href="http://www.hankcs.com/ml/hinton-more-rnn.html"><span class="thumbnail"><img src="./CS229编程4：训练神经网络_files/006Fmjmcly1ffrtpy2q27j310j0ex0uq.jpg" class="thumb" alt="Hinton神经网络公开课8 More recurrent neural networks" title="Hinton神经网络公开课8 More recurrent neural networks"></span><span class="text">Hinton神经网络公开课8 More recurrent neural networks</span><span class="muted">2017-05-20</span><span class="muted">评论(2)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/hinton-rnn.html"><span class="thumbnail"><img src="./CS229编程4：训练神经网络_files/006Fmjmcly1ffpvl1hkqej318n0hljun.jpg" class="thumb" alt="Hinton神经网络公开课7 Recurrent neural networks" title="Hinton神经网络公开课7 Recurrent neural networks"></span><span class="text">Hinton神经网络公开课7 Recurrent neural networks</span><span class="muted">2017-05-18</span><span class="muted">评论(0)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/optimization-learning.html"><span class="thumbnail"><img src="./CS229编程4：训练神经网络_files/006Fmjmcly1fdm63kxtixj30ec06g74c.jpg" class="thumb" alt="Hinton神经网络公开课6 Optimization: How to make the learning go faster" title="Hinton神经网络公开课6 Optimization: How to make the learning go faster"></span><span class="text">Hinton神经网络公开课6 Optimization: How to make the learning go faster</span><span class="muted">2017-03-29</span><span class="muted">评论(0)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/sgd-cnn.html"><span class="thumbnail"><img src="./CS229编程4：训练神经网络_files/006Fmjmcly1fe2at0v9asj30fc0boq42.jpg" class="thumb" alt="随机梯度下降与卷积神经网络" title="随机梯度下降与卷积神经网络"></span><span class="text">随机梯度下降与卷积神经网络</span><span class="muted">2017-03-28</span><span class="muted">评论(4)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/feature-extraction-using-convolution.html"><span class="thumbnail"><img src="./CS229编程4：训练神经网络_files/006Fmjmcly1fdwcx2zdqag30em0aojsv.gif" class="thumb" alt="使用卷积进行特征提取" title="使用卷积进行特征提取"></span><span class="text">使用卷积进行特征提取</span><span class="muted">2017-03-28</span><span class="muted">评论(0)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/understanding-the-convolution-in-deep-learning.html"><span class="thumbnail"><img src="./CS229编程4：训练神经网络_files/006Fmjmcly1fdwjpji6qtj30dw05d0t8.jpg" class="thumb" alt="理解深度学习中的卷积" title="理解深度学习中的卷积"></span><span class="text">理解深度学习中的卷积</span><span class="muted">2017-03-24</span><span class="muted">评论(6)</span></a></li>
</ul></div><div class="widget widget_text"><h3>订阅关注</h3>			<div class="textwidget"><iframe width="100%" height="400" class="share_self" frameborder="0" scrolling="no" src="./CS229编程4：训练神经网络_files/index.html"></iframe></div>
		</div><div class="widget widget_ui_tags" style="top: 0px;"><h3>热门标签</h3><div class="d_tags"><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%8c%91%e6%88%98%e7%a8%8b%e5%ba%8f%e8%ae%be%e8%ae%a1%e7%ab%9e%e8%b5%9b%e7%ac%ac2%e7%89%88%e3%80%8b/">《挑战程序设计竞赛(第2版)》 (184)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%97%a5%e8%af%ad%e7%bb%bc%e5%90%88%e6%95%99%e7%a8%8b%e3%80%8b/">《日语综合教程》 (57)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%96%b0%e7%bc%96%e6%97%a5%e8%af%ad%e9%98%85%e8%af%bb%e6%96%87%e9%80%89%e3%80%8b/">《新编日语阅读文选》 (34)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%99%ba%e8%83%bdweb%e7%ae%97%e6%b3%95%e3%80%8b/">《智能Web算法》 (20)</a><a href="http://www.hankcs.com/tag/%e4%b8%ad%e6%96%87%e5%88%86%e8%af%8d/">中文分词 (18)</a><a href="http://www.hankcs.com/tag/wordpress/">WordPress (17)</a><a href="http://www.hankcs.com/tag/lucene/">Lucene (15)</a><a href="http://www.hankcs.com/tag/%e7%bb%b4%e7%89%b9%e6%af%94%e7%ae%97%e6%b3%95/">维特比算法 (15)</a><a href="http://www.hankcs.com/tag/%e6%96%b0%e7%bc%96%e6%97%a5%e8%af%ad%e5%95%86%e5%8a%a1%e8%b4%b8%e6%98%93%e4%bc%9a%e8%af%9d/">新编日语商务贸易会话 (14)</a><a href="http://www.hankcs.com/tag/intellij-idea/">IntelliJ IDEA (13)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e7%bb%9f%e8%ae%a1%e5%ad%a6%e4%b9%a0%e6%96%b9%e6%b3%95%e3%80%8b/">《统计学习方法》 (12)</a><a href="http://www.hankcs.com/tag/uva/">UVa (11)</a><a href="http://www.hankcs.com/tag/drupal7%e4%b8%93%e4%b8%9a%e5%bc%80%e5%8f%91%e6%8c%87%e5%8d%97-%e7%ac%ac%e4%b8%89%e7%89%88/">Drupal7专业开发指南 第三版 (10)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%8c%91%e6%88%98%e7%bc%96%e7%a8%8b-%e7%a8%8b%e5%ba%8f%e8%ae%be%e8%ae%a1%e7%ab%9e%e8%b5%9b%e8%ae%ad%e7%bb%83%e6%89%8b%e5%86%8c%e3%80%8b/">《挑战编程-程序设计竞赛训练手册》 (10)</a><a href="http://www.hankcs.com/tag/hmm/">HMM (10)</a><a href="http://www.hankcs.com/tag/neural-networks-for-machine-learning/">Neural Networks for Machine Learning (9)</a><a href="http://www.hankcs.com/tag/matlab/">matlab (9)</a><a href="http://www.hankcs.com/tag/cs229/">CS229 (8)</a><a href="http://www.hankcs.com/tag/google-code-jam/">Google code jam (7)</a><a href="http://www.hankcs.com/tag/%e3%80%8ac%e6%a0%87%e5%87%86%e7%a8%8b%e5%ba%8f%e5%ba%93-%e8%87%aa%e4%bf%ae%e6%95%99%e7%a8%8b%e4%b8%8e%e5%8f%82%e8%80%83%e6%89%8b%e5%86%8c%e3%80%8b/">《C++标准程序库—自修教程与参考手册》 (7)</a><a href="http://www.hankcs.com/tag/crf/">CRF (7)</a><a href="http://www.hankcs.com/tag/yii/">Yii (6)</a><a href="http://www.hankcs.com/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0/">深度学习 (6)</a><a href="http://www.hankcs.com/tag/webrtc/">WebRTC (5)</a><a href="http://www.hankcs.com/tag/cocos2d-x/">Cocos2d-x (5)</a><a href="http://www.hankcs.com/tag/tensorflow/">TensorFlow (5)</a><a href="http://www.hankcs.com/tag/cnn/">CNN (5)</a><a href="http://www.hankcs.com/tag/android/">Android (4)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e7%ae%97%e6%b3%95%e5%af%bc%e8%ae%ba%e3%80%8b/">《算法导论》 (4)</a><a href="http://www.hankcs.com/tag/%e3%80%8alucene%e5%ae%9e%e6%88%98%e7%ac%ac2%e7%89%88%e3%80%8b/">《Lucene实战(第2版)》 (4)</a></div></div></aside></section>

<div class="branding branding-black">
	<div class="container">
		<h2>我的开源项目</h2>
		<a target="blank" class="btn btn-lg" href="https://github.com/hankcs/HanLP">HanLP自然语言处理包</a><a target="blank" class="btn btn-lg" href="https://github.com/hankcs/AhoCorasickDoubleArrayTrie">基于DoubleArrayTrie的Aho Corasick自动机</a>	</div>
</div>
<footer class="footer">
	<div class="container">
		<div class="fcode">
					</div>
		<p>© 2017 <a href="http://www.hankcs.com/">码农场</a> &nbsp; <a href="http://www.hankcs.com/sitemap.xml">网站地图</a> &nbsp; <a href="http://www.miitbeian.gov.cn/" target="_blank">沪ICP备14002007号-1</a></p>
		<div style="display:none">
<script language="javascript" type="text/javascript" src="./CS229编程4：训练神经网络_files/trace.js.下载"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-47205472-1', 'auto');
  ga('send', 'pageview');

</script>
<script language="javascript" type="text/javascript" src="./CS229编程4：训练神经网络_files/15590612.js.下载"></script><a href="http://www.51.la/?15590612" target="_blank" title="51.La 网站流量统计系统"><img alt="51.La 网站流量统计系统" src="./CS229编程4：训练神经网络_files/icon_2.gif" style="border:none"></a>

<noscript>&lt;a href="//www.51.la/?15590612" target="_blank"&gt;&lt;img alt="&amp;#x6211;&amp;#x8981;&amp;#x5566;&amp;#x514D;&amp;#x8D39;&amp;#x7EDF;&amp;#x8BA1;" src="//img.users.51.la/15590612.asp" style="border:none" /&gt;&lt;/a&gt;</noscript>
</div>	</div>
</footer>

<script>
window.jsui={
    www: 'http://www.hankcs.com',
    uri: 'http://www.hankcs.com/wp-content/themes/dux',
    ver: '1.3',
	roll: ["1","2","6","4"],
    ajaxpager: '500',
    url_rp: 'http://www.hankcs.com/about/'
};
</script>
<script type="text/javascript" src="./CS229编程4：训练神经网络_files/form.js.下载"></script>
<script type="text/javascript" src="./CS229编程4：训练神经网络_files/bootstrap.min.js.下载"></script>
<script type="text/javascript" src="./CS229编程4：训练神经网络_files/loader.js.下载"></script>
<script type="text/javascript" src="./CS229编程4：训练神经网络_files/wp-embed.min.js.下载"></script>

    <div class="m-mask"></div>    <div class="rollbar" style="display: none;"><ul><li><a href="javascript:(scrollTo());"><i class="fa fa-angle-up"></i></a><h6>去顶部<i></i></h6></li><li><a href="javascript:(on_click_toc_button());"><i class="fa fa-list post_open_icon"></i></a><h6 id="toc_label">打开目录<i></i></h6></li><li><a href="javascript:(scrollTo(&#39;#comments&#39;,-15));"><i class="fa fa-comments"></i></a><h6>去评论<i></i></h6></li></ul></div><ul class="m-navbar">
			<li id="menu-item-1834" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1834"><a href="http://www.hankcs.com/program/cpp/">C++</a></li>
<li id="menu-item-1835" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1835"><a href="http://www.hankcs.com/program/java/">Java</a></li>
<li id="menu-item-5754" class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent menu-item-5754"><a href="http://www.hankcs.com/ml/">机器学习</a></li>
<li id="menu-item-2954" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-2954"><a href="http://www.hankcs.com/nlp/">NLP</a>
<ul class="sub-menu">
	<li id="menu-item-4344" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4344"><a href="http://www.hankcs.com/nlp/corpus/">语料库</a></li>
	<li id="menu-item-4342" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4342"><a href="http://www.hankcs.com/nlp/segment/">中文分词</a></li>
	<li id="menu-item-4343" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4343"><a href="http://www.hankcs.com/nlp/ner/">命名实体识别</a></li>
	<li id="menu-item-4479" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4479"><a href="http://www.hankcs.com/nlp/parsing/">句法分析</a></li>
</ul>
</li>
<li id="menu-item-1837" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1837"><a href="http://www.hankcs.com/program/algorithm/">算法</a></li>
<li id="menu-item-1839" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1839"><a href="http://www.hankcs.com/software/">软件</a></li>
<li id="menu-item-1838" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1838"><a href="http://www.hankcs.com/nihongonote/">日语</a>
<ul class="sub-menu">
	<li id="menu-item-1860" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1860"><a href="http://www.hankcs.com/nihongonote/riyurimen/">日语入门</a></li>
	<li id="menu-item-1861" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1861"><a href="http://www.hankcs.com/nihongonote/listening/">日语听力</a></li>
	<li id="menu-item-1863" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1863"><a href="http://www.hankcs.com/nihongonote/tekusuto/">日语综合教程</a>
	<ul class="sub-menu">
		<li id="menu-item-2190" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2190"><a href="http://www.hankcs.com/nihongonote/tekusuto/disance/">第三册</a></li>
		<li id="menu-item-2192" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2192"><a href="http://www.hankcs.com/nihongonote/tekusuto/daiyonnsatu/">第四册</a></li>
		<li id="menu-item-2191" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2191"><a href="http://www.hankcs.com/nihongonote/tekusuto/5/">第五册</a></li>
		<li id="menu-item-2702" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2702"><a href="http://www.hankcs.com/nihongonote/tekusuto/%e7%ac%ac%e5%85%ad%e5%86%8c/">第六册</a></li>
		<li id="menu-item-3604" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3604"><a href="http://www.hankcs.com/nihongonote/tekusuto/%e7%ac%ac%e4%b8%83%e5%86%8c/">第七册</a></li>
	</ul>
</li>
	<li id="menu-item-1859" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1859"><a href="http://www.hankcs.com/nihongonote/fd2/">新编日语阅读文选</a>
	<ul class="sub-menu">
		<li id="menu-item-2187" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2187"><a href="http://www.hankcs.com/nihongonote/fd2/c1/">第一册</a></li>
		<li id="menu-item-2189" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2189"><a href="http://www.hankcs.com/nihongonote/fd2/c2/">第二册</a></li>
		<li id="menu-item-2188" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2188"><a href="http://www.hankcs.com/nihongonote/fd2/c3/">第三册</a></li>
	</ul>
</li>
	<li id="menu-item-1858" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1858"><a href="http://www.hankcs.com/nihongonote/jpkaiwa/">日语商务贸易会话</a></li>
</ul>
</li>
<li id="menu-item-1843" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1843"><a href="http://www.hankcs.com/about/">关于</a></li>
							<li class="navto-search"><a href="javascript:;" class="search-show active"><i class="fa fa-search"></i></a></li>
					</ul>			<div class="sign">			    <div class="sign-mask"></div>			    <div class="container">			        <a href="http://www.hankcs.com/ml/neural-networks-learning-cs229.html#" class="close-link signclose-loader"><i class="fa fa-close"></i></a>			        <div class="sign-tips"></div>			        <form id="sign-in">  			            <h3><small class="signup-loader">切换注册</small>登录</h3>			            <h6>			                <label for="inputEmail">用户名或邮箱</label>			                <input type="text" name="username" class="form-control" id="inputEmail" placeholder="用户名或邮箱">			            </h6>			            <h6>			                <label for="inputPassword">密码</label>			                <input type="password" name="password" class="form-control" id="inputPassword" placeholder="登录密码">			            </h6>			            <div class="sign-submit">			                <input type="button" class="btn btn-primary signsubmit-loader" name="submit" value="登录">  			                <input type="hidden" name="action" value="signin">			                <label><input type="checkbox" checked="checked" name="remember" value="forever">记住我</label>			            </div><div class="sign-info"><a href="http://www.hankcs.com/about/">找回密码？</a></div></form>			        <form id="sign-up"> 			            <h3><small class="signin-loader">切换登录</small>注册</h3>			            <h6>			                <label for="inputName">昵称</label>			                <input type="text" name="name" class="form-control" id="inputName" placeholder="设置昵称">			            </h6>			            <h6>			                <label for="inputEmail">邮箱</label>			                <input type="email" name="email" class="form-control" id="inputEmail" placeholder="邮箱">			            </h6>			            <div class="sign-submit">			                <input type="button" class="btn btn-primary btn-block signsubmit-loader" name="submit" value="快速注册">  			                <input type="hidden" name="action" value="signup">  			            </div>			        </form>			    </div>			</div>		</body></html>