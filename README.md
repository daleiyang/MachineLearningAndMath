### Andrew Ng: [Machine Learning](https://www.coursera.org/learn/machine-learning/home/welcome) 
|在线课堂|完成时间|代码地址|
|:----------|:----------|:---------:|
|Introduction|2017/03/11||
|Linear Regression with Multiple Variables|2017/04/10|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Machine%20Learning/machine-learning-ex1)|
|Logistic Regression|2017/05/13| [代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Machine%20Learning/machine-learning-ex2)|
|Neural Networks: Representation|2017/05/15|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Machine%20Learning/machine-learning-ex3)|
|Neural Networks: Learning|2018/08/13|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Machine%20Learning/machine-learning-ex4)|
|Advice for Applying Machine Learning|2018/08/16|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Machine%20Learning/machine-learning-ex5)|
|Support Vector Machines|2018/08/19|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Machine%20Learning/machine-learning-ex6)|
|Unsupervised Learning|2018/08/20|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Machine%20Learning/machine-learning-ex7)|
|Anomaly Detection & Recommender Systems|2018/08/23|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Machine%20Learning/machine-learning-ex8)|
|Large Scale Machine Learning|2018/08/23||
|Application Example: Photo OCR|2018/08/24||

### Andrew Ng: [Deep Learning Specialization - Neural Networks and Deep Learning](https://www.coursera.org/learn/machine-learning/home/welcome) 
|在线课堂|完成时间|代码地址|
|:----------|:----------|:---------:|
|Introduction to deep learning|2018/08/25|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/1-Neural%20Networks%20and%20Deep%20Learning/Week%201)|
|Neural Networks Basics|2018/08/28|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/1-Neural%20Networks%20and%20Deep%20Learning/Week%202)|
|Shallow neural networks|2018/08/30|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/1-Neural%20Networks%20and%20Deep%20Learning/Week%203)|
|Deep Neural Networks|2018/09/06|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/1-Neural%20Networks%20and%20Deep%20Learning/Week%204)|

### Andrew Ng: [Deep Learning Specialization - Improving Deep Neural Networks](https://www.coursera.org/learn/machine-learning/home/welcome) 
|在线课堂|完成时间|代码地址|
|:----------|:----------|:---------:|
|Practical aspects of Deep Learning|2018/09/10|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/2-Improving%20Deep%20Neural%20Networks/Week%201)|
|Optimization algorithms|2018/09/12|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/2-Improving%20Deep%20Neural%20Networks/Week%202)|
|Hyperparameter tuning, Batch Normalization and Programming Frameworks|2018/09/15|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/2-Improving%20Deep%20Neural%20Networks/Week%203)|

### Andrew Ng: [Deep Learning Specialization - Structuring Machine Learning Projects](https://www.coursera.org/learn/machine-learning/home/welcome) 
|在线课堂|完成时间|代码地址|
|:----------|:----------|:---------:|
|ML Strategy (1)|2018/09/17|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/3-Structuring%20Machine%20Learning%20Projects/Week%201)|
|ML Strategy (2)|2018/09/17|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/3-Structuring%20Machine%20Learning%20Projects/Week%202)|

### Andrew Ng: [Deep Learning Specialization - Convolutional Neural Networks](https://www.coursera.org/learn/machine-learning/home/welcome) 
|在线课堂|完成时间|代码地址|
|:----------|:----------|:---------:|
|Foundations of Convolutional Neural Networks|2018/09/21|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/4-Convolutional%20Neural%20Networks/Week%201)|
|Deep convolutional models: case studies|2018/09/25|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/4-Convolutional%20Neural%20Networks/Week%202)|
|Object detection|2018/09/28|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/4-Convolutional%20Neural%20Networks/Week%203)|
|Special applications: Face recognition & Neural style transfer|2018/10/29|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/4-Convolutional%20Neural%20Networks/Week%204)|

### Andrew Ng: [Deep Learning Specialization - Sequence Models](https://www.coursera.org/learn/machine-learning/home/welcome) 
|在线课堂|完成时间|代码地址|
|:----------|:----------|:---------:|
|Recurrent Neural Networks|2018/11/18|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/5-%20Sequence%20Models/Week%201)|
|
### 李航 统计学习方法
|章节|完成时间|示例代码|
|:----------|:---------:|:----------|
|[第01章] 统计学习方法概论|2017/06/15|无
|[第02章] 感知机|2017/07/20|LiHang\Ch02\下的Perceptron.py和Perceptron2.py
|[第04章] 朴素贝叶斯法|2017/06/17|LiHang\Ch04\下的Bayes.py
|[第09章] EM算法及其推广|2017/07/03|LiHang\Ch09\下的em.py和test.py
|[第10章] 隐马尔可夫模型|2017/06/27|LiHang\Ch10\下的hmm.py和test.py

###  自然语言处理
|阅读源码|完成时间|资料地址|心得|
|:----------|:----------|:----------|:----------|
|基于词表的分词程序friso|2017/07/18|Segmentation\friso-master|1|
|基于结构化平均感知机的词性标注器|2017/07/24|POSTagging\AveragedPerceptron|2|
|基于结构化平均感知机的分词器|2017/08/02|Segmentation\结构化平均感知机的分词器|3|
|神经网络正向反向传播算法|2017/08/18|BP|4|
|word2vector|2017/09/07|w2v|5|
1. 这是一份基于词表的分词器源码，使用 MMSEG 消歧。体会如下：
   * 作者 C 功底深厚，实现了 string、动态数组、双向链表、基于 buckte 的 hash 表等基础数据结构。
   * 精细的内存控制手法值得学习。
   * FMM 和 MMSEG 算法代码清晰易懂，易于做二次开发，解 bad case。
   * 每种数据结构都有相对应的测试程序，稍加改动 MakeFile 后，就能用 make debug、gdb 单步。
   * 数字英文、英文数字组合以及二次切分的逻辑复杂，以后有需要的话再仔细研究。

2. 这是一份基于感知机的词性标注器源码。体会如下：
   * 用结构化平均感知机这种判别式模型学习特征的概率，然后用 HMM 这种生成式模型生成结果，这是很先进的方法。
   * 词性标注的特征抽取方法。
   * Collins 论文中的算法是如何在结构化平均感知机的学习算法中实现的。

3. 这是一份基于感知机的分词源码。代码短小精悍，技法纯熟。体会如下：
   * 用结构化平均感知机这种判别式模型学习特征的概率，然后用 HMM 这种生成式模型生成结果，这是很先进的方法。
   * 分词，命名实体识别采用的特征抽取方法、词性标注的特征抽取方法其实大同小异。
   * 如何实现 Collins 论文中的学习算法、面对 L1 , L2 正则化时如何变通 。
   * 使用 Bakeoff05 的 MSR 训练、测试语料，经过5轮训练，无正则化处理，F 值在 0.9472。
   * 存在的问题是 L1 , L2 的学习率需要调整，模型剪裁也还没实验。
   * 体会到 python 用很短的代码就能够完成复杂功能，但是训练起来很慢。
   * 很想做一份 C 的实现，和清华分词比比效果，以后有时间再做吧。

4. 下面两篇文章讲解了单层、多层神经网络的前向、反向传播公式推导和代码实现。
   * hankcs 文章推导了单隐藏层的公式，清晰、简洁，多看几遍就能理解，其中的梯度更新公式和 BP/source/bpnn.py 十分匹配，BP/source/code_recognizer.py 是使用BP/source/bpnn.py 的例子。
   * Demystifying Deep Convolutional Neural Networks 文章推导了多隐藏层的公式，图文并茂的讲解了原理，并且用Matlab编码实现，强烈推荐。

5. 我用一个月的时间，找材料、读论文、看代码、找到了最好的训练方法、结合适合的对齐方式、活用w2v的能力，使得句子相似度匹配的精确度有了大幅度提高。具体项目细节、实现方法，不便说。这里我记录一下学习的路径和心得，方便后来人。
   * word2vec 短短700行代码，博大精深，令人叹为观止。作者 Tomas Mikolov 应该是 OIer 出身，程序写法十分竞赛风格。从代码中能直接学到的编程技巧有：处理参数输入、用分段查表的方法加速 sigmoid 函数计算、带权采样、读文件的技巧、动态控制哈希表大小、建哈夫曼树、按线程拆分文件、随机数生成方法、随机窗口大小、自适应学习率。这些技巧都可以直接拿来用。
   * peghoty 的大作讲解细致入微，其中 4.1.2 节对 HS 的讲解精彩绝伦，但 4.2.2 节对 skip-gram 的讲解出错，然后 5.2 节对 skip-gram 的理解进行了修正。公式推导和代码实现一致度很高。
   * Physcal 的文章指出了很多实现细节，值得阅读完代码后再来品味。
   * Alex 的两篇文章讲解了原理，值得一读，里面的公式推导能够对应一部分代码实现。
   * hankcs 的“反向传播神经网络极简入门“ （BP目录下）讲述的”前向传播“”反向传播“ 的原理和公式推导，完美对应源码中 skip-gram 和 negative-sampling 这种组合的代码。
   * licstar 的两篇文章是训练 w2v 和 词向量历史故事的集大成之作，是分量很重的指导性文章。

### 可汗学院公开课：统计学
|章节|完成时间|
|:----------|:----------|
|01到28节|2017/06/07|
|29到50节|2017/06/08|
|51到55节|2017/06/09|
|56到61节|2017/06/11|
|62到85节|2017/06/12|

### MIT 18.06：Linear Algebra
|课堂讲课|完成时间|习题课|完成时间|
|:----------|:----------|:----------|:---------:|
|[第1集] 方程组的几何解释|2017/02/13|[第1集] 线性代数中的几何学|2017/02/13|
|[第2集] 矩阵消元|2017/02/14|[第3集] 矩阵的消去法|2017/02/14|
|[第3集] 乘法和逆矩阵|2017/02/15|[第4集] 逆矩阵|2017/02/15|
|[第4集] A的LU分解|2017/02/16|[第5集] LU分解|2017/02/16|
|[第5集] 转置-置换-向量空间R |2017/02/18|[第6集] 三维空间的子空间|2017/02/18|
|[第6集] 列空间和零空间|2017/02/18|[第7集] 向量子空间|2017/02/18|
|[第7集] 求解Ax=0：主变量、特解|2017/02/19|[第8集] 解Ax=0|2017/02/19|
|[第8集] 求解Ax=b：可解性和解的结构|2017/02/23|[第9集] 解Ax=b|2017/02/23|

### MIT 18.01：Single Variable Calculus
|课堂讲课|完成时间|习题课|完成时间|
|:----------|:----------|:----------|:---------:|
|[第1集] 导数和变化率|2017/03/04|[第1集] 课程简介|2017/03/04|
|[第2集] 极限和连续|2017/03/04|[第2集] 导数的定义 [第3集] 导数的图像|2017/03/04|
|[第3集] 求导四则运算及三角函数导数|2017/03/05|[第4集] 分段函数的光滑化|2017/03/05|
|[第4集] 链式法则及高阶导数|2017/03/05|||
|[第5集] 隐函数微分法和逆函数导数 |2017/03/05|||