<!DOCTYPE html>
<!-- saved from url=(0049)http://www.hankcs.com/ml/hidden-markov-model.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<link rel="dns-prefetch" href="http://apps.bdimg.com/">
<meta http-equiv="X-UA-Compatible" content="IE=11,IE=10,IE=9,IE=8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=0, minimum-scale=1.0, maximum-scale=1.0">
<meta name="apple-mobile-web-app-title" content="码农场">
<meta http-equiv="Cache-Control" content="no-siteapp">
<title>隐马尔可夫模型-码农场</title>
<link rel="dns-prefetch" href="http://apps.bdimg.com/">
<link rel="dns-prefetch" href="http://s.w.org/">
		<script async="" src="./hidden-markov-model_files/analytics.js.下载"></script><script src="./hidden-markov-model_files/ca-pub-1152644711996772.js.下载"></script><script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2.2.1\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2.2.1\/svg\/","svgExt":".svg","source":{"concatemoji":"http:\/\/www.hankcs.com\/wp-includes\/js\/wp-emoji-release.min.js?ver=4.7.5"}};
			!function(a,b,c){function d(a){var b,c,d,e,f=String.fromCharCode;if(!k||!k.fillText)return!1;switch(k.clearRect(0,0,j.width,j.height),k.textBaseline="top",k.font="600 32px Arial",a){case"flag":return k.fillText(f(55356,56826,55356,56819),0,0),!(j.toDataURL().length<3e3)&&(k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57331,65039,8205,55356,57096),0,0),b=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57331,55356,57096),0,0),c=j.toDataURL(),b!==c);case"emoji4":return k.fillText(f(55357,56425,55356,57341,8205,55357,56507),0,0),d=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55357,56425,55356,57341,55357,56507),0,0),e=j.toDataURL(),d!==e}return!1}function e(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var f,g,h,i,j=b.createElement("canvas"),k=j.getContext&&j.getContext("2d");for(i=Array("flag","emoji4"),c.supports={everything:!0,everythingExceptFlag:!0},h=0;h<i.length;h++)c.supports[i[h]]=d(i[h]),c.supports.everything=c.supports.everything&&c.supports[i[h]],"flag"!==i[h]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[i[h]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(g=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",g,!1),a.addEventListener("load",g,!1)):(a.attachEvent("onload",g),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),f=c.source||{},f.concatemoji?e(f.concatemoji):f.wpemoji&&f.twemoji&&(e(f.twemoji),e(f.wpemoji)))}(window,document,window._wpemojiSettings);
		</script><script src="./hidden-markov-model_files/wp-emoji-release.min.js.下载" type="text/javascript" defer=""></script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel="stylesheet" id="_bootstrap-css" href="./hidden-markov-model_files/bootstrap.min.css" type="text/css" media="all">
<link rel="stylesheet" id="_fontawesome-css" href="./hidden-markov-model_files/font-awesome.min.css" type="text/css" media="all">
<link rel="stylesheet" id="_main-css" href="./hidden-markov-model_files/main.css" type="text/css" media="all">
<script type="text/javascript" src="./hidden-markov-model_files/jquery.min.js.下载"></script>
<link rel="https://api.w.org/" href="http://www.hankcs.com/wp-json/">
<link rel="prev" title="word2vec原理推导与代码分析" href="http://www.hankcs.com/nlp/word2vec.html">
<link rel="next" title="Codeforces 138D World of Darkraft 题解《挑战程序设计竞赛》" href="http://www.hankcs.com/program/algorithm/codeforces-138d-world-of-darkraft-notes-challenge-programming-contest.html">
<link rel="canonical" href="http://www.hankcs.com/ml/hidden-markov-model.html">
<link rel="shortlink" href="http://www.hankcs.com/?p=7471">
<link rel="alternate" type="application/json+oembed" href="http://www.hankcs.com/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fwww.hankcs.com%2Fml%2Fhidden-markov-model.html">
<link rel="alternate" type="text/xml+oembed" href="http://www.hankcs.com/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fwww.hankcs.com%2Fml%2Fhidden-markov-model.html&amp;format=xml">
<meta name="keywords" content="《统计学习方法》, 维特比算法, 机器学习">
<meta name="description" content="本文是《统计学习方法》第10章的笔记，用一段167行的Python代码实现了隐马模型观测序列的生成、前向后向算法、Baum-Welch无监督训练、维特比算法。公式与代码相互对照，循序渐进。HMM算是个特别常见的模型，早在我没有挖ML这个坑的时候，就已经在用HMM做基于字符序列标注的分词和词性标注了，甚至照葫芦画瓢实现了一个2阶的HMM分词器。但我的理解仅仅停">
<link rel="icon" href="http://www.hankcs.com/wp-content/uploads/2017/04/cropped-Hankcs_512-32x32.png" sizes="32x32">
<link rel="icon" href="http://www.hankcs.com/wp-content/uploads/2017/04/cropped-Hankcs_512-192x192.png" sizes="192x192">
<link rel="apple-touch-icon-precomposed" href="http://www.hankcs.com/wp-content/uploads/2017/04/cropped-Hankcs_512-180x180.png">
<meta name="msapplication-TileImage" content="http://www.hankcs.com/wp-content/uploads/2017/04/cropped-Hankcs_512-270x270.png">
<link rel="shortcut icon" href="http://www.hankcs.com/favicon.ico">
<!--[if lt IE 9]><script src="http://www.hankcs.com/wp-content/themes/dux/js/libs/html5.min.js"></script><![endif]-->
<!--
	generated 42508 seconds ago
	generated in 0.223 seconds
	served from batcache in 0.003 seconds
	expires in 43892 seconds
-->
<script async="" data-requirecontext="_" data-requiremodule="main" src="./hidden-markov-model_files/main.js.下载"></script><script src="./hidden-markov-model_files/share.js.下载"></script><script async="" data-requirecontext="_" data-requiremodule="lazyload" src="./hidden-markov-model_files/lazyload.min.js.下载"></script><script async="" data-requirecontext="_" data-requiremodule="prettyprint" src="./hidden-markov-model_files/prettyprint.js.下载"></script><script async="" data-requirecontext="_" data-requiremodule="signpop" src="./hidden-markov-model_files/signpop.js.下载"></script><script async="" data-requirecontext="_" data-requiremodule="comment" src="./hidden-markov-model_files/comment.js.下载"></script><link href="./hidden-markov-model_files/share.css" rel="styleSheet" type="text/css"></head>
<body class="post-template-default single single-post postid-7471 single-format-standard comment-open site-layout-2">
<header class="header">
	<div class="container">
		<div class="logo"><a href="http://www.hankcs.com/" title="码农场-自然语言处理、机器学习算法"><img src="./hidden-markov-model_files/logo.png">码农场</a></div>		<a href="http://www.hankcs.com/" title="码农场-自然语言处理、机器学习算法" class="brand">放牧代码和思想
<br>专注自然语言处理、机器学习算法</a>		<ul class="site-nav site-navbar">
			<li id="menu-item-1834" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1834"><a href="http://www.hankcs.com/program/cpp/">C++</a></li>
<li id="menu-item-1835" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1835"><a href="http://www.hankcs.com/program/java/">Java</a></li>
<li id="menu-item-5754" class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent menu-item-5754"><a href="http://www.hankcs.com/ml/">机器学习</a></li>
<li id="menu-item-2954" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-2954"><a href="http://www.hankcs.com/nlp/">NLP</a>
<ul class="sub-menu">
	<li id="menu-item-4344" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4344"><a href="http://www.hankcs.com/nlp/corpus/">语料库</a></li>
	<li id="menu-item-4342" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4342"><a href="http://www.hankcs.com/nlp/segment/">中文分词</a></li>
	<li id="menu-item-4343" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4343"><a href="http://www.hankcs.com/nlp/ner/">命名实体识别</a></li>
	<li id="menu-item-4479" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4479"><a href="http://www.hankcs.com/nlp/parsing/">句法分析</a></li>
</ul>
</li>
<li id="menu-item-1837" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1837"><a href="http://www.hankcs.com/program/algorithm/">算法</a></li>
<li id="menu-item-1839" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1839"><a href="http://www.hankcs.com/software/">软件</a></li>
<li id="menu-item-1838" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1838"><a href="http://www.hankcs.com/nihongonote/">日语</a>
<ul class="sub-menu">
	<li id="menu-item-1860" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1860"><a href="http://www.hankcs.com/nihongonote/riyurimen/">日语入门</a></li>
	<li id="menu-item-1861" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1861"><a href="http://www.hankcs.com/nihongonote/listening/">日语听力</a></li>
	<li id="menu-item-1863" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1863"><a href="http://www.hankcs.com/nihongonote/tekusuto/">日语综合教程</a>
	<ul class="sub-menu">
		<li id="menu-item-2190" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2190"><a href="http://www.hankcs.com/nihongonote/tekusuto/disance/">第三册</a></li>
		<li id="menu-item-2192" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2192"><a href="http://www.hankcs.com/nihongonote/tekusuto/daiyonnsatu/">第四册</a></li>
		<li id="menu-item-2191" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2191"><a href="http://www.hankcs.com/nihongonote/tekusuto/5/">第五册</a></li>
		<li id="menu-item-2702" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2702"><a href="http://www.hankcs.com/nihongonote/tekusuto/%e7%ac%ac%e5%85%ad%e5%86%8c/">第六册</a></li>
		<li id="menu-item-3604" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3604"><a href="http://www.hankcs.com/nihongonote/tekusuto/%e7%ac%ac%e4%b8%83%e5%86%8c/">第七册</a></li>
	</ul>
</li>
	<li id="menu-item-1859" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1859"><a href="http://www.hankcs.com/nihongonote/fd2/">新编日语阅读文选</a>
	<ul class="sub-menu">
		<li id="menu-item-2187" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2187"><a href="http://www.hankcs.com/nihongonote/fd2/c1/">第一册</a></li>
		<li id="menu-item-2189" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2189"><a href="http://www.hankcs.com/nihongonote/fd2/c2/">第二册</a></li>
		<li id="menu-item-2188" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2188"><a href="http://www.hankcs.com/nihongonote/fd2/c3/">第三册</a></li>
	</ul>
</li>
	<li id="menu-item-1858" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1858"><a href="http://www.hankcs.com/nihongonote/jpkaiwa/">日语商务贸易会话</a></li>
</ul>
</li>
<li id="menu-item-1843" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1843"><a href="http://www.hankcs.com/about/">关于</a></li>
							<li class="navto-search"><a href="javascript:;" class="search-show active"><i class="fa fa-search"></i></a></li>
					</ul>
		<div class="topbar">
			<ul class="site-nav topmenu">
				<li id="menu-item-5755" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-5755"><a href="http://www.hankcs.com/about/#comments"><i class="fa fa-comment"></i> 留言板</a></li>
				<li><a target="_blank" rel="external nofollow" href="https://github.com/hankcs"><i class="fa fa-github-alt"></i> GitHub</a></li>                                <li><a target="_blank" rel="external nofollow" href="http://weibo.com/hankcs"><i class="fa fa-weibo"></i> 微博</a></li>                                <li><a target="_blank" rel="external nofollow" href="https://twitter.com/hankcs"><i class="fa fa-twitter"></i> Twitter</a></li>                                <li><a target="_blank" href="http://www.hankcs.com/feed"><i class="fa fa-rss"></i> RSS订阅</a></li>			</ul>
							&nbsp; &nbsp; <i class="fa fa-bullhorn url"></i> 正处于一个非常忙的阶段，抱歉不会经常回应任何联络					</div>
		<i class="fa fa-bars m-icon-nav"></i>
	</div>
</header>
<div class="site-search">
	<div class="container">
		<form method="get" class="site-search-form" action="http://www.hankcs.com/"><input class="search-input" name="s" type="text" placeholder="输入关键字" value=""><button class="search-btn" type="submit"><i class="fa fa-search"></i></button></form>	</div>
</div><section class="container">
	<div class="content-wrap">
	<div class="content">
				<header class="article-header">
			<h1 class="article-title"><a href="http://www.hankcs.com/ml/hidden-markov-model.html">隐马尔可夫模型</a></h1>
			<div class="article-meta">
				<span class="item">
					<a href="http://www.hankcs.com/">码农场</a> <small>&gt;</small> <a href="http://www.hankcs.com/ml/">机器学习</a><span class="muted"></span>				</span>
				<span class="item">2016-08-08</span>
																<span class="item post-views">阅读(1552)</span>				<span class="item"><a class="pc" href="http://www.hankcs.com/ml/hidden-markov-model.html#comments">评论(1)</a></span>				<span class="item"></span>
			</div>
		</header>
		<article class="article-content">
			<div class="asb asb-post asb-post-01"><script async="" src="./hidden-markov-model_files/adsbygoogle.js.下载"></script>
<!-- 文章页 - 页面标题下 728 90 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-1152644711996772" data-ad-slot="5413029241" data-adsbygoogle-status="done"><ins id="aswift_0_expand" style="display:inline-table;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent"><ins id="aswift_0_anchor" style="display:block;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent"><iframe width="728" height="90" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_0" name="aswift_0" style="left:0;position:absolute;top:0;width:728px;height:90px;" src="./hidden-markov-model_files/saved_resource.html"></iframe></ins></ins></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>			<div class="post_nav" style="width: 0px;"><div class="post_nav_side" style="height: 100%;"><div class="post_nav_top"><p>目录</p></div><div class="post_nav_bottom"></div><span class="post_nav_close icon-remove" title="关闭目录" style="opacity: 0; display: none;"><i class="fa fa-times"></i></span></div><ul class="post_nav_content"><li class="h2_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h2-0">隐马尔可夫模型的基本概念</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-1">隐马尔可夫模型的定义</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-2">隐马尔可夫模型的实例</a><i class="post_nav_dot"></i></li>
<li class="h3_nav active"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-3">HMM的Python定义</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-4">观测序列的生成过程</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-5">观测序列生成Python实现</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-6">隐马尔可夫模型的3个基本问題</a><i class="post_nav_dot"></i></li>
<li class="h2_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h2-7">概率计算算法</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-8">直接计算法</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-9">前向算法</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-10">前向算法Python实现</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-11">后向算法</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-12">后向算法的Python实现</a><i class="post_nav_dot"></i></li>
<li class="h2_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h2-13">学习算法</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-14">监督学习方法</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-15">Baum-Welch算法</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-16">Baum-Welch模型参数估计公式</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-17">Baum-Welch算法的Python实现</a><i class="post_nav_dot"></i></li>
<li class="h2_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h2-18">预测算法</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-19">近似算法</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-20">维特比算法</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h3-21">维特比算法Python实现</a><i class="post_nav_dot"></i></li>
<li class="h2_nav"><a href="http://www.hankcs.com/ml/hidden-markov-model.html#h2-22">Reference</a><i class="post_nav_dot"></i></li>
</ul></div><p style="text-indent: 2em;">本文是<a href="http://www.hankcs.com/tag/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B/" target="_blank">《统计学习方法》</a>第10章的笔记，用一段167行的Python代码实现了隐马模型观测序列的生成、前向后向算法、Baum-Welch无监督训练、维特比算法。公式与代码相互对照，循序渐进。<img src="./hidden-markov-model_files/6cbb8645gw1f6m709iuyxj20de0e5my8.jpg" title="An_example_of_HMM.png" alt="An_example_of_HMM.png" style="text-align: center; white-space: normal; text-indent: 32px; width: 0px; height: 0px;" width="0" height="0" border="0" vspace="0" data-tag="bdshare"></p>
<p style="text-indent: 2em;">HMM算是个特别常见的模型，早在我没有挖ML这个坑的时候，就已经在用HMM做基于字符序列标注的分词和词性标注了，甚至照葫芦画瓢实现了一个2阶的HMM分词器。但我的理解仅仅停留在“前向算法”“Viterbi”等层次。现在觉得靠各种应用向的论文、博客学习到的只是些皮毛，不如看一本专著来得全面。于是静下心来，从头到尾将这章认真看完，与自己原有的理解做一个对照，加深理解。 &nbsp; &nbsp;<img src="./hidden-markov-model_files/6cbb8645gw1f6m6zgsyrej20890cc3yi.jpg" title="统计学习方法.jpg" alt="统计学习方法.jpg" width="0" height="0" border="0" vspace="0" style="width: 0px; height: 0px;" data-tag="bdshare"> &nbsp; &nbsp;<img src="./hidden-markov-model_files/6cbb8645gw1f6m709iuyxj20de0e5my8.jpg" title="An_example_of_HMM.png" alt="An_example_of_HMM.png" width="0" height="0" border="0" vspace="0" style="width: 0px; height: 0px;" data-tag="bdshare"> &nbsp; &nbsp;<img src="./hidden-markov-model_files/6cbb8645gw1f6m6ozfplig20dw07gadn.gif" title="Viterbi_animated_demo.gif" alt="Viterbi_animated_demo.gif" style="text-align: center; white-space: normal; width: 0px; height: 0px;" width="0" height="0" border="0" vspace="0" data-tag="bdshare"></p>
<h2 id="h2-0">隐马尔可夫模型的基本概念</h2>
<h3 id="h3-1">隐马尔可夫模型的定义</h3>
<p style="text-indent: 2em;">隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。隐藏的马尔可夫链随机生成的状态的序列，称为状态序列（state sequence);每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列（observation sequence)。序列的每一个位置又可以看作是一个时刻。</p>
<p style="text-indent: 2em;">什么叫隐马尔科夫链呢？简单说来，就是把时间线看做一条链，每个节点只取决于前N个节点。就像你打开朋友圈，发现你可以根据你的基友最近的几条状态猜测出接下来TA狗嘴里要吐什么东西一样。</p>
<p style="text-indent: 2em;">接下来引入一些符号来表述这个定义——</p>
<p style="text-indent: 2em;"><span style="text-align: justify; text-indent: 17px;">设Q是所有可能的状态的集合，V是所有可能的观测的集合。</span></p>
<p style="text-align: center;"><img src="./hidden-markov-model_files/6cbb8645gw1f5b8saud3dj20kw02eaa2.jpg" title="屏幕快照 2016-06-28 下午8.44.27.png" alt="屏幕快照 2016-06-28 下午8.44.27.png" width="262" height="30" border="0" vspace="0" style="width: 262px; height: 30px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">其中，N是可能的状态数，M是可能的观测数。</p>
<p style="text-indent: 2em;">状态q是不可见的，观测v是可见的。应用到词性标注系统，词就是v，词性就是q。</p>
<p style="text-indent: 2em;">I是长度为T的状态序列，O是对应的观测序列。</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f5b8wftwktj20jw022mx5.jpg" title="屏幕快照 2016-06-28 下午9.10.53.png" alt="屏幕快照 2016-06-28 下午9.10.53.png" width="271" height="28" border="0" vspace="0" style="width: 271px; height: 28px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">这可以想象为相当于给定了一个词（O）+词性（<span style="text-indent: 2em;">I）的训练集，于是我们手上有了一个可以用隐马尔可夫模型解决的实际问题。</span></p>
<p style="text-indent: 2em;">定义A为状态转移概率矩阵：</p>
<p style="text-align: center;"><img src="./hidden-markov-model_files/6cbb8645gw1f5b9f9xpl4j206o0323yc.jpg" title="屏幕快照 2016-06-28 下午9.29.17.png" alt="屏幕快照 2016-06-28 下午9.29.17.png" width="98" height="45" border="0" vspace="0" style="width: 98px; height: 45px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">其中，</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f5b9iurttrj20ta02cwel.jpg" title="屏幕快照 2016-06-28 下午9.32.33.png" alt="屏幕快照 2016-06-28 下午9.32.33.png" width="402" height="32" border="0" vspace="0" style="width: 402px; height: 32px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;"></p>
<p style="text-indent: 2em;">是在时刻t处于状态q<sub>i</sub>的条件下在时刻t+1转移到状态q<sub>j</sub>的概率。</p>
<p style="text-indent: 2em;"><span style="text-indent: 2em;">这实际在表述一个一阶的HMM，所作的假设是每个状态只跟前一个状态有关。</span></p>
<p style="text-indent: 2em;"><span style="text-indent: 2em;">B是观测概率矩阵:</span></p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f5ba0bq2wtj208e02q3yd.jpg" title="屏幕快照 2016-06-28 下午9.49.26.png" alt="屏幕快照 2016-06-28 下午9.49.26.png" width="99" height="32" border="0" vspace="0" style="width: 99px; height: 32px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 2em;"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 2em;"><span style="text-indent: 32px;">其中，</span></span></p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f5ba1jxjpxj20u0020q32.jpg" title="屏幕快照 2016-06-28 下午9.50.32.png" alt="屏幕快照 2016-06-28 下午9.50.32.png" width="375" height="25" border="0" vspace="0" style="width: 375px; height: 25px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 2em;"><span style="text-indent: 32px;"></span></span></p>
<p style="text-indent: 2em;">是在时刻t处于状态q<sub>j</sub>的条件下生成观测v<sub>k</sub>的概率（也就是所谓的“发射概率”）。</p>
<p style="text-indent: 2em;"><span style="text-indent: 2em;"><span style="text-indent: 32px;"></span></span>这实际上在作另一个假设，观测是由当前时刻的状态决定的，跟其他因素无关，这有点像Moore自动机。</p>
<p style="text-indent: 2em;"><span style="text-indent: 2em;">π是初始状态概率向量：</span></p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f5ba7rruvfj204i01omwy.jpg" title="屏幕快照 2016-06-28 下午9.56.38.png" alt="屏幕快照 2016-06-28 下午9.56.38.png" width="65" height="24" border="0" vspace="0" style="width: 65px; height: 24px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">其中，</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f5ba8dffm9j20fg01qglj.jpg" title="屏幕快照 2016-06-28 下午9.57.11.png" alt="屏幕快照 2016-06-28 下午9.57.11.png" width="197" height="22" border="0" vspace="0" style="width: 197px; height: 22px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">是时刻t=1处于状态<span style="text-indent: 32px;">q</span><sub style="text-indent: 32px; white-space: normal;">j</sub>的概率。</p>
<p style="text-indent: 2em;">隐马尔可夫模型由初始状态概率向量π、状态转移概率矩阵A和观测概率矩阵B决定。π和A决定状态序列，B决定观测序列。因此，隐马尔可夫模型<img src="./hidden-markov-model_files/6cbb8645gw1f5bab0wgz1j201601m0sh.jpg" title="屏幕快照 2016-06-28 下午9.59.49.png" alt="屏幕快照 2016-06-28 下午9.59.49.png" width="16" height="22" border="0" vspace="0" style="width: 16px; height: 22px;" data-tag="bdshare">可以用三元符号表示，即</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f5baboa4mwj206o0220sk.jpg" title="屏幕快照 2016-06-28 下午10.00.24.png" alt="屏幕快照 2016-06-28 下午10.00.24.png" width="81" height="25" border="0" vspace="0" style="width: 81px; height: 25px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><img src="./hidden-markov-model_files/6cbb8645gw1f5bacg9twjj201q00y741.jpg" title="屏幕快照 2016-06-28 下午10.01.09.png" alt="屏幕快照 2016-06-28 下午10.01.09.png" width="40" height="22" border="0" vspace="0" style="width: 40px; height: 22px;" data-tag="bdshare">称为隐马尔可夫模型的三要素。如果加上一个具体的状态集合Q和观测序列V，则构成了HMM的五元组，又是一个很有中国特色的名字吧。</p>
<p style="text-indent: 2em;">状态转移概率矩阵A与初始状态概率向量π确定了隐藏的马尔可夫链，生成不可观测的状态序列。观测概率矩阵B确定了如何从状态生成观测，与状态序列综合确定了如何产生观测序列。</p>
<p style="text-indent: 2em;">从定义可知，隐马尔可夫模型作了两个基本假设：</p>
<p style="text-indent: 2em;">(1)齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关。</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f5fsiofwt8j20e8013q2u.jpg" title="屏幕快照 2016-07-02 下午7.31.56.png" alt="屏幕快照 2016-07-02 下午7.31.56.png" width="289" height="22" border="0" vspace="0" style="width: 289px; height: 22px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">从上式左右两边的复杂程度来看，齐次马尔可夫性假设简化了许多计算。</p>
<p style="text-indent: 2em;">(2)观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关。</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f5fskgb2y6j20hm013q2w.jpg" title="屏幕快照 2016-07-02 下午7.33.50.png" alt="屏幕快照 2016-07-02 下午7.33.50.png" width="358" height="22" border="0" vspace="0" style="width: 358px; height: 22px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">依然是一个简化运算的假设。</p>
<h3 id="h3-2">隐马尔可夫模型的实例</h3>
<p style="white-space: normal; text-indent: 2em;">让我们抛开教材上拗口的红球白球与盒子模型吧，来看看这样一个来自<a href="https://en.wikipedia.org/wiki/Viterbi_algorithm#Example" target="_blank" rel="external nofollow">wiki</a>的经典的HMM例子：</p>
<p style="white-space: normal; text-indent: 2em;">你本是一个城乡结合部<span style="text-decoration: line-through;">修电脑</span>做网站的小码农，突然春风吹来全民创业。于是跟村头诊所的老王<span style="text-indent: 32px;">响应总理号召</span>合伙创业去了，有什么好的创业点子呢？对了，现在不是很流行什么“<span style="text-decoration: line-through;">大数据</span>”么，那就搞个“医疗大数据”吧，虽然只是个乡镇诊所……<span style="text-indent: 2em;">但管它呢，投资人就好这口。</span></p>
<p style="white-space: normal; text-indent: 2em;"><span style="text-indent: 2em;">数据从哪儿来呢？你把老王，哦不，是王老板的出诊记录都翻了一遍，发现从这些记录来看，村子里的人只有两种病情：要么健康，要么发烧。但村民不确定自己到底是哪种状态，只能回答你感觉正常、头晕或冷。有位村民是诊所的常客，他的病历卡上完整地记录了这三天的身体特征(</span>正常、头晕或冷<span style="text-indent: 2em;">)，他想利用王老板的“医疗大数据”得出这三天的诊断结果(</span>健康或发烧<span style="text-indent: 2em;">)。</span></p>
<p style="white-space: normal; text-indent: 2em;">这时候王老板掐指一算，说其实感冒这种病，只跟病人前一天的病情有关，并且当天的病情决定当天的身体感觉。</p>
<p style="white-space: normal; text-indent: 2em;">于是你一拍大腿，天助我也，隐马尔可夫模型的两个基本假设都满足了，于是统计了一下病历卡上的数据，撸了这么一串Python代码：</p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="pln">states&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="str">'Healthy'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'Fever'</span><span class="pun">)</span></li><li class="L1"><span class="pln">&nbsp;</span></li><li class="L2"><span class="pln">observations&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="str">'normal'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'cold'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'dizzy'</span><span class="pun">)</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pln">start_probability&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">{</span><span class="str">'Healthy'</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="lit">0.6</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'Fever'</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="lit">0.4</span><span class="pun">}</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="pln">transition_probability&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">{</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="str">'Healthy'</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="pun">{</span><span class="str">'Healthy'</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="lit">0.7</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'Fever'</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="lit">0.3</span><span class="pun">},</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="str">'Fever'</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="pun">{</span><span class="str">'Healthy'</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="lit">0.4</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'Fever'</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="lit">0.6</span><span class="pun">},</span></li><li class="L9"><span class="pun">}</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pln">emission_probability&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">{</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="str">'Healthy'</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="pun">{</span><span class="str">'normal'</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="lit">0.5</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'cold'</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="lit">0.4</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'dizzy'</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="lit">0.1</span><span class="pun">},</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="str">'Fever'</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="pun">{</span><span class="str">'normal'</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="lit">0.1</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'cold'</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="lit">0.3</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'dizzy'</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="lit">0.6</span><span class="pun">},</span></li><li class="L4"><span class="pun">}</span></li></ol></pre>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6m709iuyxj20de0e5my8.jpg" title="An_example_of_HMM.png" alt="An_example_of_HMM.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare"></p>
<p style="white-space: normal; text-indent: 2em;">states代表病情，observations表示最近三天观察到的身体感受，start_probability代表病情的分布，transition_probability是病情到病情的转移概率，emission_probability则是病情表现出身体状况的发射概率。隐马的五元组都齐了，<span style="text-decoration: line-through;">就差哪位老总投个几百万了</span>。</p>
<h3 id="h3-3">HMM的Python定义</h3>
<p style="text-indent: 2em;">为了方便对照，降低公式造成的眩晕和昏睡效果，我决定改变以前的风格，不再把代码放到最后，而是直接将代码嵌入到理论讲解中，形成一份programmatic的tutorial。本文的代码主要参考&nbsp;aehuynh 的Python实现，我在其基础上添加了simulate方法和一些utility，并且修改了他错误的Baum-Welch算法，开源在GitHub上：<a href="https://github.com/hankcs/hidden-markov-model" _src="https://github.com/hankcs/hidden-markov-model" target="_blank" rel="external nofollow">https://github.com/hankcs/hidden-markov-model</a>&nbsp;。</p>
<p style="text-indent: 2em;">打开Python编辑器，有了前面的知识，我们就可以动手写一个一阶HMM模型的定义了：</p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="kwd">class</span><span class="pln">&nbsp;HMM</span><span class="pun">:</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="str">"""</span></li><li class="L2"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;Order&nbsp;1&nbsp;Hidden&nbsp;Markov&nbsp;Model</span></li><li class="L3"><span class="str">&nbsp;</span></li><li class="L4"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;Attributes</span></li><li class="L5"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;----------</span></li><li class="L6"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;:&nbsp;numpy.ndarray</span></li><li class="L7"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;State&nbsp;transition&nbsp;probability&nbsp;matrix</span></li><li class="L8"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;B:&nbsp;numpy.ndarray</span></li><li class="L9"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Output&nbsp;emission&nbsp;probability&nbsp;matrix&nbsp;with&nbsp;shape(N,&nbsp;number&nbsp;of&nbsp;output&nbsp;types)</span></li><li class="L0"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;pi:&nbsp;numpy.ndarray</span></li><li class="L1"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Initial&nbsp;state&nbsp;probablity&nbsp;vector</span></li><li class="L2"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;"""</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">def</span><span class="pln">&nbsp;__init__</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln">&nbsp;A</span><span class="pun">,</span><span class="pln">&nbsp;B</span><span class="pun">,</span><span class="pln">&nbsp;pi</span><span class="pun">):</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">A&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;A</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;B</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">pi&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;pi</span></li></ol></pre>
<p style="text-indent: 2em;">为了让该HMM实现接受王总的数据，我们必须写一些utility方法：</p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="kwd">def</span><span class="pln">&nbsp;generate_index_map</span><span class="pun">(</span><span class="pln">lables</span><span class="pun">):</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;index_label&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">{}</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;label_index&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">{}</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;i&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;l&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;lables</span><span class="pun">:</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;index_label</span><span class="pun">[</span><span class="pln">i</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;l</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;label_index</span><span class="pun">[</span><span class="pln">l</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;i</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i&nbsp;</span><span class="pun">+=</span><span class="pln">&nbsp;</span><span class="lit">1</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;label_index</span><span class="pun">,</span><span class="pln">&nbsp;index_label</span></li><li class="L9"><span class="pln">&nbsp;</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pln">states_label_index</span><span class="pun">,</span><span class="pln">&nbsp;states_index_label&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;generate_index_map</span><span class="pun">(</span><span class="pln">states</span><span class="pun">)</span></li><li class="L2"><span class="pln">observations_label_index</span><span class="pun">,</span><span class="pln">&nbsp;observations_index_label&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;generate_index_map</span><span class="pun">(</span><span class="pln">observations</span><span class="pun">)</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pln">&nbsp;</span></li><li class="L5"><span class="kwd">def</span><span class="pln">&nbsp;convert_observations_to_index</span><span class="pun">(</span><span class="pln">observations</span><span class="pun">,</span><span class="pln">&nbsp;label_index</span><span class="pun">):</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;list&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">[]</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;o&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;observations</span><span class="pun">:</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;list</span><span class="pun">.</span><span class="pln">append</span><span class="pun">(</span><span class="pln">label_index</span><span class="pun">[</span><span class="pln">o</span><span class="pun">])</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;list</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pln">&nbsp;</span></li><li class="L2"><span class="kwd">def</span><span class="pln">&nbsp;convert_map_to_vector</span><span class="pun">(</span><span class="pln">map</span><span class="pun">,</span><span class="pln">&nbsp;label_index</span><span class="pun">):</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;v&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">empty</span><span class="pun">(</span><span class="pln">len</span><span class="pun">(</span><span class="pln">map</span><span class="pun">),</span><span class="pln">&nbsp;dtype</span><span class="pun">=</span><span class="kwd">float</span><span class="pun">)</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;e&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;map</span><span class="pun">:</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v</span><span class="pun">[</span><span class="pln">label_index</span><span class="pun">[</span><span class="pln">e</span><span class="pun">]]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;map</span><span class="pun">[</span><span class="pln">e</span><span class="pun">]</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;v</span></li><li class="L7"><span class="pln">&nbsp;</span></li><li class="L8"><span class="pln">&nbsp;</span></li><li class="L9"><span class="kwd">def</span><span class="pln">&nbsp;convert_map_to_matrix</span><span class="pun">(</span><span class="pln">map</span><span class="pun">,</span><span class="pln">&nbsp;label_index1</span><span class="pun">,</span><span class="pln">&nbsp;label_index2</span><span class="pun">):</span></li><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;m&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">empty</span><span class="pun">((</span><span class="pln">len</span><span class="pun">(</span><span class="pln">label_index1</span><span class="pun">),</span><span class="pln">&nbsp;len</span><span class="pun">(</span><span class="pln">label_index2</span><span class="pun">)),</span><span class="pln">&nbsp;dtype</span><span class="pun">=</span><span class="kwd">float</span><span class="pun">)</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;line&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;map</span><span class="pun">:</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;col&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;map</span><span class="pun">[</span><span class="pln">line</span><span class="pun">]:</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m</span><span class="pun">[</span><span class="pln">label_index1</span><span class="pun">[</span><span class="pln">line</span><span class="pun">]][</span><span class="pln">label_index2</span><span class="pun">[</span><span class="pln">col</span><span class="pun">]]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;map</span><span class="pun">[</span><span class="pln">line</span><span class="pun">][</span><span class="pln">col</span><span class="pun">]</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;m</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="pln">A&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;convert_map_to_matrix</span><span class="pun">(</span><span class="pln">transition_probability</span><span class="pun">,</span><span class="pln">&nbsp;states_label_index</span><span class="pun">,</span><span class="pln">&nbsp;states_label_index</span><span class="pun">)</span></li><li class="L8"><span class="kwd">print</span><span class="pln">&nbsp;A</span></li><li class="L9"><span class="pln">B&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;convert_map_to_matrix</span><span class="pun">(</span><span class="pln">emission_probability</span><span class="pun">,</span><span class="pln">&nbsp;states_label_index</span><span class="pun">,</span><span class="pln">&nbsp;observations_label_index</span><span class="pun">)</span></li><li class="L0"><span class="kwd">print</span><span class="pln">&nbsp;B</span></li><li class="L1"><span class="pln">observations_index&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;convert_observations_to_index</span><span class="pun">(</span><span class="pln">observations</span><span class="pun">,</span><span class="pln">&nbsp;observations_label_index</span><span class="pun">)</span></li><li class="L2"><span class="pln">pi&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;convert_map_to_vector</span><span class="pun">(</span><span class="pln">start_probability</span><span class="pun">,</span><span class="pln">&nbsp;states_label_index</span><span class="pun">)</span></li><li class="L3"><span class="kwd">print</span><span class="pln">&nbsp;pi</span></li><li class="L4"><span class="pln">&nbsp;</span></li><li class="L5"><span class="pln">h&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;hmm</span><span class="pun">.</span><span class="pln">HMM</span><span class="pun">(</span><span class="pln">A</span><span class="pun">,</span><span class="pln">&nbsp;B</span><span class="pun">,</span><span class="pln">&nbsp;pi</span><span class="pun">)</span></li></ol></pre>
<p style="text-indent: 2em;">这些方法主要将源数据的map形式转为NumPy的矩阵形式：</p>
<pre class="prettyprint lang-plain linenums"><ol class="linenums"><li class="L0"><span class="pun">[[</span><span class="pln">&nbsp;</span><span class="lit">0.7</span><span class="pln">&nbsp;&nbsp;</span><span class="lit">0.3</span><span class="pun">]</span></li><li class="L1"><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">&nbsp;</span><span class="lit">0.4</span><span class="pln">&nbsp;&nbsp;</span><span class="lit">0.6</span><span class="pun">]]</span></li><li class="L2"><span class="pln">&nbsp;</span></li><li class="L3"><span class="pun">[[</span><span class="pln">&nbsp;</span><span class="lit">0.5</span><span class="pln">&nbsp;&nbsp;</span><span class="lit">0.4</span><span class="pln">&nbsp;&nbsp;</span><span class="lit">0.1</span><span class="pun">]</span></li><li class="L4"><span class="pln">&nbsp;</span><span class="pun">[</span><span class="pln">&nbsp;</span><span class="lit">0.1</span><span class="pln">&nbsp;&nbsp;</span><span class="lit">0.3</span><span class="pln">&nbsp;&nbsp;</span><span class="lit">0.6</span><span class="pun">]]</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="pun">[</span><span class="pln">&nbsp;</span><span class="lit">0.6</span><span class="pln">&nbsp;&nbsp;</span><span class="lit">0.4</span><span class="pun">]</span></li></ol></pre>
<p style="text-indent: 2em;">分别对应着A，B和pi。<span style="text-indent: 2em;">虽然该class目前什么都不能做，但是很快我们就能让它派上实际用场。</span></p>
<h3 id="h3-4">观测序列的生成过程<br></h3>
<p style="text-indent: 2em;">根据隐马尔可夫模型定义，可以将一个长度为T的观测序列<img src="./hidden-markov-model_files/6cbb8645gw1f5fsqu04cej2058014t8i.jpg" title="屏幕快照 2016-07-02 下午7.40.03.png" alt="屏幕快照 2016-07-02 下午7.40.03.png" width="103" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 103px; height: 22px;" data-tag="bdshare">的生成过程描述如下：</p>
<p style="text-indent: 2em;">算法(观测序列的生成）</p>
<p style="text-indent: 2em;">输入：隐马尔可夫模型<img src="./hidden-markov-model_files/6cbb8645gw1f5fsqli7idj203o017dfm.jpg" title="屏幕快照 2016-07-02 下午7.39.49.png" alt="屏幕快照 2016-07-02 下午7.39.49.png" width="68" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 68px; height: 22px;" data-tag="bdshare">，观测序列长度</p>
<p style="text-indent: 2em;">输出：观测序列<img src="./hidden-markov-model_files/6cbb8645gw1f5fsqu04cej2058014t8i.jpg" title="屏幕快照 2016-07-02 下午7.40.03.png" alt="屏幕快照 2016-07-02 下午7.40.03.png" width="103" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 103px; height: 22px;" data-tag="bdshare">。</p>
<p style="text-indent: 2em;">(1)<span class="Apple-tab-span" style="white-space:pre"></span>按照初始状态分布<img src="./hidden-markov-model_files/6cbb8645gw1f6lhq9bovvj201801g0qp.jpg" title="屏幕快照 2016-08-07 下午9.12.18.png" alt="屏幕快照 2016-08-07 下午9.12.18.png" width="20" height="23" border="0" vspace="0" style="width: 20px; height: 23px;" data-tag="bdshare">产生状态<img src="./hidden-markov-model_files/6cbb8645gw1f6lhusluhhj201201s0p9.jpg" title="屏幕快照 2016-08-07 下午9.17.01.png" alt="屏幕快照 2016-08-07 下午9.17.01.png" width="14" height="23" border="0" vspace="0" style="width: 14px; height: 23px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">(2)<span class="Apple-tab-span" style="white-space:pre"></span>令t=1</p>
<p style="text-indent: 2em;">(3)<span class="Apple-tab-span" style="white-space:pre"></span>按照状态<img src="./hidden-markov-model_files/6cbb8645gw1f6lhusluhhj201201s0p9.jpg" title="屏幕快照 2016-08-07 下午9.17.01.png" alt="屏幕快照 2016-08-07 下午9.17.01.png" width="14" height="23" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 14px; height: 23px;" data-tag="bdshare">的观测概率分布<img src="./hidden-markov-model_files/6cbb8645gw1f6lhvlwk0yj2030022gle.jpg" title="屏幕快照 2016-08-07 下午9.17.47.png" alt="屏幕快照 2016-08-07 下午9.17.47.png" width="34" height="23" border="0" vspace="0" style="width: 34px; height: 23px;" data-tag="bdshare">生成<img src="./hidden-markov-model_files/6cbb8645gw1f6lhw1znu1j201e01m0qv.jpg" title="屏幕快照 2016-08-07 下午9.18.02.png" alt="屏幕快照 2016-08-07 下午9.18.02.png" width="20" height="23" border="0" vspace="0" style="width: 20px; height: 23px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">(4)<span class="Apple-tab-span" style="white-space:pre"></span>按照状态<img src="./hidden-markov-model_files/6cbb8645gw1f6lhusluhhj201201s0p9.jpg" title="屏幕快照 2016-08-07 下午9.17.01.png" alt="屏幕快照 2016-08-07 下午9.17.01.png" width="14" height="23" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 14px; height: 23px;" data-tag="bdshare">的状态转移概率分布<img src="./hidden-markov-model_files/6cbb8645gw1f6li0aue0oj203k024gle.jpg" title="屏幕快照 2016-08-07 下午9.22.16.png" alt="屏幕快照 2016-08-07 下午9.22.16.png" width="56" height="33" border="0" vspace="0" style="width: 56px; height: 33px;" data-tag="bdshare">产生状态<img src="./hidden-markov-model_files/6cbb8645gw1f6li0jwmo8j20as022745.jpg" title="屏幕快照 2016-08-07 下午9.22.32.png" alt="屏幕快照 2016-08-07 下午9.22.32.png" width="131" height="25" border="0" vspace="0" style="width: 131px; height: 25px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">令<img src="./hidden-markov-model_files/6cbb8645gw1f6li0sq8rtj204601g3ya.jpg" title="屏幕快照 2016-08-07 下午9.22.46.png" alt="屏幕快照 2016-08-07 下午9.22.46.png" width="63" height="22" border="0" vspace="0" style="width: 63px; height: 22px;" data-tag="bdshare">；如果<img src="./hidden-markov-model_files/6cbb8645gw1f6li14fmlsj202y01qmwx.jpg" title="屏幕快照 2016-08-07 下午9.23.05.png" alt="屏幕快照 2016-08-07 下午9.23.05.png" width="37" height="22" border="0" vspace="0" style="width: 37px; height: 22px;" data-tag="bdshare">则转步(3)；否则，终止。</p>
<h3 id="h3-5">观测序列生成Python实现</h3>
<p style="text-indent: 2em;">为什么要生成观测序列呢？虽然我很想剧透并没有什么卵用，但<span style="text-indent: 32px;">王老板说他基于三年乡镇诊所病历卡的“医疗大数据库”里面并没有足够的数据给你跑什么算法。你只好自己动手，写算法生成一些了：</span></p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="kwd">def</span><span class="pln">&nbsp;simulate</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln">&nbsp;T</span><span class="pun">):</span></li><li class="L1"><span class="pln">&nbsp;</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">def</span><span class="pln">&nbsp;draw_from</span><span class="pun">(</span><span class="pln">probs</span><span class="pun">):</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="kwd">where</span><span class="pun">(</span><span class="pln">np</span><span class="pun">.</span><span class="pln">random</span><span class="pun">.</span><span class="pln">multinomial</span><span class="pun">(</span><span class="lit">1</span><span class="pun">,</span><span class="pln">probs</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">==</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">)[</span><span class="lit">0</span><span class="pun">][</span><span class="lit">0</span><span class="pun">]</span></li><li class="L4"><span class="pln">&nbsp;</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;observations&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">zeros</span><span class="pun">(</span><span class="pln">T</span><span class="pun">,</span><span class="pln">&nbsp;dtype</span><span class="pun">=</span><span class="kwd">int</span><span class="pun">)</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;states&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">zeros</span><span class="pun">(</span><span class="pln">T</span><span class="pun">,</span><span class="pln">&nbsp;dtype</span><span class="pun">=</span><span class="kwd">int</span><span class="pun">)</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;states</span><span class="pun">[</span><span class="lit">0</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;draw_from</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">pi</span><span class="pun">)</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;observations</span><span class="pun">[</span><span class="lit">0</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;draw_from</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B</span><span class="pun">[</span><span class="pln">states</span><span class="pun">[</span><span class="lit">0</span><span class="pun">],:])</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;t&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="lit">1</span><span class="pun">,</span><span class="pln">&nbsp;T</span><span class="pun">):</span></li><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;states</span><span class="pun">[</span><span class="pln">t</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;draw_from</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">A</span><span class="pun">[</span><span class="pln">states</span><span class="pun">[</span><span class="pln">t</span><span class="pun">-</span><span class="lit">1</span><span class="pun">],:])</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;observations</span><span class="pun">[</span><span class="pln">t</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;draw_from</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B</span><span class="pun">[</span><span class="pln">states</span><span class="pun">[</span><span class="pln">t</span><span class="pun">],:])</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;observations</span><span class="pun">,</span><span class="pln">states</span></li></ol></pre>
<p style="text-indent: 2em;">这串代码很小巧玲珑，draw_from接受一个概率分布，然后生成该分布下的一个样本。算法首先初始化两个长度为T的向量，接着按照初始状态分布pi生成第一个状态：</p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="pln">states</span><span class="pun">[</span><span class="lit">0</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;draw_from</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">pi</span><span class="pun">)</span></li></ol></pre>
<p style="text-indent: 2em;">这还没完，有了状态，我们还可以取出状态对应的观测的概率分布，生成一个观测：</p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="pln">observations</span><span class="pun">[</span><span class="lit">0</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;draw_from</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B</span><span class="pun">[</span><span class="pln">states</span><span class="pun">[</span><span class="lit">0</span><span class="pun">],:])</span></li></ol></pre>
<p style="text-indent: 2em;">接下来一直到t，我们都是按前一个状态取出状态转移概率分布，生成状态，再取出<span style="text-indent: 32px;">状态对应的观测的概率分布，生成一个观测。重复这个步骤，就得到了长度为T的观测和状态向量了。</span></p>
<p style="text-indent: 2em;">具体的调用方法是：</p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="pln">observations_data</span><span class="pun">,</span><span class="pln">&nbsp;states_data&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;h</span><span class="pun">.</span><span class="pln">simulate</span><span class="pun">(</span><span class="lit">10</span><span class="pun">)</span></li><li class="L1"><span class="kwd">print</span><span class="pln">&nbsp;observations_data</span></li><li class="L2"><span class="kwd">print</span><span class="pln">&nbsp;states_data</span></li></ol></pre>
<p style="text-indent: 2em;">输出：</p>
<pre class="prettyprint lang-plain linenums"><ol class="linenums"><li class="L0"><span class="pun">[</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">2</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="lit">2</span><span class="pun">]</span></li><li class="L1"><span class="pun">[</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pun">]</span></li></ol></pre>
<p style="text-indent: 2em;">这里直接用了下标表示，如果要糊弄投资人的话，还请按照index_label_map转换为相应的label。</p>
<h3 id="h3-6">隐马尔可夫模型的3个基本问題</h3>
<p style="text-indent: 2em;">隐马尔可夫模型有3个基本问题：</p>
<p style="text-indent: 2em;">(1) &nbsp; 概率计算问题。给定模型<img src="./hidden-markov-model_files/6cbb8645gw1f5fsqli7idj203o017dfm.jpg" title="屏幕快照 2016-07-02 下午7.39.49.png" alt="屏幕快照 2016-07-02 下午7.39.49.png" width="68" height="22" border="0" vspace="0" style="width: 68px; height: 22px;" data-tag="bdshare">和观测序列<img src="./hidden-markov-model_files/6cbb8645gw1f5fsqu04cej2058014t8i.jpg" title="屏幕快照 2016-07-02 下午7.40.03.png" alt="屏幕快照 2016-07-02 下午7.40.03.png" width="103" height="22" border="0" vspace="0" style="width: 103px; height: 22px;" data-tag="bdshare">,计算在模型<img src="./hidden-markov-model_files/6cbb8645gw1f5fsrvvm0uj200o00v0iz.jpg" title="屏幕快照 2016-07-02 下午7.41.04.png" alt="屏幕快照 2016-07-02 下午7.41.04.png" width="17" height="22" border="0" vspace="0" style="width: 17px; height: 22px;" data-tag="bdshare">下观测序列O出现的概率<img src="./hidden-markov-model_files/6cbb8645gw1f5fssb9ijcj202e00yq2p.jpg" title="屏幕快照 2016-07-02 下午7.41.27.png" alt="屏幕快照 2016-07-02 下午7.41.27.png" width="56" height="22" border="0" vspace="0" style="width: 56px; height: 22px;" data-tag="bdshare">。</p>
<p style="text-indent: 2em;">(2) &nbsp; 学习问题。己知观测序列<img src="./hidden-markov-model_files/6cbb8645gw1f5fsqu04cej2058014t8i.jpg" title="屏幕快照 2016-07-02 下午7.40.03.png" alt="屏幕快照 2016-07-02 下午7.40.03.png" width="103" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 103px; height: 22px;" data-tag="bdshare">,估计模型<img src="./hidden-markov-model_files/6cbb8645gw1f5fsqli7idj203o017dfm.jpg" title="屏幕快照 2016-07-02 下午7.39.49.png" alt="屏幕快照 2016-07-02 下午7.39.49.png" width="68" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 68px; height: 22px;" data-tag="bdshare">参数，使得在该模型下观测序列概率<img src="./hidden-markov-model_files/6cbb8645gw1f5fsuqaukoj202j018t8h.jpg" title="屏幕快照 2016-07-02 下午7.43.48.png" alt="屏幕快照 2016-07-02 下午7.43.48.png" width="46" height="22" border="0" vspace="0" style="width: 46px; height: 22px;" data-tag="bdshare">最大。即用极大似然估计的方法估计参数。</p>
<p style="text-indent: 2em;">(3) &nbsp; 预测问题，也称为解码（decoding)问题。已知模型<img src="./hidden-markov-model_files/6cbb8645gw1f5fsqli7idj203o017dfm.jpg" title="屏幕快照 2016-07-02 下午7.39.49.png" alt="屏幕快照 2016-07-02 下午7.39.49.png" width="68" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 68px; height: 22px;" data-tag="bdshare">和观测序列<img src="./hidden-markov-model_files/6cbb8645gw1f5fsqu04cej2058014t8i.jpg" title="屏幕快照 2016-07-02 下午7.40.03.png" alt="屏幕快照 2016-07-02 下午7.40.03.png" width="103" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 103px; height: 22px;" data-tag="bdshare">，求对给定观测序列条件概率<img src="./hidden-markov-model_files/6cbb8645gw1f5fsvqz4h5j202g012mwx.jpg" title="屏幕快照 2016-07-02 下午7.44.35.png" alt="屏幕快照 2016-07-02 下午7.44.35.png" width="51" height="22" border="0" vspace="0" style="width: 51px; height: 22px;" data-tag="bdshare">最大的状态序列<img src="./hidden-markov-model_files/6cbb8645gw1f5fsw1o66oj204o014dfm.jpg" title="屏幕快照 2016-07-02 下午7.45.04.png" alt="屏幕快照 2016-07-02 下午7.45.04.png" width="92" height="22" border="0" vspace="0" style="width: 92px; height: 22px;" data-tag="bdshare">。即给定观测序列，求最有可能的对应的状态序列。</p>
<p style="text-indent: 2em;">下面各节将逐一介绍这些基本问题的解法。</p>
<h2 id="h2-7"><span style="text-indent: 2em;"><span style="text-indent: 32px;">概率计算算法</span></span></h2>
<p style="text-indent: 2em;"><span style="text-indent: 2em;"><span style="text-indent: 32px;">这节介绍</span></span><span style="text-indent: 2em;">计算观测序列概率的前向（forward)与后向（backward)算法，以及概念上可行但计算上不可行的直接计算法（枚举）。前向后向算法无非就是求第一个状态的前向概率或最后一个状态的后向概率，然后向后或向前递推即可。</span></p>
<p style="text-indent: 2em;"><span style="text-indent: 2em;"></span></p>
<h3 id="h3-8"><span style="text-indent: 2em;">直接计算法<br></span></h3>
<p style="text-indent: 2em;">给定模型，求给定长度为T的观测序列的概率，直接计算法的思路是枚举所有的长度T的状态序列，计算该状态序列与观测序列的联合概率（隐状态发射到观测），对所有的枚举项求和即可。在状态种类为N的情况下，一共有N^T种排列组合，每种组合计算联合概率的计算量为T，总的复杂度为<img src="./hidden-markov-model_files/6cbb8645gw1f6ip9d1wzfj204401q3yb.jpg" title="屏幕快照 2016-08-05 上午11.16.26.png" alt="屏幕快照 2016-08-05 上午11.16.26.png" width="53" height="22" border="0" vspace="0" style="width: 53px; height: 22px;" data-tag="bdshare">，并不可取。</p>
<h3 id="h3-9">前向算法</h3>
<p style="text-indent: 2em;">首先定义前向概率。</p>
<p style="text-indent: 2em;"><strong>定义(前向概率）</strong>给定隐马尔可夫模型<img src="./hidden-markov-model_files/6cbb8645gw1f6ipjhxp1gj201a01m0sh.jpg" title="屏幕快照 2016-08-05 上午11.26.12.png" alt="屏幕快照 2016-08-05 上午11.26.12.png" width="17" height="22" border="0" vspace="0" style="width: 17px; height: 22px;" data-tag="bdshare">，定义到时刻t为止的观测序列为<img src="./hidden-markov-model_files/6cbb8645gw1f6ipkaqt2cj205g01owea.jpg" title="屏幕快照 2016-08-05 上午11.27.00.png" alt="屏幕快照 2016-08-05 上午11.27.00.png" width="72" height="22" border="0" vspace="0" style="width: 72px; height: 22px;" data-tag="bdshare">且状态为<img src="./hidden-markov-model_files/6cbb8645gw1f6ipkj7qvjj201401m0rm.jpg" title="屏幕快照 2016-08-05 上午11.27.16.png" alt="屏幕快照 2016-08-05 上午11.27.16.png" width="15" height="22" border="0" vspace="0" style="width: 15px; height: 22px;" data-tag="bdshare">的概率为前向概率，记作</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6iplf83vcj20fg02caa1.jpg" title="屏幕快照 2016-08-05 上午11.27.49.png" alt="屏幕快照 2016-08-05 上午11.27.49.png" width="219" height="33" border="0" vspace="0" style="width: 219px; height: 33px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">可以递推地求得前向概率<img src="./hidden-markov-model_files/6cbb8645gw1f6ipm3scwqj202g01q3ya.jpg" title="屏幕快照 2016-08-05 上午11.28.46.png" alt="屏幕快照 2016-08-05 上午11.28.46.png" width="31" height="22" border="0" vspace="0" style="width: 31px; height: 22px;" data-tag="bdshare">及观测序列概率<img src="./hidden-markov-model_files/6cbb8645gw1f6ipmdnvfbj204801y0sj.jpg" title="屏幕快照 2016-08-05 上午11.28.59.png" alt="屏幕快照 2016-08-05 上午11.28.59.png" width="48" height="22" border="0" vspace="0" style="width: 48px; height: 22px;" data-tag="bdshare">。</p>
<p style="text-indent: 2em;"><strong>算法(观测序列概率的前向算法）</strong></p>
<p style="text-indent: 2em;">输入：隐马尔可夫模型<img src="./hidden-markov-model_files/6cbb8645gw1f6ipjhxp1gj201a01m0sh.jpg" title="屏幕快照 2016-08-05 上午11.26.12.png" alt="屏幕快照 2016-08-05 上午11.26.12.png" width="17" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 17px; height: 22px;" data-tag="bdshare">，观测序列<img src="./hidden-markov-model_files/6cbb8645gw1f6ipxx58u0j20140180rc.jpg" title="屏幕快照 2016-08-05 上午11.40.07.png" alt="屏幕快照 2016-08-05 上午11.40.07.png" width="20" height="22" border="0" vspace="0" style="width: 20px; height: 22px;" data-tag="bdshare">;</p>
<p style="text-indent: 2em;">输出：观测序列概率<img src="./hidden-markov-model_files/6cbb8645gw1f6ipmdnvfbj204801y0sj.jpg" title="屏幕快照 2016-08-05 上午11.28.59.png" alt="屏幕快照 2016-08-05 上午11.28.59.png" width="48" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 48px; height: 22px;" data-tag="bdshare">。</p>
<p style="text-indent: 2em;">(1)<span class="Apple-tab-span" style="white-space: pre;"></span>初值</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6ipyramrhj20fo01s748.jpg" title="屏幕快照 2016-08-05 上午11.40.51.png" alt="屏幕快照 2016-08-05 上午11.40.51.png" width="229" height="26" border="0" vspace="0" style="width: 229px; height: 26px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">前向概率的定义中一共限定了两个条件，一是到当前为止的观测序列，另一个是当前的状态。所以初值的计算也有两项（观测和状态），一项是初始状态概率，另一项是发射到当前观测的概率。</span></p>
<p style="text-indent: 2em;">(2)<span class="Apple-tab-span" style="white-space:pre"></span>递推对<img src="./hidden-markov-model_files/6cbb8645gw1f6iq0l8s2jj208801mweb.jpg" title="屏幕快照 2016-08-05 上午11.42.23.png" alt="屏幕快照 2016-08-05 上午11.42.23.png" width="112" height="22" border="0" vspace="0" style="width: 112px; height: 22px;" data-tag="bdshare"></p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6iq12sxgvj20mm03u0sv.jpg" title="屏幕快照 2016-08-05 上午11.43.07.png" alt="屏幕快照 2016-08-05 上午11.43.07.png" width="325" height="55" border="0" vspace="0" style="width: 325px; height: 55px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">每次递推同样由两部分构成，大括号中是当前状态为i且观测序列的前t个符合要求的概率，括号外的是状态i发射观测t+1的概率。</span></p>
<p style="text-indent: 2em;">(3)<span class="Apple-tab-span" style="white-space:pre"></span>终止</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6iq2z3a01j209e03ia9z.jpg" title="屏幕快照 2016-08-05 上午11.44.45.png" alt="屏幕快照 2016-08-05 上午11.44.45.png" width="118" height="44" border="0" vspace="0" style="width: 118px; height: 44px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">由于到了时间T，一共有N种状态发射了最后那个观测，所以最终的结果要将这些概率加起来。</p>
<p style="text-indent: 2em;">由于每次递推都是在前一次的基础上进行的，所以降低了复杂度。准确来说，其计算过程如下图所示：</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6iqqquzlwj20no0cs0up.jpg" title="屏幕快照 2016-08-05 下午12.07.32.png" alt="屏幕快照 2016-08-05 下午12.07.32.png" width="474" height="256" border="0" vspace="0" style="width: 474px; height: 256px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">下方标号表示时间节点，每个时间点都有N种状态，所以相邻两个时间之间的递推消耗N^2次计算。而每次递推都是在前一次的基础上做的，所以只需累加O(T)次，所以总体复杂度是<span style="text-indent: 32px;">O(T)</span>个N^2，即<img src="./hidden-markov-model_files/6cbb8645gw1f6iqubxoxjj204001m3yb.jpg" title="屏幕快照 2016-08-05 下午12.11.02.png" alt="屏幕快照 2016-08-05 下午12.11.02.png" width="55" height="22" border="0" vspace="0" style="width: 55px; height: 22px;" data-tag="bdshare">。</p>
<h3 id="h3-10">前向算法Python实现</h3>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="kwd">def</span><span class="pln">&nbsp;_forward</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln">&nbsp;obs_seq</span><span class="pun">):</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;N&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">A</span><span class="pun">.</span><span class="pln">shape</span><span class="pun">[</span><span class="lit">0</span><span class="pun">]</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;T&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;len</span><span class="pun">(</span><span class="pln">obs_seq</span><span class="pun">)</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;F&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">zeros</span><span class="pun">((</span><span class="pln">N</span><span class="pun">,</span><span class="pln">T</span><span class="pun">))</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;F</span><span class="pun">[:,</span><span class="lit">0</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">pi&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B</span><span class="pun">[:,</span><span class="pln">&nbsp;obs_seq</span><span class="pun">[</span><span class="lit">0</span><span class="pun">]]</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;t&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="lit">1</span><span class="pun">,</span><span class="pln">&nbsp;T</span><span class="pun">):</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;n&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="pln">N</span><span class="pun">):</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;F</span><span class="pun">[</span><span class="pln">n</span><span class="pun">,</span><span class="pln">t</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">dot</span><span class="pun">(</span><span class="pln">F</span><span class="pun">[:,</span><span class="pln">t</span><span class="pun">-</span><span class="lit">1</span><span class="pun">],</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">A</span><span class="pun">[:,</span><span class="pln">n</span><span class="pun">]))</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B</span><span class="pun">[</span><span class="pln">n</span><span class="pun">,</span><span class="pln">&nbsp;obs_seq</span><span class="pun">[</span><span class="pln">t</span><span class="pun">]]</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;F</span></li></ol></pre>
<p style="text-indent: 2em;">代码和伪码的对应关系还是很清晰的，F对应alpha，HMM的三个参数与伪码一致。</p>
<h3 id="h3-11">后向算法</h3>
<p style="text-indent: 2em;"><strong>定义(后向概率）</strong>给定隐马尔可夫模型<img src="./hidden-markov-model_files/6cbb8645gw1f6ipjhxp1gj201a01m0sh.jpg" title="屏幕快照 2016-08-05 上午11.26.12.png" alt="屏幕快照 2016-08-05 上午11.26.12.png" width="17" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 17px; height: 22px;" data-tag="bdshare">,定义在时刻t状态为<img src="./hidden-markov-model_files/6cbb8645gw1f6ipkj7qvjj201401m0rm.jpg" title="屏幕快照 2016-08-05 上午11.27.16.png" alt="屏幕快照 2016-08-05 上午11.27.16.png" width="15" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 15px; height: 22px;" data-tag="bdshare">的条件下，从t+1到T的部分观测序列为<img src="./hidden-markov-model_files/6cbb8645gw1f6iudlo3ixj207201smwz.jpg" title="屏幕快照 2016-08-05 下午2.13.27.png" alt="屏幕快照 2016-08-05 下午2.13.27.png" width="87" height="22" border="0" vspace="0" style="width: 87px; height: 22px;" data-tag="bdshare">的概率为后向概率，记作</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6iue7zio9j20h202c3yi.jpg" title="屏幕快照 2016-08-05 下午2.14.07.png" alt="屏幕快照 2016-08-05 下午2.14.07.png" width="241" height="33" border="0" vspace="0" style="width: 241px; height: 33px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">可以用递推的方法求得后向概率<img src="./hidden-markov-model_files/6cbb8645gw1f6iugugjx8j202k0203ya.jpg" title="屏幕快照 2016-08-05 下午2.16.37.png" alt="屏幕快照 2016-08-05 下午2.16.37.png" width="28" height="22" border="0" vspace="0" style="width: 28px; height: 22px;" data-tag="bdshare">及观测序列概率<img src="./hidden-markov-model_files/6cbb8645gw1f6iuh94zqnj203y01owea.jpg" title="屏幕快照 2016-08-05 下午2.17.04.png" alt="屏幕快照 2016-08-05 下午2.17.04.png" width="52" height="22" border="0" vspace="0" style="width: 52px; height: 22px;" data-tag="bdshare">。</p>
<p style="text-indent: 2em;"><strong>算法(观测序列概率的后向算法）</strong></p>
<p style="text-indent: 2em;">输入：隐马尔可夫模型<img src="./hidden-markov-model_files/6cbb8645gw1f6ipjhxp1gj201a01m0sh.jpg" title="屏幕快照 2016-08-05 上午11.26.12.png" alt="屏幕快照 2016-08-05 上午11.26.12.png" width="17" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 17px; height: 22px;" data-tag="bdshare">,观测序列<img src="./hidden-markov-model_files/6cbb8645gw1f6ipxx58u0j20140180rc.jpg" title="屏幕快照 2016-08-05 上午11.40.07.png" alt="屏幕快照 2016-08-05 上午11.40.07.png" width="20" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 20px; height: 22px;" data-tag="bdshare">:</p>
<p style="text-indent: 2em;">输出：观测序列概率<img src="./hidden-markov-model_files/6cbb8645gw1f6ipmdnvfbj204801y0sj.jpg" title="屏幕快照 2016-08-05 上午11.28.59.png" alt="屏幕快照 2016-08-05 上午11.28.59.png" width="48" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 48px; height: 22px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">(1)初值</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6iuke89wtj20cc024a9y.jpg" title="屏幕快照 2016-08-05 下午2.19.52.png" alt="屏幕快照 2016-08-05 下午2.19.52.png" width="176" height="30" border="0" vspace="0" style="width: 176px; height: 30px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">根据定义，从T+1到T的部分观测序列其实不存在，所以硬性规定这个值是1。</p>
<p style="text-indent: 2em;">(2)<span class="Apple-tab-span" style="white-space:pre"></span>对<img src="./hidden-markov-model_files/6cbb8645gw1f6iul2i5h6j209c01oq2s.jpg" title="屏幕快照 2016-08-05 下午2.20.45.png" alt="屏幕快照 2016-08-05 下午2.20.45.png" width="123" height="22" border="0" vspace="0" style="width: 123px; height: 22px;" data-tag="bdshare"></p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6ium6pc0kj20kk03aq32.jpg" title="屏幕快照 2016-08-05 下午2.21.46.png" alt="屏幕快照 2016-08-05 下午2.21.46.png" width="238" height="38" border="0" vspace="0" style="width: 238px; height: 38px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">a<sub>ij</sub>表示状态i转移到j的概率，b<sub>j</sub>表示发射O<sub>t+1</sub>，<img src="./hidden-markov-model_files/6cbb8645gw1f6iuuhwo80j203i01mjr6.jpg" title="屏幕快照 2016-08-05 下午2.29.52.png" alt="屏幕快照 2016-08-05 下午2.29.52.png" width="48" height="22" border="0" vspace="0" style="width: 48px; height: 22px;" data-tag="bdshare">表示j后面的序列对应的后向概率。</p>
<p style="text-indent: 2em;">(3)</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6iun4a4j8j20di03cdfu.jpg" title="屏幕快照 2016-08-05 下午2.22.44.png" alt="屏幕快照 2016-08-05 下午2.22.44.png" width="154" height="38" border="0" vspace="0" style="width: 154px; height: 38px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">最后的求和是因为，在第一个时间点上有N种后向概率都能输出从2到T的观测序列，所以乘上输出O1的概率后求和得到最终结果。</p>
<h3 id="h3-12">后向算法的Python实现</h3>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="kwd">def</span><span class="pln">&nbsp;_backward</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln">&nbsp;obs_seq</span><span class="pun">):</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;N&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">A</span><span class="pun">.</span><span class="pln">shape</span><span class="pun">[</span><span class="lit">0</span><span class="pun">]</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;T&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;len</span><span class="pun">(</span><span class="pln">obs_seq</span><span class="pun">)</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">zeros</span><span class="pun">((</span><span class="pln">N</span><span class="pun">,</span><span class="pln">T</span><span class="pun">))</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;X</span><span class="pun">[:,-</span><span class="lit">1</span><span class="pun">:]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;t&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;reversed</span><span class="pun">(</span><span class="pln">range</span><span class="pun">(</span><span class="pln">T</span><span class="pun">-</span><span class="lit">1</span><span class="pun">)):</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;n&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="pln">N</span><span class="pun">):</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X</span><span class="pun">[</span><span class="pln">n</span><span class="pun">,</span><span class="pln">t</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">sum</span><span class="pun">(</span><span class="pln">X</span><span class="pun">[:,</span><span class="pln">t</span><span class="pun">+</span><span class="lit">1</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">A</span><span class="pun">[</span><span class="pln">n</span><span class="pun">,:]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B</span><span class="pun">[:,</span><span class="pln">&nbsp;obs_seq</span><span class="pun">[</span><span class="pln">t</span><span class="pun">+</span><span class="lit">1</span><span class="pun">]])</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;X</span></li></ol></pre>
<h2 id="h2-13">学习算法<br></h2>
<p style="text-indent: 2em;">隐马尔可夫模型的学习，根据训练数据是包括观测序列和对应的状态序列还是只有观测序列，可以分别由监督学习与非监督学习实现。本节首先介绍监督学习算法，而后介绍非监督学习算法——Baum-Weich算法（也就是EM算法)。</p>
<h3 id="h3-14">监督学习方法</h3>
<p style="text-indent: 2em;">假设已给训练数据包含S个长度相同的观测序列和对应的状态序列<img src="./hidden-markov-model_files/6cbb8645gw1f6iv5qhfj4j20e201wjrd.jpg" title="屏幕快照 2016-08-05 下午2.39.29.png" alt="屏幕快照 2016-08-05 下午2.39.29.png" width="164" height="22" border="0" vspace="0" style="width: 164px; height: 22px;" data-tag="bdshare">，那么可以利用极大似然估计法来估计隐马尔可夫模型的参数。具体方法如下。</p>
<p style="text-indent: 2em;">1.<span class="Apple-tab-span" style="white-space:pre"></span>转移概率<img src="./hidden-markov-model_files/6cbb8645gw1f6iv6ajhzqj201i01q0sh.jpg" title="屏幕快照 2016-08-05 下午2.41.05.png" alt="屏幕快照 2016-08-05 下午2.41.05.png" width="29" height="33" border="0" vspace="0" style="width: 29px; height: 33px;" data-tag="bdshare">的估计</p>
<p style="text-indent: 2em;">设样本中时刻t处于状态i时刻t+1<span style="text-indent: 2em;">转移到状态j的频数为<img src="./hidden-markov-model_files/6cbb8645gw1f6iv7qclddj201q01o0sh.jpg" title="屏幕快照 2016-08-05 下午2.42.32.png" alt="屏幕快照 2016-08-05 下午2.42.32.png" width="29" height="28" border="0" vspace="0" style="width: 29px; height: 28px;" data-tag="bdshare">，那么状态转移概率<img src="./hidden-markov-model_files/6cbb8645gw1f6iv6ajhzqj201i01q0sh.jpg" title="屏幕快照 2016-08-05 下午2.41.05.png" alt="屏幕快照 2016-08-05 下午2.41.05.png" width="29" height="33" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 29px; height: 33px;" data-tag="bdshare">的估计是</span></p>
<p><span class="Apple-tab-span" style="white-space:pre"></span></p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6iv8jy64lj20lo05cwel.jpg" title="屏幕快照 2016-08-05 下午2.43.15.png" alt="屏幕快照 2016-08-05 下午2.43.15.png" width="313" height="77" border="0" vspace="0" style="width: 313px; height: 77px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">很简单的最大似然估计。</p>
<p style="text-indent: 2em;">2. &nbsp; 观测概率的估计</p>
<p style="text-indent: 2em;">设样本中状态为j并观测为k的频数是<img src="./hidden-markov-model_files/6cbb8645gw1f6ive83ztuj201q01udfl.jpg" title="屏幕快照 2016-08-05 下午2.48.41.png" alt="屏幕快照 2016-08-05 下午2.48.41.png" width="26" height="28" border="0" vspace="0" style="width: 26px; height: 28px;" data-tag="bdshare">，那么状态为j观测为k的概率的估计是</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6iveznlaej20nm04yaa8.jpg" title="屏幕快照 2016-08-05 下午2.49.30.png" alt="屏幕快照 2016-08-05 下午2.49.30.png" width="368" height="77" border="0" vspace="0" style="width: 368px; height: 77px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">3. &nbsp; &nbsp;初始状态概率<img src="./hidden-markov-model_files/6cbb8645gw1f6ivfx08fvj201c01m0sh.jpg" title="屏幕快照 2016-08-05 下午2.50.24.png" alt="屏幕快照 2016-08-05 下午2.50.24.png" width="18" height="22" border="0" vspace="0" style="width: 18px; height: 22px;" data-tag="bdshare">的估计<img src="./hidden-markov-model_files/6cbb8645gw1f6ivg824qpj201g01s741.jpg" title="屏幕快照 2016-08-05 下午2.50.40.png" alt="屏幕快照 2016-08-05 下午2.50.40.png" width="20" height="24" border="0" vspace="0" style="width: 20px; height: 24px;" data-tag="bdshare">为S个样本中初始状态为<img src="./hidden-markov-model_files/6cbb8645gw1f6ivglegu8j201a01q0rm.jpg" title="屏幕快照 2016-08-05 下午2.51.02.png" alt="屏幕快照 2016-08-05 下午2.51.02.png" width="16" height="22" border="0" vspace="0" style="width: 16px; height: 22px;" data-tag="bdshare">的频率。</p>
<p style="text-indent: 2em;">由于监督学习需要使用训练数据，而人工标注训练数据往往代价很高，有时就会利用非监督学习的方法。</p>
<h3 id="h3-15">Baum-Welch算法</h3>
<p style="text-indent: 2em;">假设给定训练数据只包含S个长度为T的观测序列<img src="./hidden-markov-model_files/6cbb8645gw1f6ivv42id5j207i01uq2s.jpg" title="屏幕快照 2016-08-05 下午3.05.00.png" alt="屏幕快照 2016-08-05 下午3.05.00.png" width="90" height="22" border="0" vspace="0" style="width: 90px; height: 22px;" data-tag="bdshare">而没有对应的状态序列，目标是学习隐马尔可夫模型<img src="./hidden-markov-model_files/6cbb8645gw1f6ivvljyiqj206201qt8j.jpg" title="屏幕快照 2016-08-05 下午3.05.26.png" alt="屏幕快照 2016-08-05 下午3.05.26.png" width="77" height="22" border="0" vspace="0" style="width: 77px; height: 22px;" data-tag="bdshare">的参数。我们将观测序列数据看作观测数据O，状态序列数据看作不可观测的隐数据I，那么隐马尔可夫模型事实上是一个含有隐变量的概率模型</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6ivx5mbwuj20f402qwei.jpg" title="屏幕快照 2016-08-05 下午3.06.57.png" alt="屏幕快照 2016-08-05 下午3.06.57.png" width="183" height="33" border="0" vspace="0" style="width: 183px; height: 33px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">它的参数学习可以由EM算法实现。</p>
<p style="text-indent: 2em;">1&nbsp;确定完全数据的对数似然函数</p>
<p style="text-indent: 2em;">所有观测数据写成<img src="./hidden-markov-model_files/6cbb8645gw1f6iw01wm94j208q01uq2s.jpg" title="屏幕快照 2016-08-05 下午3.09.43.png" alt="屏幕快照 2016-08-05 下午3.09.43.png" width="105" height="22" border="0" vspace="0" style="width: 105px; height: 22px;" data-tag="bdshare">,所有隐数据写成<img src="./hidden-markov-model_files/6cbb8645gw1f6iw0ccde8j207c01ydfo.jpg" title="屏幕快照 2016-08-05 下午3.10.02.png" alt="屏幕快照 2016-08-05 下午3.10.02.png" width="83" height="22" border="0" vspace="0" style="width: 83px; height: 22px;" data-tag="bdshare">，完全数据是<img src="./hidden-markov-model_files/6cbb8645gw1f6iw19v09tj20fc020t8o.jpg" title="屏幕快照 2016-08-05 下午3.10.55.png" alt="屏幕快照 2016-08-05 下午3.10.55.png" width="169" height="22" border="0" vspace="0" style="width: 169px; height: 22px;" data-tag="bdshare">。完全数据的对数似然函数是<img src="./hidden-markov-model_files/6cbb8645gw1f6iw1tfdcoj206g01sdfo.jpg" title="屏幕快照 2016-08-05 下午3.11.24.png" alt="屏幕快照 2016-08-05 下午3.11.24.png" width="80" height="22" border="0" vspace="0" style="width: 80px; height: 22px;" data-tag="bdshare">。</p>
<p style="text-indent: 2em;">2&nbsp;EM算法的E步：求Q函数<img src="./hidden-markov-model_files/6cbb8645gw1f6iw2nmgohj203q024dfn.jpg" title="屏幕快照 2016-08-05 下午3.12.12.png" alt="屏幕快照 2016-08-05 下午3.12.12.png" width="39" height="22" border="0" vspace="0" style="width: 39px; height: 22px;" data-tag="bdshare"></p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6iw35p54bj20i402wt8s.jpg" title="屏幕快照 2016-08-05 下午3.12.41.png" alt="屏幕快照 2016-08-05 下午3.12.41.png" width="207" height="33" border="0" vspace="0" style="width: 207px; height: 33px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">其中，<img src="./hidden-markov-model_files/6cbb8645gw1f6iwavs0s7j201801s0sh.jpg" title="屏幕快照 2016-08-05 下午3.20.12.png" alt="屏幕快照 2016-08-05 下午3.20.12.png" width="15" height="22" border="0" vspace="0" style="width: 15px; height: 22px;" data-tag="bdshare">是隐马尔可夫模型参数的当前估计值，<img src="./hidden-markov-model_files/6cbb8645gw1f6iwbdi5v6j201601o0rm.jpg" title="屏幕快照 2016-08-05 下午3.20.39.png" alt="屏幕快照 2016-08-05 下午3.20.39.png" width="15" height="22" border="0" vspace="0" style="width: 15px; height: 22px;" data-tag="bdshare">是要极大化的隐马尔可夫模型参数。（Q函数的标准定义是：<img src="./hidden-markov-model_files/6cbb8645gw1f6iwzmkj9oj20cs01ot8n.jpg" title="屏幕快照 2016-08-05 下午3.43.56.png" alt="屏幕快照 2016-08-05 下午3.43.56.png" width="169" height="22" border="0" vspace="0" style="width: 169px; height: 22px;" data-tag="bdshare">，式子内部其实是条件概率，其中的<img src="./hidden-markov-model_files/6cbb8645gw1f6ix2rw1t1j201q01egld.jpg" title="屏幕快照 2016-08-05 下午3.46.56.png" alt="屏幕快照 2016-08-05 下午3.46.56.png" width="27" height="22" border="0" vspace="0" style="width: 27px; height: 22px;" data-tag="bdshare">对应<img src="./hidden-markov-model_files/6cbb8645gw1f6ix3kcxxdj204601oq2q.jpg" title="屏幕快照 2016-08-05 下午3.47.45.png" alt="屏幕快照 2016-08-05 下午3.47.45.png" width="55" height="22" border="0" vspace="0" style="width: 55px; height: 22px;" data-tag="bdshare">；其与<img src="./hidden-markov-model_files/6cbb8645gw1f6ix3zc1trj201401a0mj.jpg" title="屏幕快照 2016-08-05 下午3.48.11.png" alt="屏幕快照 2016-08-05 下午3.48.11.png" width="19" height="22" border="0" vspace="0" style="width: 19px; height: 22px;" data-tag="bdshare">无关，所以省略掉了。）</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6iwc94rgbj20l802m3ym.jpg" title="屏幕快照 2016-08-05 下午3.21.15.png" alt="屏幕快照 2016-08-05 下午3.21.15.png" width="268" height="33" border="0" vspace="0" style="width: 268px; height: 33px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">这个式子从左到右依次是初始概率、发射概率、转移概率、发射概率……</p>
<p style="text-indent: 2em;">于是函数<img src="./hidden-markov-model_files/6cbb8645gw1f6iwdgpcjjj203u0203yb.jpg" title="屏幕快照 2016-08-05 下午3.22.37.png" alt="屏幕快照 2016-08-05 下午3.22.37.png" width="42" height="22" border="0" vspace="0" style="width: 42px; height: 22px;" data-tag="bdshare">可以写成：</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6iwffch5tj20xo06ogmc.jpg" title="屏幕快照 2016-08-05 下午3.24.30.png" alt="屏幕快照 2016-08-05 下午3.24.30.png" width="444" height="88" border="0" vspace="0" style="width: 444px; height: 88px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">式中求和都是对所有训练数据的序列总长度T进行的。这个式子是将<img src="./hidden-markov-model_files/6cbb8645gw1f6iwc94rgbj20l802m3ym.jpg" title="屏幕快照 2016-08-05 下午3.21.15.png" alt="屏幕快照 2016-08-05 下午3.21.15.png" width="268" height="33" border="0" vspace="0" style="text-align: center; white-space: normal; width: 268px; height: 33px;" data-tag="bdshare">代入<img src="./hidden-markov-model_files/6cbb8645gw1f6iw35p54bj20i402wt8s.jpg" title="屏幕快照 2016-08-05 下午3.12.41.png" alt="屏幕快照 2016-08-05 下午3.12.41.png" width="207" height="33" border="0" vspace="0" style="text-align: center; white-space: normal; width: 207px; height: 33px;" data-tag="bdshare">后，将初始概率、转移概率、发射概率这三部分乘积的对数拆分为对数之和，所以有三项。</p>
<p style="text-indent: 2em;">3&nbsp;EM算法的M步:极大化Q函数<img src="./hidden-markov-model_files/6cbb8645gw1f6iw2nmgohj203q024dfn.jpg" title="屏幕快照 2016-08-05 下午3.12.12.png" alt="屏幕快照 2016-08-05 下午3.12.12.png" width="39" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 39px; height: 22px;" data-tag="bdshare">求模型参数<img src="./hidden-markov-model_files/6cbb8645gw1f6ix7nkbtwj203q01o742.jpg" title="屏幕快照 2016-08-05 下午3.51.36.png" alt="屏幕快照 2016-08-05 下午3.51.36.png" width="49" height="22" border="0" vspace="0" style="width: 49px; height: 22px;" data-tag="bdshare">，由于要极大化的参数在Q函数表达式中单独地出现在3个项中，所以只需对各项分别极大化。</p>
<p style="text-indent: 2em;"><strong>第1项可以写成</strong>：</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6ixa4r85fj20le03at8v.jpg" title="屏幕快照 2016-08-05 下午3.53.59.png" alt="屏幕快照 2016-08-05 下午3.53.59.png" width="287" height="44" border="0" vspace="0" style="width: 287px; height: 44px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">注意到<img src="./hidden-markov-model_files/6cbb8645gw1f6ixaudqjrj201601u0sh.jpg" title="屏幕快照 2016-08-05 下午3.54.45.png" alt="屏幕快照 2016-08-05 下午3.54.45.png" width="14" height="22" border="0" vspace="0" style="width: 14px; height: 22px;" data-tag="bdshare">满足约束条件利用拉格朗日乘子法，写出拉格朗日函数：</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6ixc12xnoj20im03mjri.jpg" title="屏幕快照 2016-08-05 下午3.55.34.png" alt="屏幕快照 2016-08-05 下午3.55.34.png" width="283" height="55" border="0" vspace="0" style="width: 283px; height: 55px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">对其求偏导数并令结果为0</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6ixe4jmfzj20mc03uaab.jpg" title="屏幕快照 2016-08-05 下午3.57.50.png" alt="屏幕快照 2016-08-05 下午3.57.50.png" width="320" height="55" border="0" vspace="0" style="width: 320px; height: 55px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">得到</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6ixfh6mtyj20ba02ct8m.jpg" title="屏幕快照 2016-08-05 下午3.58.51.png" alt="屏幕快照 2016-08-05 下午3.58.51.png" width="160" height="33" border="0" vspace="0" style="width: 160px; height: 33px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">这个求导是很简单的，求和项中非i的项对πi求导都是0，logπ的导数是1/π，γ那边求导就剩下πi自己对自己求导，也就是γ。等式两边同时乘以πi就得到了上式。</p>
<p style="text-indent: 2em;">对i求和得到γ：</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6ixlw7swmj20780203yc.jpg" title="屏幕快照 2016-08-05 下午4.05.21.png" alt="屏幕快照 2016-08-05 下午4.05.21.png" width="119" height="33" border="0" vspace="0" style="width: 119px; height: 33px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">代入<img src="./hidden-markov-model_files/6cbb8645gw1f6ixfh6mtyj20ba02ct8m.jpg" title="屏幕快照 2016-08-05 下午3.58.51.png" alt="屏幕快照 2016-08-05 下午3.58.51.png" width="160" height="33" border="0" vspace="0" style="text-align: center; white-space: normal; width: 160px; height: 33px;" data-tag="bdshare">中得到：</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6ixnq6jznj20aq03wdft.jpg" title="屏幕快照 2016-08-05 下午4.07.06.png" alt="屏幕快照 2016-08-05 下午4.07.06.png" width="151" height="55" border="0" vspace="0" style="width: 151px; height: 55px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><strong>第2项可以写成：</strong></p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6ixwcg14fj20wc046dga.jpg" title="屏幕快照 2016-08-05 下午4.15.18.png" alt="屏幕快照 2016-08-05 下午4.15.18.png" width="512" height="66" border="0" vspace="0" style="width: 512px; height: 66px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">类似第1项，应用具有约束条件<img src="./hidden-markov-model_files/6cbb8645gw1f6ixz8lqwaj204e03it8j.jpg" title="屏幕快照 2016-08-05 下午4.18.07.png" alt="屏幕快照 2016-08-05 下午4.18.07.png" width="69" height="55" border="0" vspace="0" style="width: 69px; height: 55px;" data-tag="bdshare">的拉格朗日乘子法可以求出</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6iy35obmzj20eq06kaaa.jpg" title="屏幕快照 2016-08-05 下午4.21.51.png" alt="屏幕快照 2016-08-05 下午4.21.51.png" width="225" height="100" border="0" vspace="0" style="width: 225px; height: 100px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><strong>第3项为:</strong></p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6iy64lub7j20t804edg9.jpg" title="屏幕快照 2016-08-05 下午4.24.43.png" alt="屏幕快照 2016-08-05 下午4.24.43.png" width="440" height="66" border="0" vspace="0" style="width: 440px; height: 66px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">同样用拉格朗日乘子法，约束条件是<img src="./hidden-markov-model_files/6cbb8645gw1f6iy74t7t2j205y03ca9x.jpg" title="屏幕快照 2016-08-05 下午4.25.35.png" alt="屏幕快照 2016-08-05 下午4.25.35.png" width="79" height="44" border="0" vspace="0" style="width: 79px; height: 44px;" data-tag="bdshare">。注意，只有在对<img src="./hidden-markov-model_files/6cbb8645gw1f6iy7t98kyj203401st8h.jpg" title="屏幕快照 2016-08-05 下午4.26.02.png" alt="屏幕快照 2016-08-05 下午4.26.02.png" width="58" height="33" border="0" vspace="0" style="width: 58px; height: 33px;" data-tag="bdshare">时<img src="./hidden-markov-model_files/6cbb8645gw1f6iy8lkjpvj203202adfm.jpg" title="屏幕快照 2016-08-05 下午4.27.09.png" alt="屏幕快照 2016-08-05 下午4.27.09.png" width="44" height="33" border="0" vspace="0" style="width: 44px; height: 33px;" data-tag="bdshare">对<img src="./hidden-markov-model_files/6cbb8645gw1f6iy8vvlk8j203402ca9u.jpg" title="屏幕快照 2016-08-05 下午4.27.27.png" alt="屏幕快照 2016-08-05 下午4.27.27.png" width="44" height="33" border="0" vspace="0" style="width: 44px; height: 33px;" data-tag="bdshare">的偏导数才不为0,以<img src="./hidden-markov-model_files/6cbb8645gw1f6iy9h976lj204s01owea.jpg" title="屏幕快照 2016-08-05 下午4.28.02.png" alt="屏幕快照 2016-08-05 下午4.28.02.png" width="95" height="33" border="0" vspace="0" style="width: 95px; height: 33px;" data-tag="bdshare">表示。求得</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6iy9z3sslj20ho06sq38.jpg" title="屏幕快照 2016-08-05 下午4.28.27.png" alt="屏幕快照 2016-08-05 下午4.28.27.png" width="230" height="88" border="0" vspace="0" style="width: 230px; height: 88px;" data-tag="bdshare"></p>
<h3 id="h3-16">Baum-Welch模型参数估计公式</h3>
<p style="text-indent: 2em;">将这三个式子中的各概率分别简写如下：</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6jwwx8o58j20iu05yjrq.jpg" title="屏幕快照 2016-08-06 下午12.26.43.png" alt="屏幕快照 2016-08-06 下午12.26.43.png" width="244" height="77" border="0" vspace="0" style="width: 244px; height: 77px;" data-tag="bdshare"></p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6jwxqgow8j20k205omxl.jpg" title="屏幕快照 2016-08-06 下午12.27.35.png" alt="屏幕快照 2016-08-06 下午12.27.35.png" width="273" height="77" border="0" vspace="0" style="width: 273px; height: 77px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">则可将相应的公式写成：</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6jwyjjct5j20d80hw0ta.jpg" title="屏幕快照 2016-08-06 下午12.28.26.png" alt="屏幕快照 2016-08-06 下午12.28.26.png" width="203" height="280" border="0" vspace="0" style="width: 203px; height: 280px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">这三个表达式就是Baum-Welch算法（Baum-Welch algorithm)，它是EM算法在隐马尔可夫模型学习中的具体实现，由Baum和Welch提出。</p>
<p style="text-indent: 2em;">算法 (Baum-Welch算法）</p>
<p style="text-indent: 2em;">输入：观测数据<img src="./hidden-markov-model_files/6cbb8645gw1f6jx097an3j2094020745.jpg" title="屏幕快照 2016-08-06 下午12.30.06.png" alt="屏幕快照 2016-08-06 下午12.30.06.png" width="114" height="25" border="0" vspace="0" style="width: 114px; height: 25px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">输出：隐马尔可夫模型参数。</p>
<p style="text-indent: 2em;">(1)<span class="Apple-tab-span" style="white-space:pre"></span>初始化</p>
<p style="text-indent: 2em;">对<img src="./hidden-markov-model_files/6cbb8645gw1f6jx0tzyhtj202y01mq2p.jpg" title="屏幕快照 2016-08-06 下午12.30.34.png" alt="屏幕快照 2016-08-06 下午12.30.34.png" width="40" height="22" border="0" vspace="0" style="width: 40px; height: 22px;" data-tag="bdshare">,选取<img src="./hidden-markov-model_files/6cbb8645gw1f6jx1ldir3j20ak02imx2.jpg" title="屏幕快照 2016-08-06 下午12.31.26.png" alt="屏幕快照 2016-08-06 下午12.31.26.png" width="139" height="33" border="0" vspace="0" style="width: 139px; height: 33px;" data-tag="bdshare">，得到模型<img src="./hidden-markov-model_files/6cbb8645gw1f6jx1wnaxij20b602q747.jpg" title="屏幕快照 2016-08-06 下午12.31.38.png" alt="屏幕快照 2016-08-06 下午12.31.38.png" width="135" height="33" border="0" vspace="0" style="width: 135px; height: 33px;" data-tag="bdshare">。</p>
<p style="text-indent: 2em;">(2)<span class="Apple-tab-span" style="white-space: pre;"></span>递推。对<img src="./hidden-markov-model_files/6cbb8645gw1f6jx2so4p8j20680200sj.jpg" title="屏幕快照 2016-08-06 下午12.32.34.png" alt="屏幕快照 2016-08-06 下午12.32.34.png" width="78" height="25" border="0" vspace="0" style="width: 78px; height: 25px;" data-tag="bdshare"></p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6jx33okzrj20bi06ydfx.jpg" title="屏幕快照 2016-08-06 下午12.32.49.png" alt="屏幕快照 2016-08-06 下午12.32.49.png" width="184" height="111" border="0" vspace="0" style="width: 184px; height: 111px;" data-tag="bdshare"></p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6jx44p5b0j20ce09c0sz.jpg" title="屏幕快照 2016-08-06 下午12.33.49.png" alt="屏幕快照 2016-08-06 下午12.33.49.png" width="205" height="155" border="0" vspace="0" style="width: 205px; height: 155px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">右端各值按观测<img src="./hidden-markov-model_files/6cbb8645gw1f6jx097an3j2094020745.jpg" title="屏幕快照 2016-08-06 下午12.30.06.png" alt="屏幕快照 2016-08-06 下午12.30.06.png" width="114" height="25" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 114px; height: 25px;" data-tag="bdshare">和模型<img src="./hidden-markov-model_files/6cbb8645gw1f6jx50bex3j20bc024t8m.jpg" title="屏幕快照 2016-08-06 下午12.34.40.png" alt="屏幕快照 2016-08-06 下午12.34.40.png" width="134" height="25" border="0" vspace="0" style="width: 134px; height: 25px;" data-tag="bdshare">计算。</p>
<p style="text-indent: 2em;">(3)<span class="Apple-tab-span" style="white-space:pre"></span>终止。得到模型参数<img src="./hidden-markov-model_files/6cbb8645gw1f6jx6vbpudj20ea020748.jpg" title="屏幕快照 2016-08-06 下午12.36.26.png" alt="屏幕快照 2016-08-06 下午12.36.26.png" width="179" height="25" border="0" vspace="0" style="width: 179px; height: 25px;" data-tag="bdshare">。</p>
<h3 id="h3-17">Baum-Welch算法的Python实现</h3>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="kwd">def</span><span class="pln">&nbsp;baum_welch_train</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln">&nbsp;observations</span><span class="pun">,</span><span class="pln">&nbsp;criterion</span><span class="pun">=</span><span class="lit">0.05</span><span class="pun">):</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;n_states&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">A</span><span class="pun">.</span><span class="pln">shape</span><span class="pun">[</span><span class="lit">0</span><span class="pun">]</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;n_samples&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;len</span><span class="pun">(</span><span class="pln">observations</span><span class="pun">)</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">done</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="kwd">False</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">while</span><span class="pln">&nbsp;</span><span class="kwd">not</span><span class="pln">&nbsp;</span><span class="kwd">done</span><span class="pun">:</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;alpha_t(i)&nbsp;=&nbsp;P(O_1&nbsp;O_2&nbsp;...&nbsp;O_t,&nbsp;q_t&nbsp;=&nbsp;S_i&nbsp;|&nbsp;hmm)</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;Initialize&nbsp;alpha</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;alpha&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">_forward</span><span class="pun">(</span><span class="pln">observations</span><span class="pun">)</span></li><li class="L9"><span class="pln">&nbsp;</span></li><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;beta_t(i)&nbsp;=&nbsp;P(O_t+1&nbsp;O_t+2&nbsp;...&nbsp;O_T&nbsp;|&nbsp;q_t&nbsp;=&nbsp;S_i&nbsp;,&nbsp;hmm)</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;Initialize&nbsp;beta</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;beta&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">_backward</span><span class="pun">(</span><span class="pln">observations</span><span class="pun">)</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;xi&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">zeros</span><span class="pun">((</span><span class="pln">n_states</span><span class="pun">,</span><span class="pln">n_states</span><span class="pun">,</span><span class="pln">n_samples</span><span class="pun">-</span><span class="lit">1</span><span class="pun">))</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;t&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="pln">n_samples</span><span class="pun">-</span><span class="lit">1</span><span class="pun">):</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;denom&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">dot</span><span class="pun">(</span><span class="pln">np</span><span class="pun">.</span><span class="pln">dot</span><span class="pun">(</span><span class="pln">alpha</span><span class="pun">[:,</span><span class="pln">t</span><span class="pun">].</span><span class="pln">T</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">A</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B</span><span class="pun">[:,</span><span class="pln">observations</span><span class="pun">[</span><span class="pln">t</span><span class="pun">+</span><span class="lit">1</span><span class="pun">]].</span><span class="pln">T</span><span class="pun">,</span><span class="pln">&nbsp;beta</span><span class="pun">[:,</span><span class="pln">t</span><span class="pun">+</span><span class="lit">1</span><span class="pun">])</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;i&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="pln">n_states</span><span class="pun">):</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;numer&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;alpha</span><span class="pun">[</span><span class="pln">i</span><span class="pun">,</span><span class="pln">t</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">A</span><span class="pun">[</span><span class="pln">i</span><span class="pun">,:]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B</span><span class="pun">[:,</span><span class="pln">observations</span><span class="pun">[</span><span class="pln">t</span><span class="pun">+</span><span class="lit">1</span><span class="pun">]].</span><span class="pln">T&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;beta</span><span class="pun">[:,</span><span class="pln">t</span><span class="pun">+</span><span class="lit">1</span><span class="pun">].</span><span class="pln">T</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;xi</span><span class="pun">[</span><span class="pln">i</span><span class="pun">,:,</span><span class="pln">t</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;numer&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;denom</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;gamma_t(i)&nbsp;=&nbsp;P(q_t&nbsp;=&nbsp;S_i&nbsp;|&nbsp;O,&nbsp;hmm)</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gamma&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">sum</span><span class="pun">(</span><span class="pln">xi</span><span class="pun">,</span><span class="pln">axis</span><span class="pun">=</span><span class="lit">1</span><span class="pun">)</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;Need&nbsp;final&nbsp;gamma&nbsp;element&nbsp;for&nbsp;new&nbsp;B</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prod&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;&nbsp;</span><span class="pun">(</span><span class="pln">alpha</span><span class="pun">[:,</span><span class="pln">n_samples</span><span class="pun">-</span><span class="lit">1</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;beta</span><span class="pun">[:,</span><span class="pln">n_samples</span><span class="pun">-</span><span class="lit">1</span><span class="pun">]).</span><span class="pln">reshape</span><span class="pun">((-</span><span class="lit">1</span><span class="pun">,</span><span class="lit">1</span><span class="pun">))</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gamma&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">hstack</span><span class="pun">((</span><span class="pln">gamma</span><span class="pun">,</span><span class="pln">&nbsp;&nbsp;prod&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">sum</span><span class="pun">(</span><span class="pln">prod</span><span class="pun">)))</span><span class="pln">&nbsp;</span><span class="com">#append&nbsp;one&nbsp;more&nbsp;to&nbsp;gamma!!!</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;newpi&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;gamma</span><span class="pun">[:,</span><span class="lit">0</span><span class="pun">]</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;newA&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">sum</span><span class="pun">(</span><span class="pln">xi</span><span class="pun">,</span><span class="lit">2</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">sum</span><span class="pun">(</span><span class="pln">gamma</span><span class="pun">[:,:-</span><span class="lit">1</span><span class="pun">],</span><span class="pln">axis</span><span class="pun">=</span><span class="lit">1</span><span class="pun">).</span><span class="pln">reshape</span><span class="pun">((-</span><span class="lit">1</span><span class="pun">,</span><span class="lit">1</span><span class="pun">))</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;newB&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">copy</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B</span><span class="pun">)</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_levels&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B</span><span class="pun">.</span><span class="pln">shape</span><span class="pun">[</span><span class="lit">1</span><span class="pun">]</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sumgamma&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">sum</span><span class="pun">(</span><span class="pln">gamma</span><span class="pun">,</span><span class="pln">axis</span><span class="pun">=</span><span class="lit">1</span><span class="pun">)</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;lev&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="pln">num_levels</span><span class="pun">):</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;observations&nbsp;</span><span class="pun">==</span><span class="pln">&nbsp;lev</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;newB</span><span class="pun">[:,</span><span class="pln">lev</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">sum</span><span class="pun">(</span><span class="pln">gamma</span><span class="pun">[:,</span><span class="pln">mask</span><span class="pun">],</span><span class="pln">axis</span><span class="pun">=</span><span class="lit">1</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;sumgamma</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">if</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">max</span><span class="pun">(</span><span class="pln">abs</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">pi&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;newpi</span><span class="pun">))</span><span class="pln">&nbsp;</span><span class="pun">&lt;</span><span class="pln">&nbsp;criterion&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;\</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;np</span><span class="pun">.</span><span class="pln">max</span><span class="pun">(</span><span class="pln">abs</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">A&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;newA</span><span class="pun">))</span><span class="pln">&nbsp;</span><span class="pun">&lt;</span><span class="pln">&nbsp;criterion&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;\</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;np</span><span class="pun">.</span><span class="pln">max</span><span class="pun">(</span><span class="pln">abs</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;newB</span><span class="pun">))</span><span class="pln">&nbsp;</span><span class="pun">&lt;</span><span class="pln">&nbsp;criterion</span><span class="pun">:</span></li><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">done</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1</span></li><li class="L1"><span class="pln">&nbsp;</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">A</span><span class="pun">[:],</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B</span><span class="pun">[:],</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">pi</span><span class="pun">[:]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;newA</span><span class="pun">,</span><span class="pln">newB</span><span class="pun">,</span><span class="pln">newpi</span></li></ol></pre>
<p style="text-indent: 2em;">代码有点长，一段一段地看。</p>
<p style="text-indent: 2em;">先拿到前后向概率：</p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="pln">alpha&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">_forward</span><span class="pun">(</span><span class="pln">observations</span><span class="pun">)</span></li><li class="L1"><span class="pln">beta&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">_backward</span><span class="pun">(</span><span class="pln">observations</span><span class="pun">)</span></li></ol></pre>
<p style="text-indent: 2em;">然后计算<img src="./hidden-markov-model_files/6cbb8645gw1f6jwxqgow8j20k205omxl.jpg" title="屏幕快照 2016-08-06 下午12.27.35.png" alt="屏幕快照 2016-08-06 下午12.27.35.png" width="273" height="77" border="0" vspace="0" style="text-align: center; white-space: normal; width: 273px; height: 77px;" data-tag="bdshare">：</p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="pln">xi&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">zeros</span><span class="pun">((</span><span class="pln">n_states</span><span class="pun">,</span><span class="pln">n_states</span><span class="pun">,</span><span class="pln">n_samples</span><span class="pun">-</span><span class="lit">1</span><span class="pun">))</span></li><li class="L1"><span class="kwd">for</span><span class="pln">&nbsp;t&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="pln">n_samples</span><span class="pun">-</span><span class="lit">1</span><span class="pun">):</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;denom&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">dot</span><span class="pun">(</span><span class="pln">np</span><span class="pun">.</span><span class="pln">dot</span><span class="pun">(</span><span class="pln">alpha</span><span class="pun">[:,</span><span class="pln">t</span><span class="pun">].</span><span class="pln">T</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">A</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B</span><span class="pun">[:,</span><span class="pln">observations</span><span class="pun">[</span><span class="pln">t</span><span class="pun">+</span><span class="lit">1</span><span class="pun">]].</span><span class="pln">T</span><span class="pun">,</span><span class="pln">&nbsp;beta</span><span class="pun">[:,</span><span class="pln">t</span><span class="pun">+</span><span class="lit">1</span><span class="pun">])</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;i&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="pln">n_states</span><span class="pun">):</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;numer&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;alpha</span><span class="pun">[</span><span class="pln">i</span><span class="pun">,</span><span class="pln">t</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">A</span><span class="pun">[</span><span class="pln">i</span><span class="pun">,:]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B</span><span class="pun">[:,</span><span class="pln">observations</span><span class="pun">[</span><span class="pln">t</span><span class="pun">+</span><span class="lit">1</span><span class="pun">]].</span><span class="pln">T&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;beta</span><span class="pun">[:,</span><span class="pln">t</span><span class="pun">+</span><span class="lit">1</span><span class="pun">].</span><span class="pln">T</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;xi</span><span class="pun">[</span><span class="pln">i</span><span class="pun">,:,</span><span class="pln">t</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;numer&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;denom</span></li></ol></pre>
<p style="text-indent: 2em;">注意xi的下标t少了一个，这是因为对于t=T，没法用t+1去定位后向概率。所以这一个时刻是这么计算的：</p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="com">#&nbsp;gamma_t(i)&nbsp;=&nbsp;P(q_t&nbsp;=&nbsp;S_i&nbsp;|&nbsp;O,&nbsp;hmm)</span></li><li class="L1"><span class="pln">gamma&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">sum</span><span class="pun">(</span><span class="pln">xi</span><span class="pun">,</span><span class="pln">axis</span><span class="pun">=</span><span class="lit">1</span><span class="pun">)</span></li><li class="L2"><span class="com">#&nbsp;Need&nbsp;final&nbsp;gamma&nbsp;element&nbsp;for&nbsp;new&nbsp;B</span></li><li class="L3"><span class="pln">prod&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;&nbsp;</span><span class="pun">(</span><span class="pln">alpha</span><span class="pun">[:,</span><span class="pln">n_samples</span><span class="pun">-</span><span class="lit">1</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;beta</span><span class="pun">[:,</span><span class="pln">n_samples</span><span class="pun">-</span><span class="lit">1</span><span class="pun">]).</span><span class="pln">reshape</span><span class="pun">((-</span><span class="lit">1</span><span class="pun">,</span><span class="lit">1</span><span class="pun">))</span></li><li class="L4"><span class="pln">gamma&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">hstack</span><span class="pun">((</span><span class="pln">gamma</span><span class="pun">,</span><span class="pln">&nbsp;&nbsp;prod&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">sum</span><span class="pun">(</span><span class="pln">prod</span><span class="pun">)))</span><span class="pln">&nbsp;</span><span class="com">#append&nbsp;one&nbsp;more&nbsp;to&nbsp;gamma!!!</span></li></ol></pre>
<p style="text-indent: 2em;">gamma有了，于是<img src="./hidden-markov-model_files/6cbb8645gw1f6m5pkqpk2j206u01u3yc.jpg" title="屏幕快照 2016-08-08 上午11.02.07.png" alt="屏幕快照 2016-08-08 上午11.02.07.png" width="86" height="23" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 86px; height: 23px;" data-tag="bdshare">取下标1得到新的pi：</p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="pln">newpi&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;gamma</span><span class="pun">[:,</span><span class="lit">0</span><span class="pun">]</span></li></ol></pre>
<p style="text-indent: 2em;">xi求和除以gamma求和得到新的A：</p>
<p style="text-indent: 2em;"><img src="./hidden-markov-model_files/6cbb8645gw1f6jx33okzrj20bi06ydfx.jpg" title="屏幕快照 2016-08-06 下午12.32.49.png" alt="屏幕快照 2016-08-06 下午12.32.49.png" width="184" height="111" border="0" vspace="0" style="text-align: center; white-space: normal; width: 184px; height: 111px;" data-tag="bdshare"></p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="pln">newA&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">sum</span><span class="pun">(</span><span class="pln">xi</span><span class="pun">,</span><span class="lit">2</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">sum</span><span class="pun">(</span><span class="pln">gamma</span><span class="pun">[:,:-</span><span class="lit">1</span><span class="pun">],</span><span class="pln">axis</span><span class="pun">=</span><span class="lit">1</span><span class="pun">).</span><span class="pln">reshape</span><span class="pun">((-</span><span class="lit">1</span><span class="pun">,</span><span class="lit">1</span><span class="pun">))</span></li></ol></pre>
<p style="text-indent: 2em;" dir="ltr">利用下式得到新的B：</p>
<p style="text-indent: 2em;" dir="ltr"><img src="./hidden-markov-model_files/6cbb8645gw1f6m5tsegtij20cc06mdfx.jpg" title="屏幕快照 2016-08-08 上午11.06.22.png" alt="屏幕快照 2016-08-08 上午11.06.22.png" width="186" height="100" border="0" vspace="0" style="width: 186px; height: 100px;" data-tag="bdshare"></p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="pln">num_levels&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B</span><span class="pun">.</span><span class="pln">shape</span><span class="pun">[</span><span class="lit">1</span><span class="pun">]</span></li><li class="L1"><span class="pln">sumgamma&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">sum</span><span class="pun">(</span><span class="pln">gamma</span><span class="pun">,</span><span class="pln">axis</span><span class="pun">=</span><span class="lit">1</span><span class="pun">)</span></li><li class="L2"><span class="kwd">for</span><span class="pln">&nbsp;lev&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="pln">num_levels</span><span class="pun">):</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;mask&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;observations&nbsp;</span><span class="pun">==</span><span class="pln">&nbsp;lev</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;newB</span><span class="pun">[:,</span><span class="pln">lev</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">sum</span><span class="pun">(</span><span class="pln">gamma</span><span class="pun">[:,</span><span class="pln">mask</span><span class="pun">],</span><span class="pln">axis</span><span class="pun">=</span><span class="lit">1</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;sumgamma</span></li></ol></pre>
<p style="text-indent: 2em;">接着检查是否满足终止阈值，否则继续下一轮训练。</p>
<p style="text-indent: 2em;">回到诊所的例子，我们可以用这样一串代码完成Baum-Welch算法的训练，并且评估其准确率：</p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="com">#&nbsp;run&nbsp;a&nbsp;baum_welch_train</span></li><li class="L1"><span class="pln">observations_data</span><span class="pun">,</span><span class="pln">&nbsp;states_data&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;h</span><span class="pun">.</span><span class="pln">simulate</span><span class="pun">(</span><span class="lit">100</span><span class="pun">)</span></li><li class="L2"><span class="com">#&nbsp;print&nbsp;observations_data</span></li><li class="L3"><span class="com">#&nbsp;print&nbsp;states_data</span></li><li class="L4"><span class="pln">guess&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;hmm</span><span class="pun">.</span><span class="pln">HMM</span><span class="pun">(</span><span class="pln">np</span><span class="pun">.</span><span class="pln">array</span><span class="pun">([[</span><span class="lit">0.5</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">0.5</span><span class="pun">],</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">[</span><span class="lit">0.5</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">0.5</span><span class="pun">]]),</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;np</span><span class="pun">.</span><span class="pln">array</span><span class="pun">([[</span><span class="lit">0.3</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">0.3</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">0.3</span><span class="pun">],</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">[</span><span class="lit">0.3</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">0.3</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">0.3</span><span class="pun">]]),</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;np</span><span class="pun">.</span><span class="pln">array</span><span class="pun">([</span><span class="lit">0.5</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">0.5</span><span class="pun">])</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="pun">)</span></li><li class="L0"><span class="pln">guess</span><span class="pun">.</span><span class="pln">baum_welch_train</span><span class="pun">(</span><span class="pln">observations_data</span><span class="pun">)</span></li><li class="L1"><span class="pln">states_out&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;guess</span><span class="pun">.</span><span class="pln">state_path</span><span class="pun">(</span><span class="pln">observations_data</span><span class="pun">)[</span><span class="lit">1</span><span class="pun">]</span></li><li class="L2"><span class="pln">p&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0.0</span></li><li class="L3"><span class="kwd">for</span><span class="pln">&nbsp;s&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;states_data</span><span class="pun">:</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">if</span><span class="pln">&nbsp;</span><span class="kwd">next</span><span class="pun">(</span><span class="pln">states_out</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">==</span><span class="pln">&nbsp;s</span><span class="pun">:</span><span class="pln">&nbsp;p&nbsp;</span><span class="pun">+=</span><span class="pln">&nbsp;</span><span class="lit">1</span></li><li class="L5"><span class="pln">&nbsp;</span></li><li class="L6"><span class="kwd">print</span><span class="pln">&nbsp;p&nbsp;</span><span class="pun">/</span><span class="pln">&nbsp;len</span><span class="pun">(</span><span class="pln">states_data</span><span class="pun">)</span></li></ol></pre>
<p style="text-indent: 2em;">输出：</p>
<pre class="prettyprint lang-plain linenums"><ol class="linenums"><li class="L0"><span class="lit">0.58</span></li></ol></pre>
<p style="text-indent: 2em;">视simulate出来的随机数据不同，准确率在40%-70%之间波动。这其实说明对于这个例子，无监督学习并不靠谱，只能全靠创业团队的PPT了。</p>
<p style="text-indent: 2em;">另外，由于这是一份来自<a href="http://www.cs.colostate.edu/~anderson/cs440/index.html/doku.php?id=notes:hmm2" target="_blank" rel="external nofollow">colostate大学</a>的教学代码，全部运算都是浮点数乘法，没有取对数，所以在数据量较大的时候可能发生除零错误。</p>
<h2 id="h2-18">预测算法</h2>
<p style="text-indent: 2em;">下面介绍隐马尔可夫模型预测的两种算法：近似算法与维特比算法（Viterbi algorithm)。</p>
<h3 id="h3-19">近似算法</h3>
<p style="text-indent: 2em;">近似算法的想法是，在每个时刻t选择在该时刻最有可能出现的状态<img src="./hidden-markov-model_files/6cbb8645gw1f6jx983r4hj201a0220sh.jpg" title="屏幕快照 2016-08-06 下午12.38.43.png" alt="屏幕快照 2016-08-06 下午12.38.43.png" width="21" height="33" border="0" vspace="0" style="width: 21px; height: 33px;" data-tag="bdshare">，从而得到一个状态序列<img src="./hidden-markov-model_files/6cbb8645gw1f6jx9h7q24j208s01y745.jpg" title="屏幕快照 2016-08-06 下午12.38.58.png" alt="屏幕快照 2016-08-06 下午12.38.58.png" width="113" height="25" border="0" vspace="0" style="width: 113px; height: 25px;" data-tag="bdshare">，将它作为预测的结果。</p>
<p style="text-indent: 2em;">给定隐马尔可夫模型<img src="./hidden-markov-model_files/6cbb8645gw1f6jxa0q026j201a01q0sh.jpg" title="屏幕快照 2016-08-06 下午12.39.32.png" alt="屏幕快照 2016-08-06 下午12.39.32.png" width="24" height="33" border="0" vspace="0" style="width: 24px; height: 33px;" data-tag="bdshare">和观测序列<img src="./hidden-markov-model_files/6cbb8645gw1f6jxaffnenj201601g0sh.jpg" title="屏幕快照 2016-08-06 下午12.39.55.png" alt="屏幕快照 2016-08-06 下午12.39.55.png" width="18" height="22" border="0" vspace="0" style="width: 18px; height: 22px;" data-tag="bdshare">，在时刻t处于状态<img src="./hidden-markov-model_files/6cbb8645gw1f6jxb31nf0j201801w0sh.jpg" title="屏幕快照 2016-08-06 下午12.40.31.png" alt="屏幕快照 2016-08-06 下午12.40.31.png" width="21" height="33" border="0" vspace="0" style="width: 21px; height: 33px;" data-tag="bdshare">的概率<img src="./hidden-markov-model_files/6cbb8645gw1f6jxbb18ayj202i020742.jpg" title="屏幕快照 2016-08-06 下午12.40.45.png" alt="屏幕快照 2016-08-06 下午12.40.45.png" width="41" height="33" border="0" vspace="0" style="width: 41px; height: 33px;" data-tag="bdshare">是</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6jxc6bwvpj20hy05iaae.jpg" title="屏幕快照 2016-08-06 下午12.41.29.png" alt="屏幕快照 2016-08-06 下午12.41.29.png" width="215" height="66" border="0" vspace="0" style="width: 215px; height: 66px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">在每一时刻t最优可能的状态<img src="./hidden-markov-model_files/6cbb8645gw1f6jx983r4hj201a0220sh.jpg" title="屏幕快照 2016-08-06 下午12.38.43.png" alt="屏幕快照 2016-08-06 下午12.38.43.png" width="21" height="33" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 21px; height: 33px;" data-tag="bdshare">是</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6jxdbf0ttj20hs02aaa3.jpg" title="屏幕快照 2016-08-06 下午12.42.37.png" alt="屏幕快照 2016-08-06 下午12.42.37.png" width="258" height="33" border="0" vspace="0" style="width: 258px; height: 33px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">从而得到状态序列<img src="./hidden-markov-model_files/6cbb8645gw1f6jx9h7q24j208s01y745.jpg" title="屏幕快照 2016-08-06 下午12.38.58.png" alt="屏幕快照 2016-08-06 下午12.38.58.png" width="99" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 99px; height: 22px;" data-tag="bdshare">。</p>
<p style="text-indent: 2em;">近似算法的优点是计算简单，其缺点是不能保证预测的状态序列整体是最有可能的状态序列，因为预测的状态序列可能有实际不发生的部分。事实上，上述方法得到的状态序列中有可能存在转移概率为0的相邻状态，即对某些<img src="./hidden-markov-model_files/6cbb8645gw1f6jxl22zwaj203q0203ya.jpg" title="屏幕快照 2016-08-06 下午12.50.04.png" alt="屏幕快照 2016-08-06 下午12.50.04.png" width="61" height="33" border="0" vspace="0" style="width: 61px; height: 33px;" data-tag="bdshare">时。尽管如此，近似算法仍然是有用的（没看出来有什么用<img src="./hidden-markov-model_files/i_f12.gif" data-tag="bdshare">）。</p>
<h3 id="h3-20">维特比算法</h3>
<p style="text-indent: 2em;">维特比算法实际是用动态规划解隐马尔可夫模型预测问题，即用动态规划(dynamic programming)求概率最大路径（最优路径）。这时一条路径对应着一个状态序列。</p>
<p style="text-indent: 2em;">根据动态规划原理，最优路径具有这样的特性：如果最优路径在时刻t通过结点<img src="./hidden-markov-model_files/6cbb8645gw1f6jx983r4hj201a0220sh.jpg" title="屏幕快照 2016-08-06 下午12.38.43.png" alt="屏幕快照 2016-08-06 下午12.38.43.png" width="21" height="33" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 21px; height: 33px;" data-tag="bdshare">,那么这一路径从结点<img src="./hidden-markov-model_files/6cbb8645gw1f6jx983r4hj201a0220sh.jpg" title="屏幕快照 2016-08-06 下午12.38.43.png" alt="屏幕快照 2016-08-06 下午12.38.43.png" width="21" height="33" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 21px; height: 33px;" data-tag="bdshare">到终点<img src="./hidden-markov-model_files/6cbb8645gw1f6k228sap2j201201s0sh.jpg" title="屏幕快照 2016-08-06 下午3.24.25.png" alt="屏幕快照 2016-08-06 下午3.24.25.png" width="20" height="33" border="0" vspace="0" style="width: 20px; height: 33px;" data-tag="bdshare">的部分路径，对于从<img src="./hidden-markov-model_files/6cbb8645gw1f6jx983r4hj201a0220sh.jpg" title="屏幕快照 2016-08-06 下午12.38.43.png" alt="屏幕快照 2016-08-06 下午12.38.43.png" width="21" height="33" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 21px; height: 33px;" data-tag="bdshare">到<img src="./hidden-markov-model_files/6cbb8645gw1f6k228sap2j201201s0sh.jpg" title="屏幕快照 2016-08-06 下午3.24.25.png" alt="屏幕快照 2016-08-06 下午3.24.25.png" width="20" height="33" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 20px; height: 33px;" data-tag="bdshare">的所有可能的部分路径来说，必须是最优的。因为假如不是这样，那么从<img src="./hidden-markov-model_files/6cbb8645gw1f6jx983r4hj201a0220sh.jpg" title="屏幕快照 2016-08-06 下午12.38.43.png" alt="屏幕快照 2016-08-06 下午12.38.43.png" width="21" height="33" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 21px; height: 33px;" data-tag="bdshare">到<img src="./hidden-markov-model_files/6cbb8645gw1f6k228sap2j201201s0sh.jpg" title="屏幕快照 2016-08-06 下午3.24.25.png" alt="屏幕快照 2016-08-06 下午3.24.25.png" width="20" height="33" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 20px; height: 33px;" data-tag="bdshare">就有另一条更好的部分路径存在，如果把它和从<img src="./hidden-markov-model_files/6cbb8645gw1f6k24mx5enj201e01s741.jpg" title="屏幕快照 2016-08-06 下午3.27.18.png" alt="屏幕快照 2016-08-06 下午3.27.18.png" width="26" height="33" border="0" vspace="0" style="width: 26px; height: 33px;" data-tag="bdshare">到达<img src="./hidden-markov-model_files/6cbb8645gw1f6jx983r4hj201a0220sh.jpg" title="屏幕快照 2016-08-06 下午12.38.43.png" alt="屏幕快照 2016-08-06 下午12.38.43.png" width="21" height="33" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 21px; height: 33px;" data-tag="bdshare">的部分路径连接起来，就会形成一条比原来的路径更优的路径，这是矛盾的。依据这一原理，我们只需从时刻t=l开始，递推地计算在时刻t状态为i的各条部分路径的最大概率，直至得到时刻<img src="./hidden-markov-model_files/6cbb8645gw1f6k27alqmrj202u01mjr5.jpg" title="屏幕快照 2016-08-06 下午3.29.48.png" alt="屏幕快照 2016-08-06 下午3.29.48.png" width="39" height="22" border="0" vspace="0" style="width: 39px; height: 22px;" data-tag="bdshare">状态为i的各条路径的最大概率。时刻<img src="./hidden-markov-model_files/6cbb8645gw1f6k27alqmrj202u01mjr5.jpg" title="屏幕快照 2016-08-06 下午3.29.48.png" alt="屏幕快照 2016-08-06 下午3.29.48.png" width="39" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 39px; height: 22px;" data-tag="bdshare">的最大概率即为最优路径的概率<img src="./hidden-markov-model_files/6cbb8645gw1f6k2856xu1j201i01m0sh.jpg" title="屏幕快照 2016-08-06 下午3.30.40.png" alt="屏幕快照 2016-08-06 下午3.30.40.png" width="21" height="22" border="0" vspace="0" style="width: 21px; height: 22px;" data-tag="bdshare">,最优路径的终结点<img src="./hidden-markov-model_files/6cbb8645gw1f6jx983r4hj201a0220sh.jpg" title="屏幕快照 2016-08-06 下午12.38.43.png" alt="屏幕快照 2016-08-06 下午12.38.43.png" width="21" height="33" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 21px; height: 33px;" data-tag="bdshare">也同时得到。之后，为了找出最优路径的各个结点，从终结点<img src="./hidden-markov-model_files/6cbb8645gw1f6jx983r4hj201a0220sh.jpg" title="屏幕快照 2016-08-06 下午12.38.43.png" alt="屏幕快照 2016-08-06 下午12.38.43.png" width="21" height="33" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 21px; height: 33px;" data-tag="bdshare">开始，由后向前逐步求得结点<img src="./hidden-markov-model_files/6cbb8645gw1f6k24mx5enj201e01s741.jpg" title="屏幕快照 2016-08-06 下午3.27.18.png" alt="屏幕快照 2016-08-06 下午3.27.18.png" width="26" height="33" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 26px; height: 33px;" data-tag="bdshare">,得到最优路径这就是维特比算法。</p>
<p style="text-indent: 2em;">首先导入两个变量<img src="./hidden-markov-model_files/6cbb8645gw1f6k29fz9hxj201201m0rk.jpg" title="屏幕快照 2016-08-06 下午3.31.56.png" alt="屏幕快照 2016-08-06 下午3.31.56.png" width="19" height="28" border="0" vspace="0" style="width: 19px; height: 28px;" data-tag="bdshare">和<img src="./hidden-markov-model_files/6cbb8645gw1f6k29ootnqj201a01u0sh.jpg" title="屏幕快照 2016-08-06 下午3.32.09.png" alt="屏幕快照 2016-08-06 下午3.32.09.png" width="20" height="28" border="0" vspace="0" style="width: 20px; height: 28px;" data-tag="bdshare">。定义在时刻t状态为i的所有单个路径<img src="./hidden-markov-model_files/6cbb8645gw1f6k2ae0jw2j205y01st8j.jpg" title="屏幕快照 2016-08-06 下午3.32.50.png" alt="屏幕快照 2016-08-06 下午3.32.50.png" width="93" height="28" border="0" vspace="0" style="width: 93px; height: 28px;" data-tag="bdshare">,<span style="text-indent: 2em;">中概率最大值为</span></p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6k2axr5brj20u002sq35.jpg" title="屏幕快照 2016-08-06 下午3.33.20.png" alt="屏幕快照 2016-08-06 下午3.33.20.png" width="475" height="44" border="0" vspace="0" style="width: 475px; height: 44px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">由定义可得变量<img src="./hidden-markov-model_files/6cbb8645gw1f6k29fz9hxj201201m0rk.jpg" title="屏幕快照 2016-08-06 下午3.31.56.png" alt="屏幕快照 2016-08-06 下午3.31.56.png" width="19" height="28" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 19px; height: 28px;" data-tag="bdshare">的递推公式：</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6k2gxe89zj20xi04w3z3.jpg" title="屏幕快照 2016-08-06 下午3.38.18.png" alt="屏幕快照 2016-08-06 下午3.38.18.png" width="528" height="77" border="0" vspace="0" style="width: 528px; height: 77px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">定义在时刻t状态为i的所有单个路径<img src="./hidden-markov-model_files/6cbb8645gw1f6k2hzvmguj207m020glg.jpg" title="屏幕快照 2016-08-06 下午3.40.01.png" alt="屏幕快照 2016-08-06 下午3.40.01.png" width="107" height="28" border="0" vspace="0" style="width: 107px; height: 28px;" data-tag="bdshare">中概率最大的路径的第t-1个结点为</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6k2irmamcj20n002e74e.jpg" title="屏幕快照 2016-08-06 下午3.40.53.png" alt="屏幕快照 2016-08-06 下午3.40.53.png" width="318" height="33" border="0" vspace="0" style="width: 318px; height: 33px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">下面介绍维特比算法。</p>
<p style="text-indent: 2em;"><strong>算法(维特比算法）</strong></p>
<p style="text-indent: 2em;">输入：模型<img src="./hidden-markov-model_files/6cbb8645gw1f6k2jprjsoj206m01ydfo.jpg" title="屏幕快照 2016-08-06 下午3.41.46.png" alt="屏幕快照 2016-08-06 下午3.41.46.png" width="95" height="28" border="0" vspace="0" style="width: 95px; height: 28px;" data-tag="bdshare">和观测<img src="./hidden-markov-model_files/6cbb8645gw1f6k2k0lb6bj2096028a9x.jpg" title="屏幕快照 2016-08-06 下午3.42.03.png" alt="屏幕快照 2016-08-06 下午3.42.03.png" width="116" height="28" border="0" vspace="0" style="width: 116px; height: 28px;" data-tag="bdshare">;</p>
<p style="text-indent: 2em;">输出：最优路径<img src="./hidden-markov-model_files/6cbb8645gw1f6jx9h7q24j208s01y745.jpg" title="屏幕快照 2016-08-06 下午12.38.58.png" alt="屏幕快照 2016-08-06 下午12.38.58.png" width="99" height="22" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 99px; height: 22px;" data-tag="bdshare">。</p>
<p style="text-indent: 2em;">⑴初始化</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6k2leqvgsj20gi042glr.jpg" title="屏幕快照 2016-08-06 下午3.43.21.png" alt="屏幕快照 2016-08-06 下午3.43.21.png" width="244" height="60" border="0" vspace="0" style="width: 244px; height: 60px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">(2)<span class="Apple-tab-span" style="white-space:pre"></span>递推。对<img src="./hidden-markov-model_files/6cbb8645gw1f6k2lz71sgj206s01mjr7.jpg" title="屏幕快照 2016-08-06 下午3.43.59.png" alt="屏幕快照 2016-08-06 下午3.43.59.png" width="93" height="22" border="0" vspace="0" style="width: 93px; height: 22px;" data-tag="bdshare"></p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6k59b3833j20ny05at98.jpg" title="屏幕快照 2016-08-06 下午5.15.29.png" alt="屏幕快照 2016-08-06 下午5.15.29.png" width="349" height="77" border="0" vspace="0" style="width: 349px; height: 77px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">(3)<span class="Apple-tab-span" style="white-space:pre"></span>终止</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6k5a5x9mgj20b004s0su.jpg" title="屏幕快照 2016-08-06 下午5.16.09.png" alt="屏幕快照 2016-08-06 下午5.16.09.png" width="161" height="70" border="0" vspace="0" style="width: 161px; height: 70px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">(4)<span class="Apple-tab-span" style="white-space:pre"></span>最优路径回溯。对<img src="./hidden-markov-model_files/6cbb8645gw1f6k5p1slc8j20a401qglg.jpg" title="屏幕快照 2016-08-06 下午5.30.40.png" alt="屏幕快照 2016-08-06 下午5.30.40.png" width="147" height="25" border="0" vspace="0" style="width: 147px; height: 25px;" data-tag="bdshare"></p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6k5q611bjj2078028mx0.jpg" title="屏幕快照 2016-08-06 下午5.31.49.png" alt="屏幕快照 2016-08-06 下午5.31.49.png" width="107" height="33" border="0" vspace="0" style="width: 107px; height: 33px;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">求得最优路径<img src="./hidden-markov-model_files/6cbb8645gw1f6jx9h7q24j208s01y745.jpg" title="屏幕快照 2016-08-06 下午12.38.58.png" alt="屏幕快照 2016-08-06 下午12.38.58.png" width="113" height="25" border="0" vspace="0" style="text-indent: 32px; white-space: normal; width: 113px; height: 25px;" data-tag="bdshare">。</p>
<h3 id="h3-21">维特比算法Python实现</h3>
<p style="text-indent: 2em;">不管是监督学习，还是非监督学习，我们反正都训练出了一个HMM模型。现在诊所来了一位病人，他最近三天的病状是：</p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="pln">observations&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="str">'normal'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'cold'</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">'dizzy'</span><span class="pun">)</span></li></ol></pre>
<p style="text-indent: 2em;">如何用Viterbi算法计算他的病情以及相应的概率呢？</p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="kwd">def</span><span class="pln">&nbsp;viterbi</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln">&nbsp;obs_seq</span><span class="pun">):</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="str">"""</span></li><li class="L2"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;Returns</span></li><li class="L3"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;-------</span></li><li class="L4"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;V&nbsp;:&nbsp;numpy.ndarray</span></li><li class="L5"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;V&nbsp;[s][t]&nbsp;=&nbsp;Maximum&nbsp;probability&nbsp;of&nbsp;an&nbsp;observation&nbsp;sequence&nbsp;ending</span></li><li class="L6"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;at&nbsp;time&nbsp;'t'&nbsp;with&nbsp;final&nbsp;state&nbsp;'s'</span></li><li class="L7"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;prev&nbsp;:&nbsp;numpy.ndarray</span></li><li class="L8"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Contains&nbsp;a&nbsp;pointer&nbsp;to&nbsp;the&nbsp;previous&nbsp;state&nbsp;at&nbsp;t-1&nbsp;that&nbsp;maximizes</span></li><li class="L9"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;V[state][t]</span></li><li class="L0"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;"""</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;N&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">A</span><span class="pun">.</span><span class="pln">shape</span><span class="pun">[</span><span class="lit">0</span><span class="pun">]</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;T&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;len</span><span class="pun">(</span><span class="pln">obs_seq</span><span class="pun">)</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;prev&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">zeros</span><span class="pun">((</span><span class="pln">T&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">,</span><span class="pln">&nbsp;N</span><span class="pun">),</span><span class="pln">&nbsp;dtype</span><span class="pun">=</span><span class="kwd">int</span><span class="pun">)</span></li><li class="L4"><span class="pln">&nbsp;</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;DP&nbsp;matrix&nbsp;containing&nbsp;max&nbsp;likelihood&nbsp;of&nbsp;state&nbsp;at&nbsp;a&nbsp;given&nbsp;time</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;V&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">zeros</span><span class="pun">((</span><span class="pln">N</span><span class="pun">,</span><span class="pln">&nbsp;T</span><span class="pun">))</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;V</span><span class="pun">[:,</span><span class="lit">0</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">pi&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B</span><span class="pun">[:,</span><span class="pln">obs_seq</span><span class="pun">[</span><span class="lit">0</span><span class="pun">]]</span></li><li class="L8"><span class="pln">&nbsp;</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;t&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="lit">1</span><span class="pun">,</span><span class="pln">&nbsp;T</span><span class="pun">):</span></li><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;n&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="pln">N</span><span class="pun">):</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;seq_probs&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;V</span><span class="pun">[:,</span><span class="pln">t</span><span class="pun">-</span><span class="lit">1</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">A</span><span class="pun">[:,</span><span class="pln">n</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">B</span><span class="pun">[</span><span class="pln">n</span><span class="pun">,</span><span class="pln">&nbsp;obs_seq</span><span class="pun">[</span><span class="pln">t</span><span class="pun">]]</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prev</span><span class="pun">[</span><span class="pln">t</span><span class="pun">-</span><span class="lit">1</span><span class="pun">,</span><span class="pln">n</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">argmax</span><span class="pun">(</span><span class="pln">seq_probs</span><span class="pun">)</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;V</span><span class="pun">[</span><span class="pln">n</span><span class="pun">,</span><span class="pln">t</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;np</span><span class="pun">.</span><span class="pln">max</span><span class="pun">(</span><span class="pln">seq_probs</span><span class="pun">)</span></li><li class="L4"><span class="pln">&nbsp;</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;V</span><span class="pun">,</span><span class="pln">&nbsp;prev</span></li></ol></pre>
<p style="text-indent: 2em;">这应该是目前最简洁最优雅的实现了，调用方法如下：</p>
<pre class="prettyprint lang-python linenums"><ol class="linenums"><li class="L0"><span class="pln">h&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;hmm</span><span class="pun">.</span><span class="pln">HMM</span><span class="pun">(</span><span class="pln">A</span><span class="pun">,</span><span class="pln">&nbsp;B</span><span class="pun">,</span><span class="pln">&nbsp;pi</span><span class="pun">)</span></li><li class="L1"><span class="pln">V</span><span class="pun">,</span><span class="pln">&nbsp;p&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;h</span><span class="pun">.</span><span class="pln">viterbi</span><span class="pun">(</span><span class="pln">observations_index</span><span class="pun">)</span></li><li class="L2"><span class="kwd">print</span><span class="pln">&nbsp;</span><span class="str">"&nbsp;"</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="lit">7</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="str">"&nbsp;"</span><span class="pun">.</span><span class="pln">join</span><span class="pun">((</span><span class="str">"%10s"</span><span class="pln">&nbsp;</span><span class="pun">%</span><span class="pln">&nbsp;observations_index_label</span><span class="pun">[</span><span class="pln">i</span><span class="pun">])</span><span class="pln">&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;i&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;observations_index</span><span class="pun">)</span></li><li class="L3"><span class="kwd">for</span><span class="pln">&nbsp;s&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="lit">0</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">2</span><span class="pun">):</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">print</span><span class="pln">&nbsp;</span><span class="str">"%7s:&nbsp;"</span><span class="pln">&nbsp;</span><span class="pun">%</span><span class="pln">&nbsp;states_index_label</span><span class="pun">[</span><span class="pln">s</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;</span><span class="str">"&nbsp;"</span><span class="pun">.</span><span class="pln">join</span><span class="pun">(</span><span class="str">"%10s"</span><span class="pln">&nbsp;</span><span class="pun">%</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="str">"%f"</span><span class="pln">&nbsp;</span><span class="pun">%</span><span class="pln">&nbsp;v</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;v&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;V</span><span class="pun">[</span><span class="pln">s</span><span class="pun">])</span></li><li class="L5"><span class="kwd">print</span><span class="pln">&nbsp;</span><span class="str">'\nThe&nbsp;most&nbsp;possible&nbsp;states&nbsp;and&nbsp;probability&nbsp;are:'</span></li><li class="L6"><span class="pln">p</span><span class="pun">,</span><span class="pln">&nbsp;ss&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;h</span><span class="pun">.</span><span class="pln">state_path</span><span class="pun">(</span><span class="pln">observations_index</span><span class="pun">)</span></li><li class="L7"><span class="kwd">for</span><span class="pln">&nbsp;s&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;ss</span><span class="pun">:</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">print</span><span class="pln">&nbsp;states_index_label</span><span class="pun">[</span><span class="pln">s</span><span class="pun">],</span></li><li class="L9"><span class="kwd">print</span><span class="pln">&nbsp;p</span></li></ol></pre>
<p style="text-indent: 2em;">输出结果如下：</p>
<pre class="prettyprint lang-plain linenums"><ol class="linenums"><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normal&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cold&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dizzy</span></li><li class="L1"><span class="typ">Healthy</span><span class="pun">:</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="lit">0.300000</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="lit">0.084000</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="lit">0.005880</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;</span><span class="typ">Fever</span><span class="pun">:</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="lit">0.040000</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="lit">0.027000</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="lit">0.015120</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="typ">The</span><span class="pln">&nbsp;most&nbsp;possible&nbsp;states&nbsp;</span><span class="kwd">and</span><span class="pln">&nbsp;probability&nbsp;are</span></li><li class="L5"><span class="typ">Healthy</span><span class="pln">&nbsp;</span><span class="typ">Healthy</span><span class="pln">&nbsp;</span><span class="typ">Fever</span><span class="pln">&nbsp;</span><span class="lit">0.01512</span></li></ol></pre>
<p style="text-indent: 2em;">对比维基百科的结果：</p>
<pre class="prettyprint lang-plain linenums"><ol class="linenums"><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="lit">0</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="lit">1</span><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="lit">2</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;</span><span class="typ">Healthy</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="lit">0.30000</span><span class="pln">&nbsp;</span><span class="lit">0.08400</span><span class="pln">&nbsp;</span><span class="lit">0.005884</span><span class="pln">&nbsp;</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="typ">Fever</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="lit">0.04000</span><span class="pln">&nbsp;</span><span class="lit">0.02700</span><span class="pln">&nbsp;</span><span class="lit">0.015125</span><span class="pln">&nbsp;</span></li><li class="L3"><span class="typ">The</span><span class="pln">&nbsp;steps&nbsp;of&nbsp;states&nbsp;are&nbsp;</span><span class="typ">Healthy</span><span class="pln">&nbsp;</span><span class="typ">Healthy</span><span class="pln">&nbsp;</span><span class="typ">Fever</span><span class="pln">&nbsp;</span><span class="kwd">with</span><span class="pln">&nbsp;highest&nbsp;probability&nbsp;of&nbsp;</span><span class="lit">0.01512</span></li></ol></pre>
<p style="text-indent: 2em;">两者是完全一致的，对算法有疑问的可以参考这段动画，将代码单步一遍，什么都明白了：</p>
<p style="text-align:center"><img src="./hidden-markov-model_files/6cbb8645gw1f6m6ozfplig20dw07gadn.gif" title="Viterbi_animated_demo.gif" alt="Viterbi_animated_demo.gif" data-tag="bdshare"></p>
<p style="text-indent: 2em;">我还用Java实现过Viterbi算法：<a href="https://github.com/hankcs/Viterbi" _src="https://github.com/hankcs/Viterbi" target="_blank" rel="external nofollow">https://github.com/hankcs/Viterbi</a>&nbsp;。该Java实现同样很简洁，并且附带了上述诊所和天气预测的例子，欢迎Java用户查阅。</p>
<h2 id="h2-22">Reference</h2>
<p style="text-indent: 2em;">《统计学习方法》</p>
<p style="text-indent: 2em;"><a href="https://en.wikipedia.org/wiki/Viterbi_algorithm#Example" _src="https://en.wikipedia.org/wiki/Viterbi_algorithm#Example" target="_blank" rel="external nofollow">https://en.wikipedia.org/wiki/Viterbi_algorithm#Example</a> </p>
<p style="text-indent: 2em;"><a href="https://github.com/aehuynh/hidden-markov-model" _src="https://github.com/aehuynh/hidden-markov-model" target="_blank" rel="external nofollow">https://github.com/aehuynh/hidden-markov-model</a> </p>
<p style="text-indent: 2em;"><a href="http://www.cs.colostate.edu/~anderson/cs440/index.html/doku.php?id=notes:hmm2" _src="http://www.cs.colostate.edu/~anderson/cs440/index.html/doku.php?id=notes:hmm2" target="_blank" rel="external nofollow">http://www.cs.colostate.edu/~anderson/cs440/index.html/doku.php?id=notes:hmm2</a></p>
<p class="post-copyright"><a href="http://www.hankcs.com/license/" target="_blank"><img alt="知识共享许可协议" style="border-width: 0px;margin: 0 !important;" src="./hidden-markov-model_files/CC-BY-NC-SA-icon-88x31.png" width="88" height="31" border="0" vspace="0" title="知识共享许可协议" data-tag="bdshare"></a>&nbsp;<a href="http://www.hankcs.com/license/" target="_blank" textvalue="知识共享署名-非商业性使用-相同方式共享">知识共享署名-非商业性使用-相同方式共享</a>：<a href="http://www.hankcs.com/">码农场</a> » <a href="http://www.hankcs.com/ml/hidden-markov-model.html">隐马尔可夫模型</a></p>		</article>
								<div class="action-share bdsharebuttonbox bdshare-button-style0-24" data-bd-bind="1497671413981">
			<span>分享到：</span><a class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博"></a><a class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a><a class="bds_bdhome" data-cmd="bdhome" title="分享到百度新首页"></a><a class="bds_tqf" data-cmd="tqf" title="分享到腾讯朋友"></a><a class="bds_renren" data-cmd="renren" title="分享到人人网"></a><a class="bds_diandian" data-cmd="diandian" title="分享到点点网"></a><a class="bds_youdao" data-cmd="youdao" title="分享到有道云笔记"></a><a class="bds_ty" data-cmd="ty" title="分享到天涯社区"></a><a class="bds_kaixin001" data-cmd="kaixin001" title="分享到开心网"></a><a class="bds_taobao" data-cmd="taobao"></a><a class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a class="bds_mail" data-cmd="mail" title="分享到邮件分享"></a><a class="bds_copy" data-cmd="copy" title="分享到复制网址"></a><a class="bds_more" data-cmd="more">更多</a> <span>(</span><a class="bds_count" data-cmd="count" title="累计分享8次">8</a><span>)</span>		</div>
		<div class="article-tags">继续浏览有关 <a href="http://www.hankcs.com/ml/"><i class="fa fa-folder-open"></i> 机器学习</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e7%bb%9f%e8%ae%a1%e5%ad%a6%e4%b9%a0%e6%96%b9%e6%b3%95%e3%80%8b/" rel="tag">《统计学习方法》</a><a href="http://www.hankcs.com/tag/%e7%bb%b4%e7%89%b9%e6%af%94%e7%ae%97%e6%b3%95/" rel="tag">维特比算法</a> 的文章</div>		<div class="asb asb-post asb-post-02"><script async="" src="./hidden-markov-model_files/adsbygoogle.js.下载"></script>
<!-- 文章页正文下 页首横幅 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-1152644711996772" data-ad-slot="2657945648" data-adsbygoogle-status="done"><ins id="aswift_1_expand" style="display:inline-table;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent"><ins id="aswift_1_anchor" style="display:block;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent"><iframe width="728" height="90" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_1" name="aswift_1" style="left:0;position:absolute;top:0;width:728px;height:90px;" src="./hidden-markov-model_files/saved_resource(1).html"></iframe></ins></ins></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>		<nav class="article-nav">
			<span class="article-nav-prev">上一篇 <a href="http://www.hankcs.com/ml/em-algorithm-and-its-generalization.html" rel="prev">EM算法及其推广</a></span>
			<span class="article-nav-next"><a href="http://www.hankcs.com/ml/l-bfgs.html" rel="next">数值优化：理解L-BFGS算法</a> 下一篇</span>
		</nav>
				<div class="asb asb-post asb-post-03"><script async="" src="./hidden-markov-model_files/adsbygoogle.js.下载"></script>
<!-- 匹配内容 -->
<ins class="adsbygoogle" style="display: block; height: 466px;" data-ad-client="ca-pub-1152644711996772" data-ad-slot="7343699642" data-ad-format="autorelaxed" data-adsbygoogle-status="done"><ins id="aswift_2_expand" style="display:inline-table;border:none;height:466px;margin:0;padding:0;position:relative;visibility:visible;width:778px;background-color:transparent"><ins id="aswift_2_anchor" style="display:block;border:none;height:466px;margin:0;padding:0;position:relative;visibility:visible;width:778px;background-color:transparent"><iframe width="778" height="466" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_2" name="aswift_2" style="left:0;position:absolute;top:0;width:778px;height:466px;" src="./hidden-markov-model_files/saved_resource(2).html"></iframe></ins></ins></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>		<div class="title" id="comments">
	<h3>评论 <b>1</b></h3>
</div>
<div id="respond" class="no_webshot">
		
	<form action="http://www.hankcs.com/wp-comments-post.php" method="post" id="commentform">
		<div class="comt">
			<div class="comt-title">
				<img alt="" data-src="http://2.gravatar.com/avatar/?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://0.gravatar.com/avatar/?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo avatar-default" height="50" width="50" src="./hidden-markov-model_files/saved_resource" style="display: inline;">				<p><a id="cancel-comment-reply-link" href="javascript:;">取消</a></p>
			</div>
			<div class="comt-box">
				<textarea placeholder="此处不受理任何开源项目问题，请在GitHub上发issue ，大家一起讨论，谢谢。" class="input-block-level comt-area" name="comment" id="comment" cols="100%" rows="3" tabindex="1" onkeydown="if(event.ctrlKey&amp;&amp;event.keyCode==13){document.getElementById(&#39;submit&#39;).click();return false};"></textarea>
				<div class="comt-ctrl">
					<div class="comt-tips"><input type="hidden" name="comment_post_ID" value="7471" id="comment_post_ID">
<input type="hidden" name="comment_parent" id="comment_parent" value="0">
<p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="7ff74ead0e"></p><label for="comment_mail_notify" class="checkbox inline hide" style="padding-top:0"><input type="checkbox" name="comment_mail_notify" id="comment_mail_notify" value="comment_mail_notify" checked="checked">有人回复时邮件通知我</label><p style="display: none;"></p><div class="comt-tip comt-loading" style="display: none;">评论提交中...</div><div class="comt-tip comt-error" style="display: none;">#</div></div>
					<button type="submit" name="submit" id="submit" tabindex="5">提交评论</button>
					<!-- <span data-type="comment-insert-smilie" class="muted comt-smilie"><i class="icon-thumbs-up icon12"></i> 表情</span> -->
				</div>
			</div>

												<div class="comt-comterinfo" id="comment-author-info">
						<ul>
							<li class="form-inline"><label class="hide" for="author">昵称</label><input class="ipt" type="text" name="author" id="author" value="" tabindex="2" placeholder="昵称"><span class="text-muted">昵称 (必填)</span></li>
							<li class="form-inline"><label class="hide" for="email">邮箱</label><input class="ipt" type="text" name="email" id="email" value="" tabindex="3" placeholder="邮箱"><span class="text-muted">邮箱 (必填)</span></li>
							<li class="form-inline"><label class="hide" for="url">网址</label><input class="ipt" type="text" name="url" id="url" value="" tabindex="4" placeholder="网址"><span class="text-muted">网址</span></li>
						</ul>
					</div>
									</div>

	<input type="hidden" id="ak_js" name="ak_js" value="1497671413617"></form>
	</div>
<div id="postcomments">
	<ol class="commentlist">
		<li class="comment even thread-even depth-1" id="comment-4296"><span class="comt-f">#1</span><div class="comt-avatar"><img alt="" data-src="http://2.gravatar.com/avatar/?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://1.gravatar.com/avatar/?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo avatar-default" height="50" width="50" src="./hidden-markov-model_files/saved_resource" style="display: block;"></div><div class="comt-main" id="div-comment-4296"><p>终于等到更新鸟</p>
<div class="comt-meta"><span class="comt-author"><a href="http://t.qq.com/fanshujiang00" rel="external nofollow" class="url" target="_blank">番薯江00</a></span>10个月前 (08-11)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-4296&quot;, &quot;4296&quot;, &quot;respond&quot;, &quot;7471&quot; )" aria-label="回复给番薯江00">回复</a></div></div></li><!-- #comment-## -->
	</ol>
	<div class="pagenav">
			</div>
</div>
	</div>
	</div>
	<aside class="sidebar">
<div class="widget widget_categories affix-top" style="top: 0px;"><h3>栏目分类</h3><label class="screen-reader-text" for="cat">栏目分类</label><select name="cat" id="cat" class="postform">
	<option value="-1">选择分类目录</option>
	<option class="level-0" value="18">ACG&nbsp;&nbsp;(7)</option>
	<option class="level-1" value="117">&nbsp;&nbsp;&nbsp;游戏&nbsp;&nbsp;(5)</option>
	<option class="level-0" value="7">Web开发&nbsp;&nbsp;(80)</option>
	<option class="level-1" value="64">&nbsp;&nbsp;&nbsp;BAE&nbsp;&nbsp;(13)</option>
	<option class="level-1" value="11">&nbsp;&nbsp;&nbsp;Linux相关&nbsp;&nbsp;(9)</option>
	<option class="level-1" value="54">&nbsp;&nbsp;&nbsp;Mac OS&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="27">&nbsp;&nbsp;&nbsp;WordPress&nbsp;&nbsp;(8)</option>
	<option class="level-1" value="65">&nbsp;&nbsp;&nbsp;Yii&nbsp;&nbsp;(17)</option>
	<option class="level-1" value="2">&nbsp;&nbsp;&nbsp;主机域名&nbsp;&nbsp;(26)</option>
	<option class="level-1" value="66">&nbsp;&nbsp;&nbsp;数据库&nbsp;&nbsp;(4)</option>
	<option class="level-0" value="140">信息安全&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="1">其他类别&nbsp;&nbsp;(184)</option>
	<option class="level-1" value="78">&nbsp;&nbsp;&nbsp;心情&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="15">&nbsp;&nbsp;&nbsp;旧的博文&nbsp;&nbsp;(170)</option>
	<option class="level-0" value="87">操作系统&nbsp;&nbsp;(3)</option>
	<option class="level-1" value="88">&nbsp;&nbsp;&nbsp;Windows&nbsp;&nbsp;(2)</option>
	<option class="level-0" value="81">数学基礎&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="4">日语教程&nbsp;&nbsp;(120)</option>
	<option class="level-1" value="96">&nbsp;&nbsp;&nbsp;口译&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="59">&nbsp;&nbsp;&nbsp;新编日语商务贸易会话&nbsp;&nbsp;(14)</option>
	<option class="level-1" value="19">&nbsp;&nbsp;&nbsp;新编日语阅读文选&nbsp;&nbsp;(34)</option>
	<option class="level-2" value="44">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一册&nbsp;&nbsp;(20)</option>
	<option class="level-2" value="61">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第三册&nbsp;&nbsp;(2)</option>
	<option class="level-2" value="20">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第二册&nbsp;&nbsp;(10)</option>
	<option class="level-1" value="46">&nbsp;&nbsp;&nbsp;日语入门&nbsp;&nbsp;(2)</option>
	<option class="level-1" value="62">&nbsp;&nbsp;&nbsp;日语听力&nbsp;&nbsp;(2)</option>
	<option class="level-1" value="5">&nbsp;&nbsp;&nbsp;日语综合教程&nbsp;&nbsp;(64)</option>
	<option class="level-2" value="120">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第七册&nbsp;&nbsp;(14)</option>
	<option class="level-2" value="50">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第三册&nbsp;&nbsp;(7)</option>
	<option class="level-2" value="73">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第五册&nbsp;&nbsp;(12)</option>
	<option class="level-2" value="98">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第六册&nbsp;&nbsp;(18)</option>
	<option class="level-2" value="6">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第四册&nbsp;&nbsp;(12)</option>
	<option class="level-1" value="86">&nbsp;&nbsp;&nbsp;月の珊瑚&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="131">机器学习&nbsp;&nbsp;(56)</option>
	<option class="level-0" value="16">经济人文&nbsp;&nbsp;(19)</option>
	<option class="level-1" value="17">&nbsp;&nbsp;&nbsp;国际贸易理论与政策&nbsp;&nbsp;(9)</option>
	<option class="level-1" value="30">&nbsp;&nbsp;&nbsp;当代世界经济与政治&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="9">编程开发&nbsp;&nbsp;(556)</option>
	<option class="level-1" value="8">&nbsp;&nbsp;&nbsp;Android&nbsp;&nbsp;(30)</option>
	<option class="level-1" value="13">&nbsp;&nbsp;&nbsp;C++&nbsp;&nbsp;(237)</option>
	<option class="level-1" value="25">&nbsp;&nbsp;&nbsp;Drupal&nbsp;&nbsp;(23)</option>
	<option class="level-1" value="10">&nbsp;&nbsp;&nbsp;Java&nbsp;&nbsp;(69)</option>
	<option class="level-1" value="123">&nbsp;&nbsp;&nbsp;Javascript&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="24">&nbsp;&nbsp;&nbsp;PHP&nbsp;&nbsp;(57)</option>
	<option class="level-1" value="94">&nbsp;&nbsp;&nbsp;Python&nbsp;&nbsp;(8)</option>
	<option class="level-1" value="14">&nbsp;&nbsp;&nbsp;汇编逆向&nbsp;&nbsp;(12)</option>
	<option class="level-1" value="70">&nbsp;&nbsp;&nbsp;算法&nbsp;&nbsp;(235)</option>
	<option class="level-1" value="121">&nbsp;&nbsp;&nbsp;网络&nbsp;&nbsp;(6)</option>
	<option class="level-0" value="104">自然语言处理&nbsp;&nbsp;(64)</option>
	<option class="level-1" value="109">&nbsp;&nbsp;&nbsp;中文分词&nbsp;&nbsp;(10)</option>
	<option class="level-1" value="128">&nbsp;&nbsp;&nbsp;句法分析&nbsp;&nbsp;(6)</option>
	<option class="level-1" value="127">&nbsp;&nbsp;&nbsp;命名实体识别&nbsp;&nbsp;(6)</option>
	<option class="level-1" value="105">&nbsp;&nbsp;&nbsp;语料库&nbsp;&nbsp;(4)</option>
	<option class="level-0" value="12">软件发布&nbsp;&nbsp;(9)</option>
</select>

<script type="text/javascript">
/* <![CDATA[ */
(function() {
	var dropdown = document.getElementById( "cat" );
	function onCatChange() {
		if ( dropdown.options[ dropdown.selectedIndex ].value > 0 ) {
			location.href = "http://www.hankcs.com/?cat=" + dropdown.options[ dropdown.selectedIndex ].value;
		}
	}
	dropdown.onchange = onCatChange;
})();
/* ]]> */
</script>

</div><div class="widget widget_archive" style="top: 0px;"><h3>文章归档</h3>		<label class="screen-reader-text" for="archives-dropdown-5">文章归档</label>
		<select id="archives-dropdown-5" name="archive-dropdown" onchange="document.location.href=this.options[this.selectedIndex].value;">
			
			<option value="">选择月份</option>
				<option value="http://www.hankcs.com/2017/06/"> 2017年六月 &nbsp;(15)</option>
	<option value="http://www.hankcs.com/2017/05/"> 2017年五月 &nbsp;(8)</option>
	<option value="http://www.hankcs.com/2017/03/"> 2017年三月 &nbsp;(12)</option>
	<option value="http://www.hankcs.com/2017/02/"> 2017年二月 &nbsp;(13)</option>
	<option value="http://www.hankcs.com/2017/01/"> 2017年一月 &nbsp;(22)</option>
	<option value="http://www.hankcs.com/2016/12/"> 2016年十二月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2016/11/"> 2016年十一月 &nbsp;(15)</option>
	<option value="http://www.hankcs.com/2016/10/"> 2016年十月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2016/09/"> 2016年九月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2016/08/"> 2016年八月 &nbsp;(7)</option>
	<option value="http://www.hankcs.com/2016/07/"> 2016年七月 &nbsp;(1)</option>
	<option value="http://www.hankcs.com/2016/06/"> 2016年六月 &nbsp;(1)</option>
	<option value="http://www.hankcs.com/2016/05/"> 2016年五月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2016/04/"> 2016年四月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2016/03/"> 2016年三月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2016/02/"> 2016年二月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2015/12/"> 2015年十二月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2015/11/"> 2015年十一月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2015/10/"> 2015年十月 &nbsp;(4)</option>
	<option value="http://www.hankcs.com/2015/09/"> 2015年九月 &nbsp;(4)</option>
	<option value="http://www.hankcs.com/2015/08/"> 2015年八月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2015/07/"> 2015年七月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2015/05/"> 2015年五月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2015/04/"> 2015年四月 &nbsp;(5)</option>
	<option value="http://www.hankcs.com/2015/03/"> 2015年三月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2015/02/"> 2015年二月 &nbsp;(22)</option>
	<option value="http://www.hankcs.com/2015/01/"> 2015年一月 &nbsp;(14)</option>
	<option value="http://www.hankcs.com/2014/12/"> 2014年十二月 &nbsp;(10)</option>
	<option value="http://www.hankcs.com/2014/11/"> 2014年十一月 &nbsp;(21)</option>
	<option value="http://www.hankcs.com/2014/10/"> 2014年十月 &nbsp;(14)</option>
	<option value="http://www.hankcs.com/2014/09/"> 2014年九月 &nbsp;(16)</option>
	<option value="http://www.hankcs.com/2014/08/"> 2014年八月 &nbsp;(11)</option>
	<option value="http://www.hankcs.com/2014/07/"> 2014年七月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2014/06/"> 2014年六月 &nbsp;(13)</option>
	<option value="http://www.hankcs.com/2014/05/"> 2014年五月 &nbsp;(28)</option>
	<option value="http://www.hankcs.com/2014/04/"> 2014年四月 &nbsp;(41)</option>
	<option value="http://www.hankcs.com/2014/03/"> 2014年三月 &nbsp;(26)</option>
	<option value="http://www.hankcs.com/2014/02/"> 2014年二月 &nbsp;(52)</option>
	<option value="http://www.hankcs.com/2014/01/"> 2014年一月 &nbsp;(28)</option>
	<option value="http://www.hankcs.com/2013/12/"> 2013年十二月 &nbsp;(29)</option>
	<option value="http://www.hankcs.com/2013/11/"> 2013年十一月 &nbsp;(21)</option>
	<option value="http://www.hankcs.com/2013/10/"> 2013年十月 &nbsp;(11)</option>
	<option value="http://www.hankcs.com/2013/09/"> 2013年九月 &nbsp;(19)</option>
	<option value="http://www.hankcs.com/2013/08/"> 2013年八月 &nbsp;(22)</option>
	<option value="http://www.hankcs.com/2013/07/"> 2013年七月 &nbsp;(36)</option>
	<option value="http://www.hankcs.com/2013/06/"> 2013年六月 &nbsp;(24)</option>
	<option value="http://www.hankcs.com/2013/05/"> 2013年五月 &nbsp;(36)</option>
	<option value="http://www.hankcs.com/2013/04/"> 2013年四月 &nbsp;(29)</option>
	<option value="http://www.hankcs.com/2013/03/"> 2013年三月 &nbsp;(46)</option>
	<option value="http://www.hankcs.com/2013/02/"> 2013年二月 &nbsp;(5)</option>
	<option value="http://www.hankcs.com/2012/05/"> 2012年五月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2012/04/"> 2012年四月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2010/12/"> 2010年十二月 &nbsp;(5)</option>
	<option value="http://www.hankcs.com/2010/11/"> 2010年十一月 &nbsp;(10)</option>
	<option value="http://www.hankcs.com/2010/10/"> 2010年十月 &nbsp;(13)</option>
	<option value="http://www.hankcs.com/2010/09/"> 2010年九月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2010/08/"> 2010年八月 &nbsp;(5)</option>
	<option value="http://www.hankcs.com/2010/07/"> 2010年七月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2010/06/"> 2010年六月 &nbsp;(12)</option>
	<option value="http://www.hankcs.com/2010/05/"> 2010年五月 &nbsp;(14)</option>
	<option value="http://www.hankcs.com/2010/04/"> 2010年四月 &nbsp;(8)</option>
	<option value="http://www.hankcs.com/2010/03/"> 2010年三月 &nbsp;(16)</option>
	<option value="http://www.hankcs.com/2010/01/"> 2010年一月 &nbsp;(16)</option>
	<option value="http://www.hankcs.com/2009/12/"> 2009年十二月 &nbsp;(33)</option>
	<option value="http://www.hankcs.com/2009/11/"> 2009年十一月 &nbsp;(26)</option>
	<option value="http://www.hankcs.com/2009/09/"> 2009年九月 &nbsp;(2)</option>

		</select>
		</div><div class="widget widget_ui_posts"><h3>热门文章</h3><ul>		<li><a target="_blank" href="http://www.hankcs.com/ml/machine-learning-entry-list.html"><span class="thumbnail"><img src="./hidden-markov-model_files/6cbb8645gw1ew7s3qoi2uj20h30meaco.jpg" class="thumb" alt="机器学习入门书单" title="机器学习入门书单"></span><span class="text">机器学习入门书单</span><span class="muted">2015-02-04</span><span class="muted">评论(26)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/back-propagation-neural-network.html"><span class="thumbnail"><img src="./hidden-markov-model_files/6cbb8645gw1exsm4yho09j208c044t8o.jpg" class="thumb" alt="反向传播神经网络极简入门" title="反向传播神经网络极简入门"></span><span class="text">反向传播神经网络极简入门</span><span class="muted">2015-11-08</span><span class="muted">评论(25)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/k-nearest-neighbor-method.html"><span class="thumbnail"><img src="./hidden-markov-model_files/6cbb8645jw1eoxj45stqqg20m80godki.gif" class="thumb" alt="k近邻法" title="k近邻法"></span><span class="text">k近邻法</span><span class="muted">2015-02-06</span><span class="muted">评论(11)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/naive-bayesian-method.html"><span class="thumbnail"><img src="./hidden-markov-model_files/6cbb8645jw1eozn0y3spcj20mn0h3q3a.jpg" class="thumb" alt="朴素贝叶斯法" title="朴素贝叶斯法"></span><span class="text">朴素贝叶斯法</span><span class="muted">2015-02-09</span><span class="muted">评论(8)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/crf-code-analysis.html"><span class="thumbnail"><img src="./hidden-markov-model_files/6cbb8645gw1f72dgz2npkj21hc0uhwrp.jpg" class="thumb" alt="CRF++代码分析" title="CRF++代码分析"></span><span class="text">CRF++代码分析</span><span class="muted">2016-08-22</span><span class="muted">评论(8)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/understanding-the-convolution-in-deep-learning.html"><span class="thumbnail"><img src="./hidden-markov-model_files/006Fmjmcly1fdwjpji6qtj30dw05d0t8.jpg" class="thumb" alt="理解深度学习中的卷积" title="理解深度学习中的卷积"></span><span class="text">理解深度学习中的卷积</span><span class="muted">2017-03-24</span><span class="muted">评论(7)</span></a></li>
</ul></div><div class="widget widget_ui_posts" style="top: 0px;"><h3>最新文章</h3><ul>		<li><a target="_blank" href="http://www.hankcs.com/ml/hinton-recent-applications-of-deep-neural-nets.html"><span class="thumbnail"><img src="./hidden-markov-model_files/006Fmjmcly1fga3c5iit4j30n40hg0tw.jpg" class="thumb" alt="Hinton神经网络公开课16 Recent applications of deep neural nets" title="Hinton神经网络公开课16 Recent applications of deep neural nets"></span><span class="text">Hinton神经网络公开课16 Recent applications of deep neural nets</span><span class="muted">2017-06-05</span><span class="muted">评论(0)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/hinton-modeling-hierarchical-structure-with-neural-nets.html"><span class="thumbnail"><img src="./hidden-markov-model_files/006Fmjmcly1fg9g1g542jj30z60n07t8.jpg" class="thumb" alt="Hinton神经网络公开课15 Modeling hierarchical structure with neural nets" title="Hinton神经网络公开课15 Modeling hierarchical structure with neural nets"></span><span class="text">Hinton神经网络公开课15 Modeling hierarchical structure with neural nets</span><span class="muted">2017-06-04</span><span class="muted">评论(0)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/nnml-rbm.html"><span class="thumbnail"><img src="./hidden-markov-model_files/006Fmjmcly1fg8ao6kthsj31kw11x48q.jpg" class="thumb" alt="Hinton神经网络公开课编程练习4 Restricted Boltzmann Machines" title="Hinton神经网络公开课编程练习4 Restricted Boltzmann Machines"></span><span class="text">Hinton神经网络公开课编程练习4 Restricted Boltzmann Machines</span><span class="muted">2017-06-03</span><span class="muted">评论(0)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/hinton-deep-neural-nets-with-generative-pre-training.html"><span class="thumbnail"><img src="./hidden-markov-model_files/006Fmjmcly1fg61rho89qj30p00e6gsm.jpg" class="thumb" alt="Hinton神经网络公开课14 Deep neural nets with generative pre-training" title="Hinton神经网络公开课14 Deep neural nets with generative pre-training"></span><span class="text">Hinton神经网络公开课14 Deep neural nets with generative pre-training</span><span class="muted">2017-06-02</span><span class="muted">评论(0)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/hinton-stacking-rbms-to-make-deep-belief-nets.html"><span class="thumbnail"><img src="./hidden-markov-model_files/006Fmjmcly1fg4sosou7jj30zk0aqac0.jpg" class="thumb" alt="Hinton神经网络公开课13 Stacking RBMs to make Deep Belief Nets" title="Hinton神经网络公开课13 Stacking RBMs to make Deep Belief Nets"></span><span class="text">Hinton神经网络公开课13 Stacking RBMs to make Deep Belief Nets</span><span class="muted">2017-05-31</span><span class="muted">评论(0)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/hinton-rbm.html"><span class="thumbnail"><img src="./hidden-markov-model_files/006Fmjmcly1fg2jgwwqzxj30h70ay0td.jpg" class="thumb" alt="Hinton神经网络公开课12 Restricted Boltzmann machines (RBMs)" title="Hinton神经网络公开课12 Restricted Boltzmann machines (RBMs)"></span><span class="text">Hinton神经网络公开课12 Restricted Boltzmann machines (RBMs)</span><span class="muted">2017-05-29</span><span class="muted">评论(0)</span></a></li>
</ul></div><div class="widget widget_text"><h3>订阅关注</h3>			<div class="textwidget"><iframe width="100%" height="400" class="share_self" frameborder="0" scrolling="no" src="./hidden-markov-model_files/index.html"></iframe></div>
		</div><div class="widget widget_ui_tags" style="top: 0px;"><h3>热门标签</h3><div class="d_tags"><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%8c%91%e6%88%98%e7%a8%8b%e5%ba%8f%e8%ae%be%e8%ae%a1%e7%ab%9e%e8%b5%9b%e7%ac%ac2%e7%89%88%e3%80%8b/">《挑战程序设计竞赛(第2版)》 (184)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%97%a5%e8%af%ad%e7%bb%bc%e5%90%88%e6%95%99%e7%a8%8b%e3%80%8b/">《日语综合教程》 (57)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%96%b0%e7%bc%96%e6%97%a5%e8%af%ad%e9%98%85%e8%af%bb%e6%96%87%e9%80%89%e3%80%8b/">《新编日语阅读文选》 (34)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%99%ba%e8%83%bdweb%e7%ae%97%e6%b3%95%e3%80%8b/">《智能Web算法》 (20)</a><a href="http://www.hankcs.com/tag/neural-networks-for-machine-learning/">Neural Networks for Machine Learning (19)</a><a href="http://www.hankcs.com/tag/%e4%b8%ad%e6%96%87%e5%88%86%e8%af%8d/">中文分词 (18)</a><a href="http://www.hankcs.com/tag/wordpress/">WordPress (17)</a><a href="http://www.hankcs.com/tag/lucene/">Lucene (15)</a><a href="http://www.hankcs.com/tag/%e7%bb%b4%e7%89%b9%e6%af%94%e7%ae%97%e6%b3%95/">维特比算法 (15)</a><a href="http://www.hankcs.com/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0/">深度学习 (15)</a><a href="http://www.hankcs.com/tag/%e6%96%b0%e7%bc%96%e6%97%a5%e8%af%ad%e5%95%86%e5%8a%a1%e8%b4%b8%e6%98%93%e4%bc%9a%e8%af%9d/">新编日语商务贸易会话 (14)</a><a href="http://www.hankcs.com/tag/intellij-idea/">IntelliJ IDEA (13)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e7%bb%9f%e8%ae%a1%e5%ad%a6%e4%b9%a0%e6%96%b9%e6%b3%95%e3%80%8b/">《统计学习方法》 (12)</a><a href="http://www.hankcs.com/tag/uva/">UVa (11)</a><a href="http://www.hankcs.com/tag/drupal7%e4%b8%93%e4%b8%9a%e5%bc%80%e5%8f%91%e6%8c%87%e5%8d%97-%e7%ac%ac%e4%b8%89%e7%89%88/">Drupal7专业开发指南 第三版 (10)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%8c%91%e6%88%98%e7%bc%96%e7%a8%8b-%e7%a8%8b%e5%ba%8f%e8%ae%be%e8%ae%a1%e7%ab%9e%e8%b5%9b%e8%ae%ad%e7%bb%83%e6%89%8b%e5%86%8c%e3%80%8b/">《挑战编程-程序设计竞赛训练手册》 (10)</a><a href="http://www.hankcs.com/tag/hmm/">HMM (10)</a><a href="http://www.hankcs.com/tag/cs224n/">CS224n (10)</a><a href="http://www.hankcs.com/tag/matlab/">matlab (9)</a><a href="http://www.hankcs.com/tag/cs229/">CS229 (8)</a><a href="http://www.hankcs.com/tag/google-code-jam/">Google code jam (7)</a><a href="http://www.hankcs.com/tag/%e3%80%8ac%e6%a0%87%e5%87%86%e7%a8%8b%e5%ba%8f%e5%ba%93-%e8%87%aa%e4%bf%ae%e6%95%99%e7%a8%8b%e4%b8%8e%e5%8f%82%e8%80%83%e6%89%8b%e5%86%8c%e3%80%8b/">《C++标准程序库—自修教程与参考手册》 (7)</a><a href="http://www.hankcs.com/tag/crf/">CRF (7)</a><a href="http://www.hankcs.com/tag/word2vec/">word2vec (7)</a><a href="http://www.hankcs.com/tag/yii/">Yii (6)</a><a href="http://www.hankcs.com/tag/webrtc/">WebRTC (5)</a><a href="http://www.hankcs.com/tag/cocos2d-x/">Cocos2d-x (5)</a><a href="http://www.hankcs.com/tag/tensorflow/">TensorFlow (5)</a><a href="http://www.hankcs.com/tag/cnn/">CNN (5)</a><a href="http://www.hankcs.com/tag/android/">Android (4)</a></div></div></aside></section>

<div class="branding branding-black">
	<div class="container">
		<h2>我的开源项目</h2>
		<a target="blank" class="btn btn-lg" href="https://github.com/hankcs/HanLP">HanLP自然语言处理包</a><a target="blank" class="btn btn-lg" href="https://github.com/hankcs/AhoCorasickDoubleArrayTrie">基于DoubleArrayTrie的Aho Corasick自动机</a>	</div>
</div>
<footer class="footer">
	<div class="container">
		<div class="fcode">
					</div>
		<p>© 2017 <a href="http://www.hankcs.com/">码农场</a> &nbsp; <a href="http://www.hankcs.com/sitemap.xml">网站地图</a> &nbsp; <a href="http://www.miitbeian.gov.cn/" target="_blank">沪ICP备14002007号-1</a></p>
		<div style="display:none">
<script language="javascript" type="text/javascript" src="./hidden-markov-model_files/trace.js.下载"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-47205472-1', 'auto');
  ga('send', 'pageview');

</script>
<script language="javascript" type="text/javascript" src="./hidden-markov-model_files/15590612.js.下载"></script><a href="http://www.51.la/?15590612" target="_blank" title="51.La 网站流量统计系统"><img alt="51.La 网站流量统计系统" src="./hidden-markov-model_files/icon_2.gif" style="border:none"></a>

<noscript>&lt;a href="//www.51.la/?15590612" target="_blank"&gt;&lt;img alt="&amp;#x6211;&amp;#x8981;&amp;#x5566;&amp;#x514D;&amp;#x8D39;&amp;#x7EDF;&amp;#x8BA1;" src="//img.users.51.la/15590612.asp" style="border:none" /&gt;&lt;/a&gt;</noscript>
</div>	</div>
</footer>

<script>
window.jsui={
    www: 'http://www.hankcs.com',
    uri: 'http://www.hankcs.com/wp-content/themes/dux',
    ver: '1.3',
	roll: ["1","2","6","4"],
    ajaxpager: '500',
    url_rp: 'http://www.hankcs.com/about/'
};
</script>
<script type="text/javascript" src="./hidden-markov-model_files/form.js.下载"></script>
<script type="text/javascript" src="./hidden-markov-model_files/bootstrap.min.js.下载"></script>
<script type="text/javascript" src="./hidden-markov-model_files/loader.js.下载"></script>
<script type="text/javascript" src="./hidden-markov-model_files/wp-embed.min.js.下载"></script>

    <div class="m-mask"></div>    <div class="rollbar" style="display: none;"><ul><li><a href="javascript:(scrollTo());"><i class="fa fa-angle-up"></i></a><h6>去顶部<i></i></h6></li><li><a href="javascript:(on_click_toc_button());"><i class="fa fa-list post_open_icon"></i></a><h6 id="toc_label">打开目录</h6></li><li><a href="javascript:(scrollTo(&#39;#comments&#39;,-15));"><i class="fa fa-comments"></i></a><h6>去评论<i></i></h6></li></ul></div><ul class="m-navbar">
			<li id="menu-item-1834" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1834"><a href="http://www.hankcs.com/program/cpp/">C++</a></li>
<li id="menu-item-1835" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1835"><a href="http://www.hankcs.com/program/java/">Java</a></li>
<li id="menu-item-5754" class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent menu-item-5754"><a href="http://www.hankcs.com/ml/">机器学习</a></li>
<li id="menu-item-2954" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-2954"><a href="http://www.hankcs.com/nlp/">NLP</a>
<ul class="sub-menu">
	<li id="menu-item-4344" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4344"><a href="http://www.hankcs.com/nlp/corpus/">语料库</a></li>
	<li id="menu-item-4342" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4342"><a href="http://www.hankcs.com/nlp/segment/">中文分词</a></li>
	<li id="menu-item-4343" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4343"><a href="http://www.hankcs.com/nlp/ner/">命名实体识别</a></li>
	<li id="menu-item-4479" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4479"><a href="http://www.hankcs.com/nlp/parsing/">句法分析</a></li>
</ul>
</li>
<li id="menu-item-1837" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1837"><a href="http://www.hankcs.com/program/algorithm/">算法</a></li>
<li id="menu-item-1839" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1839"><a href="http://www.hankcs.com/software/">软件</a></li>
<li id="menu-item-1838" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1838"><a href="http://www.hankcs.com/nihongonote/">日语</a>
<ul class="sub-menu">
	<li id="menu-item-1860" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1860"><a href="http://www.hankcs.com/nihongonote/riyurimen/">日语入门</a></li>
	<li id="menu-item-1861" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1861"><a href="http://www.hankcs.com/nihongonote/listening/">日语听力</a></li>
	<li id="menu-item-1863" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1863"><a href="http://www.hankcs.com/nihongonote/tekusuto/">日语综合教程</a>
	<ul class="sub-menu">
		<li id="menu-item-2190" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2190"><a href="http://www.hankcs.com/nihongonote/tekusuto/disance/">第三册</a></li>
		<li id="menu-item-2192" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2192"><a href="http://www.hankcs.com/nihongonote/tekusuto/daiyonnsatu/">第四册</a></li>
		<li id="menu-item-2191" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2191"><a href="http://www.hankcs.com/nihongonote/tekusuto/5/">第五册</a></li>
		<li id="menu-item-2702" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2702"><a href="http://www.hankcs.com/nihongonote/tekusuto/%e7%ac%ac%e5%85%ad%e5%86%8c/">第六册</a></li>
		<li id="menu-item-3604" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3604"><a href="http://www.hankcs.com/nihongonote/tekusuto/%e7%ac%ac%e4%b8%83%e5%86%8c/">第七册</a></li>
	</ul>
</li>
	<li id="menu-item-1859" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1859"><a href="http://www.hankcs.com/nihongonote/fd2/">新编日语阅读文选</a>
	<ul class="sub-menu">
		<li id="menu-item-2187" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2187"><a href="http://www.hankcs.com/nihongonote/fd2/c1/">第一册</a></li>
		<li id="menu-item-2189" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2189"><a href="http://www.hankcs.com/nihongonote/fd2/c2/">第二册</a></li>
		<li id="menu-item-2188" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2188"><a href="http://www.hankcs.com/nihongonote/fd2/c3/">第三册</a></li>
	</ul>
</li>
	<li id="menu-item-1858" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1858"><a href="http://www.hankcs.com/nihongonote/jpkaiwa/">日语商务贸易会话</a></li>
</ul>
</li>
<li id="menu-item-1843" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1843"><a href="http://www.hankcs.com/about/">关于</a></li>
							<li class="navto-search"><a href="javascript:;" class="search-show active"><i class="fa fa-search"></i></a></li>
					</ul>			<div class="sign">			    <div class="sign-mask"></div>			    <div class="container">			        <a href="http://www.hankcs.com/ml/hidden-markov-model.html#" class="close-link signclose-loader"><i class="fa fa-close"></i></a>			        <div class="sign-tips"></div>			        <form id="sign-in">  			            <h3><small class="signup-loader">切换注册</small>登录</h3>			            <h6>			                <label for="inputEmail">用户名或邮箱</label>			                <input type="text" name="username" class="form-control" id="inputEmail" placeholder="用户名或邮箱">			            </h6>			            <h6>			                <label for="inputPassword">密码</label>			                <input type="password" name="password" class="form-control" id="inputPassword" placeholder="登录密码">			            </h6>			            <div class="sign-submit">			                <input type="button" class="btn btn-primary signsubmit-loader" name="submit" value="登录">  			                <input type="hidden" name="action" value="signin">			                <label><input type="checkbox" checked="checked" name="remember" value="forever">记住我</label>			            </div><div class="sign-info"><a href="http://www.hankcs.com/about/">找回密码？</a></div></form>			        <form id="sign-up"> 			            <h3><small class="signin-loader">切换登录</small>注册</h3>			            <h6>			                <label for="inputName">昵称</label>			                <input type="text" name="name" class="form-control" id="inputName" placeholder="设置昵称">			            </h6>			            <h6>			                <label for="inputEmail">邮箱</label>			                <input type="email" name="email" class="form-control" id="inputEmail" placeholder="邮箱">			            </h6>			            <div class="sign-submit">			                <input type="button" class="btn btn-primary btn-block signsubmit-loader" name="submit" value="快速注册">  			                <input type="hidden" name="action" value="signup">  			            </div>			        </form>			    </div>			</div>		</body></html>