<!DOCTYPE html>
<!-- saved from url=(0061)http://www.hankcs.com/ml/back-propagation-neural-network.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<link rel="dns-prefetch" href="http://apps.bdimg.com/">
<meta http-equiv="X-UA-Compatible" content="IE=11,IE=10,IE=9,IE=8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=0, minimum-scale=1.0, maximum-scale=1.0">
<meta name="apple-mobile-web-app-title" content="码农场">
<meta http-equiv="Cache-Control" content="no-siteapp">
<title>反向传播神经网络极简入门-码农场</title>
<link rel="dns-prefetch" href="http://apps.bdimg.com/">
<link rel="dns-prefetch" href="http://s.w.org/">
		<script async="" src="./反向传播神经网络极简入门-码农场_files/analytics.js.下载"></script><script src="./反向传播神经网络极简入门-码农场_files/ca-pub-1152644711996772.js.下载"></script><script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2.3\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2.3\/svg\/","svgExt":".svg","source":{"concatemoji":"http:\/\/www.hankcs.com\/wp-includes\/js\/wp-emoji-release.min.js?ver=4.8.1"}};
			!function(a,b,c){function d(a){var b,c,d,e,f=String.fromCharCode;if(!k||!k.fillText)return!1;switch(k.clearRect(0,0,j.width,j.height),k.textBaseline="top",k.font="600 32px Arial",a){case"flag":return k.fillText(f(55356,56826,55356,56819),0,0),b=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,56826,8203,55356,56819),0,0),c=j.toDataURL(),b===c&&(k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447),0,0),b=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447),0,0),c=j.toDataURL(),b!==c);case"emoji4":return k.fillText(f(55358,56794,8205,9794,65039),0,0),d=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55358,56794,8203,9794,65039),0,0),e=j.toDataURL(),d!==e}return!1}function e(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var f,g,h,i,j=b.createElement("canvas"),k=j.getContext&&j.getContext("2d");for(i=Array("flag","emoji4"),c.supports={everything:!0,everythingExceptFlag:!0},h=0;h<i.length;h++)c.supports[i[h]]=d(i[h]),c.supports.everything=c.supports.everything&&c.supports[i[h]],"flag"!==i[h]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[i[h]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(g=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",g,!1),a.addEventListener("load",g,!1)):(a.attachEvent("onload",g),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),f=c.source||{},f.concatemoji?e(f.concatemoji):f.wpemoji&&f.twemoji&&(e(f.twemoji),e(f.wpemoji)))}(window,document,window._wpemojiSettings);
		</script><script src="./反向传播神经网络极简入门-码农场_files/wp-emoji-release.min.js.下载" type="text/javascript" defer=""></script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel="stylesheet" id="_bootstrap-css" href="./反向传播神经网络极简入门-码农场_files/bootstrap.min.css" type="text/css" media="all">
<link rel="stylesheet" id="_fontawesome-css" href="./反向传播神经网络极简入门-码农场_files/font-awesome.min.css" type="text/css" media="all">
<link rel="stylesheet" id="_main-css" href="./反向传播神经网络极简入门-码农场_files/main.css" type="text/css" media="all">
<script type="text/javascript" src="./反向传播神经网络极简入门-码农场_files/jquery.min.js.下载"></script>
<link rel="https://api.w.org/" href="http://www.hankcs.com/wp-json/">
<link rel="prev" title="解决cc1plus.exe: out of memory allocating bytes" href="http://www.hankcs.com/program/cpp/cc1plus-exe-out-of-memory-allocating-bytes-solve.html">
<link rel="next" title="基于神经网络的高性能依存句法分析器" href="http://www.hankcs.com/nlp/parsing/neural-network-based-dependency-parser.html">
<link rel="canonical" href="http://www.hankcs.com/ml/back-propagation-neural-network.html">
<link rel="shortlink" href="http://www.hankcs.com/?p=6796">
<link rel="alternate" type="application/json+oembed" href="http://www.hankcs.com/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fwww.hankcs.com%2Fml%2Fback-propagation-neural-network.html">
<link rel="alternate" type="text/xml+oembed" href="http://www.hankcs.com/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fwww.hankcs.com%2Fml%2Fback-propagation-neural-network.html&amp;format=xml">
<meta name="keywords" content="机器学习">
<meta name="description" content="我一直在找一份简明的神经网络入门，然而在中文圈里并没有找到。直到我看到了这份162行的Python实现，以及对应的油管视频之后，我才觉得这就是我需要的极简入门资料。这份极简入门笔记不需要突触的图片做装饰，也不需要赘述神经网络的发展历史；要推导有推导，要代码有代码，关键是，它们还对得上。对于欠缺的背景知识，利用斯坦福大学的神经网络wiki进行了补全。单个神经元">
<link rel="icon" href="http://www.hankcs.com/wp-content/uploads/2017/04/cropped-Hankcs_512-32x32.png" sizes="32x32">
<link rel="icon" href="http://www.hankcs.com/wp-content/uploads/2017/04/cropped-Hankcs_512-192x192.png" sizes="192x192">
<link rel="apple-touch-icon-precomposed" href="http://www.hankcs.com/wp-content/uploads/2017/04/cropped-Hankcs_512-180x180.png">
<meta name="msapplication-TileImage" content="http://www.hankcs.com/wp-content/uploads/2017/04/cropped-Hankcs_512-270x270.png">
<link rel="shortcut icon" href="http://www.hankcs.com/favicon.ico">
<!--[if lt IE 9]><script src="http://www.hankcs.com/wp-content/themes/dux/js/libs/html5.min.js"></script><![endif]-->
<!--
	generated 30545 seconds ago
	generated in 0.259 seconds
	served from batcache in 0.003 seconds
	expires in 55855 seconds
-->
<script async="" data-requirecontext="_" data-requiremodule="main" src="./反向传播神经网络极简入门-码农场_files/main.js.下载"></script><script src="./反向传播神经网络极简入门-码农场_files/share.js.下载"></script><script async="" data-requirecontext="_" data-requiremodule="lazyload" src="./反向传播神经网络极简入门-码农场_files/lazyload.min.js.下载"></script><script async="" data-requirecontext="_" data-requiremodule="prettyprint" src="./反向传播神经网络极简入门-码农场_files/prettyprint.js.下载"></script><script async="" data-requirecontext="_" data-requiremodule="signpop" src="./反向传播神经网络极简入门-码农场_files/signpop.js.下载"></script><script async="" data-requirecontext="_" data-requiremodule="comment" src="./反向传播神经网络极简入门-码农场_files/comment.js.下载"></script><link href="./反向传播神经网络极简入门-码农场_files/share.css" rel="styleSheet" type="text/css"></head>
<body class="post-template-default single single-post postid-6796 single-format-standard comment-open site-layout-2">
<header class="header">
	<div class="container">
		<div class="logo"><a href="http://www.hankcs.com/" title="码农场-自然语言处理、机器学习算法"><img src="./反向传播神经网络极简入门-码农场_files/logo.png">码农场</a></div>		<a href="http://www.hankcs.com/" title="码农场-自然语言处理、机器学习算法" class="brand">放牧代码和思想
<br>专注自然语言处理、机器学习算法</a>		<ul class="site-nav site-navbar">
			<li id="menu-item-1834" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1834"><a href="http://www.hankcs.com/program/cpp/">C++</a></li>
<li id="menu-item-1835" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1835"><a href="http://www.hankcs.com/program/java/">Java</a></li>
<li id="menu-item-5754" class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent menu-item-5754"><a href="http://www.hankcs.com/ml/">机器学习</a></li>
<li id="menu-item-2954" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-2954"><a href="http://www.hankcs.com/nlp/">NLP</a>
<ul class="sub-menu">
	<li id="menu-item-4344" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4344"><a href="http://www.hankcs.com/nlp/corpus/">语料库</a></li>
	<li id="menu-item-4342" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4342"><a href="http://www.hankcs.com/nlp/segment/">中文分词</a></li>
	<li id="menu-item-4343" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4343"><a href="http://www.hankcs.com/nlp/ner/">命名实体识别</a></li>
	<li id="menu-item-4479" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4479"><a href="http://www.hankcs.com/nlp/parsing/">句法分析</a></li>
</ul>
</li>
<li id="menu-item-1837" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1837"><a href="http://www.hankcs.com/program/algorithm/">算法</a></li>
<li id="menu-item-1839" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1839"><a href="http://www.hankcs.com/software/">软件</a></li>
<li id="menu-item-1838" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1838"><a href="http://www.hankcs.com/nihongonote/">日语</a>
<ul class="sub-menu">
	<li id="menu-item-1860" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1860"><a href="http://www.hankcs.com/nihongonote/riyurimen/">日语入门</a></li>
	<li id="menu-item-1861" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1861"><a href="http://www.hankcs.com/nihongonote/listening/">日语听力</a></li>
	<li id="menu-item-1863" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1863"><a href="http://www.hankcs.com/nihongonote/tekusuto/">日语综合教程</a>
	<ul class="sub-menu">
		<li id="menu-item-2190" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2190"><a href="http://www.hankcs.com/nihongonote/tekusuto/disance/">第三册</a></li>
		<li id="menu-item-2192" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2192"><a href="http://www.hankcs.com/nihongonote/tekusuto/daiyonnsatu/">第四册</a></li>
		<li id="menu-item-2191" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2191"><a href="http://www.hankcs.com/nihongonote/tekusuto/5/">第五册</a></li>
		<li id="menu-item-2702" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2702"><a href="http://www.hankcs.com/nihongonote/tekusuto/%e7%ac%ac%e5%85%ad%e5%86%8c/">第六册</a></li>
		<li id="menu-item-3604" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3604"><a href="http://www.hankcs.com/nihongonote/tekusuto/%e7%ac%ac%e4%b8%83%e5%86%8c/">第七册</a></li>
	</ul>
</li>
	<li id="menu-item-1859" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1859"><a href="http://www.hankcs.com/nihongonote/fd2/">新编日语阅读文选</a>
	<ul class="sub-menu">
		<li id="menu-item-2187" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2187"><a href="http://www.hankcs.com/nihongonote/fd2/c1/">第一册</a></li>
		<li id="menu-item-2189" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2189"><a href="http://www.hankcs.com/nihongonote/fd2/c2/">第二册</a></li>
		<li id="menu-item-2188" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2188"><a href="http://www.hankcs.com/nihongonote/fd2/c3/">第三册</a></li>
	</ul>
</li>
	<li id="menu-item-1858" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1858"><a href="http://www.hankcs.com/nihongonote/jpkaiwa/">日语商务贸易会话</a></li>
</ul>
</li>
<li id="menu-item-1843" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1843"><a href="http://www.hankcs.com/about/">关于</a></li>
							<li class="navto-search"><a href="javascript:;" class="search-show active"><i class="fa fa-search"></i></a></li>
					</ul>
		<div class="topbar">
			<ul class="site-nav topmenu">
				<li id="menu-item-5755" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-5755"><a href="http://www.hankcs.com/about/#comments"><i class="fa fa-comment"></i> 留言板</a></li>
				<li><a target="_blank" rel="external nofollow" href="https://github.com/hankcs"><i class="fa fa-github-alt"></i> GitHub</a></li>                                <li><a target="_blank" rel="external nofollow" href="http://weibo.com/hankcs"><i class="fa fa-weibo"></i> 微博</a></li>                                <li><a target="_blank" rel="external nofollow" href="https://twitter.com/hankcs"><i class="fa fa-twitter"></i> Twitter</a></li>                                <li><a target="_blank" href="http://www.hankcs.com/feed"><i class="fa fa-rss"></i> RSS订阅</a></li>			</ul>
							&nbsp; &nbsp; <i class="fa fa-bullhorn url"></i> 时间有限，只有GitHub上的issue能及时处理，大约每周末一次。另外，不要叫我楼主，谢谢。					</div>
		<i class="fa fa-bars m-icon-nav"></i>
	</div>
</header>
<div class="site-search">
	<div class="container">
		<form method="get" class="site-search-form" action="http://www.hankcs.com/"><input class="search-input" name="s" type="text" placeholder="输入关键字" value=""><button class="search-btn" type="submit"><i class="fa fa-search"></i></button></form>	</div>
</div><section class="container">
	<div class="content-wrap">
	<div class="content">
				<header class="article-header">
			<h1 class="article-title"><a href="http://www.hankcs.com/ml/back-propagation-neural-network.html">反向传播神经网络极简入门</a></h1>
			<div class="article-meta">
				<span class="item">
					<a href="http://www.hankcs.com/">码农场</a> <small>&gt;</small> <a href="http://www.hankcs.com/ml/">机器学习</a><span class="muted"></span>				</span>
				<span class="item">2015-11-08</span>
																<span class="item post-views">阅读(3760)</span>				<span class="item"><a class="pc" href="http://www.hankcs.com/ml/back-propagation-neural-network.html#respond">评论(25)</a></span>				<span class="item"></span>
			</div>
		</header>
		<article class="article-content">
			<div class="asb asb-post asb-post-01"><script async="" src="./反向传播神经网络极简入门-码农场_files/adsbygoogle.js.下载"></script>
<!-- 文章页 - 页面标题下 728 90 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-1152644711996772" data-ad-slot="5413029241" data-adsbygoogle-status="done"><ins id="aswift_0_expand" style="display:inline-table;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent"><ins id="aswift_0_anchor" style="display:block;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent"><iframe width="728" height="90" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_0" name="aswift_0" style="left:0;position:absolute;top:0;width:728px;height:90px;" src="./反向传播神经网络极简入门-码农场_files/saved_resource.html"></iframe></ins></ins></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>			<div class="post_nav" style="width: 0px;"><div class="post_nav_side" style="height: 100%;"><div class="post_nav_top"><p>目录</p></div><div class="post_nav_bottom"></div><span class="post_nav_close icon-remove" title="关闭目录" style="opacity: 0; display: none;"><i class="fa fa-times"></i></span></div><ul class="post_nav_content"><li class="h2_nav"><a href="http://www.hankcs.com/ml/back-propagation-neural-network.html#h2-0">单个神经元</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/back-propagation-neural-network.html#h3-1">神经元</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/back-propagation-neural-network.html#h3-2">Sigmoid函数</a><i class="post_nav_dot"></i></li>
<li class="h2_nav"><a href="http://www.hankcs.com/ml/back-propagation-neural-network.html#h2-3">神经网络模型</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/back-propagation-neural-network.html#h3-4">前向传播</a><i class="post_nav_dot"></i></li>
<li class="h2_nav"><a href="http://www.hankcs.com/ml/back-propagation-neural-network.html#h2-5">后向传播</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/back-propagation-neural-network.html#h3-6">符号定义</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/back-propagation-neural-network.html#h3-7">输出层权值调整</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/back-propagation-neural-network.html#h3-8">隐藏层权值调整</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/back-propagation-neural-network.html#h3-9">偏置的调整</a><i class="post_nav_dot"></i></li>
<li class="h3_nav"><a href="http://www.hankcs.com/ml/back-propagation-neural-network.html#h3-10">后向传播算法步骤</a><i class="post_nav_dot"></i></li>
<li class="h3_nav active"><a href="http://www.hankcs.com/ml/back-propagation-neural-network.html#h3-11">后向传播算法实现</a><i class="post_nav_dot"></i></li>
<li class="h2_nav"><a href="http://www.hankcs.com/ml/back-propagation-neural-network.html#h2-12">完整的实现</a><i class="post_nav_dot"></i></li>
<li class="h2_nav"><a href="http://www.hankcs.com/ml/back-propagation-neural-network.html#h2-13">Reference</a><i class="post_nav_dot"></i></li>
</ul></div><p style="text-indent: 2em;">我一直在找一份简明的神经网络入门，然而在中文圈里并没有找到。直到我看到了这份162行的Python实现，以及对应的油管视频之后，我才觉得这就是我需要的极简入门资料。这份极简入门笔记不需要突触的图片做装饰，也不需要赘述神经网络的发展历史；要推导有推导，要代码有代码，关键是，它们还对得上。对于欠缺的背景知识，利用斯坦福大学的神经网络wiki进行了补全。</p>
<h2 id="h2-0">单个神经元</h2>
<p style="text-indent: 2em;">神经网络是多个“神经元”（感知机）的带权级联，神经网络算法可以提供非线性的复杂模型，它有两个参数：权值矩阵{W<sup>l</sup>}和偏置向量{b<sup>l</sup>}，不同于感知机的单一向量形式，<span style="text-indent: 32px;">{W</span><sup style="text-indent: 32px; white-space: normal;">l</sup><span style="text-indent: 32px;">}</span>是复数个矩阵，<span style="text-indent: 32px;">{b</span><sup style="text-indent: 32px; white-space: normal;">l</sup><span style="text-indent: 32px;">}</span>是复数个向量，其中的元素分别属于单个层，而每个层的组成单元，就是神经元。</p>
<h3 id="h3-1">神经元<br></h3>
<p style="text-indent: 2em;">神经网络是由多个“神经元”（感知机）组成的，每个神经元图示如下：</p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsm4yho09j208c044t8o.jpg" title="神经元.png" alt="神经元.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">这其实就是一个单层感知机，其输入是由<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsm87f1juj202200da9t.jpg" title="神经网络2.png" alt="神经网络2.png" data-tag="bdshare">和+1组成的向量，其输出为<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsm8wslcqj209200nwec.jpg" title="神经网络3.png" alt="神经网络3.png" data-tag="bdshare">，其中f是一个激活函数，模拟的是生物神经元在接受一定的刺激之后产生兴奋信号，否则刺激不够的话，神经元保持抑制状态这种现象。这种由一个阈值决定两个极端的函数有点像示性函数，然而这里采用的是Sigmoid函数，其优点是连续可导。</p>
<h3 id="h3-2">Sigmoid函数</h3>
<p style="text-indent: 2em;">常用的Sigmoid有两种——</p>
<p style="text-indent: 2em;"><strong>单极性Sigmoid函数</strong></p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsmi0qapmj204v01awea.jpg" title="单极性Sigmoid.png" alt="单极性Sigmoid.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 2em;">或者写成</span></p>
<p style="text-align:center"><span style="text-indent: 2em;"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsmk9h09aj204s01st8k.jpg" title="神经网络5.png" alt="神经网络5.png" data-tag="bdshare"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 2em;">其图像如下<br></span></p>
<p style="text-align:center"><span style="text-indent: 2em;"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsmps4xz7j20b408cmx8.jpg" title="sigmoid及tanh的函数图像.png" alt="sigmoid及tanh的函数图像.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare"></span></p>
<p style="text-indent: 2em;"><strong><span style="text-indent: 2em;">双极性Sigmoid函数</span></strong></p>
<p style="text-align:center"><span style="text-indent: 2em;"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsmlquru2j206e0173yc.jpg" title="双曲正切函数.png" alt="双曲正切函数.png" data-tag="bdshare"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 2em;">或者写成</span></p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsmnqs1p6j204o02bwee.jpg" title="神经网络6.png" alt="神经网络6.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 2em;"></span>把第一个式子分子分母同时除以e<sup>z</sup>，令x=-2z就得到第二个式子了，换汤不换药。</p>
<p style="text-indent: 2em;">其图像如下</p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsmq4drggj20b408ct8r.jpg" title="tanh的函数图像.png" alt="tanh的函数图像.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">从它们两个的值域来看，两者名称里的极性应该指的是正负号。<span style="text-indent: 2em;">从导数来看，它们的导数都非常便于计算：</span></p>
<p style="text-indent: 2em;">对于<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsmi0qapmj204v01awea.jpg" title="单极性Sigmoid.png" alt="单极性Sigmoid.png" style="text-align: center; white-space: normal;" data-tag="bdshare">有<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsmwdv9bmj205900lwea.jpg" title="神经网络7.png" alt="神经网络7.png" data-tag="bdshare">，对于tanh，有<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsmwrhefuj204g00ma9u.jpg" title="神经网络8.png" alt="神经网络8.png" data-tag="bdshare">。</p>
<p style="text-indent: 2em;">视频作者Ryan还担心观众微积分学的不好，细心地给出了1/(1+e^-x)求导的过程：</p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exspyed1nrj20bz094js7.jpg" title="神经网络20.png" alt="神经网络20.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">一旦知道了f(z)，就可以直接求f'(z)，所以说很方便。</p>
<p style="text-indent: 2em;">本Python实现使用的就是1/(1+e^-x)</p>
<pre class="brush:python;toolbar:false prettyprint linenums"><ol class="linenums"><li class="L0"><span class="kwd">def</span><span class="pln">&nbsp;sigmoid</span><span class="pun">(</span><span class="pln">x</span><span class="pun">):</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="str">"""</span></li><li class="L2"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;sigmoid&nbsp;函数，1/(1+e^-x)</span></li><li class="L3"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;:param&nbsp;x:</span></li><li class="L4"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;:return:</span></li><li class="L5"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;"""</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;</span><span class="lit">1.0</span><span class="pun">/(</span><span class="lit">1.0</span><span class="pun">+</span><span class="pln">math</span><span class="pun">.</span><span class="pln">exp</span><span class="pun">(-</span><span class="pln">x</span><span class="pun">))</span></li><li class="L7"><span class="pln">&nbsp;</span></li><li class="L8"><span class="pln">&nbsp;</span></li><li class="L9"><span class="kwd">def</span><span class="pln">&nbsp;dsigmoid</span><span class="pun">(</span><span class="pln">y</span><span class="pun">):</span></li><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="str">"""</span></li><li class="L1"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;sigmoid&nbsp;函数的导数</span></li><li class="L2"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;:param&nbsp;y:</span></li><li class="L3"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;:return:</span></li><li class="L4"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;"""</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;y&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="lit">1</span><span class="pln">&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;y</span><span class="pun">)</span></li></ol></pre>
<p style="text-indent: 2em;"><span style="background-color: rgb(255, 255, 255);">也可以使用</span><span style="font-size: 16.51px; line-height: 24.765px; background-color: rgb(255, 255, 255);">双曲正切函数</span>tanh</p>
<pre class="brush:python;toolbar:false prettyprint linenums"><ol class="linenums"><li class="L0"><span class="kwd">def</span><span class="pln">&nbsp;sigmoid</span><span class="pun">(</span><span class="pln">x</span><span class="pun">):</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="str">"""</span></li><li class="L2"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;sigmoid&nbsp;函数，tanh&nbsp;</span></li><li class="L3"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;:param&nbsp;x:</span></li><li class="L4"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;:return:</span></li><li class="L5"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;"""</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;math</span><span class="pun">.</span><span class="pln">tanh</span><span class="pun">(</span><span class="pln">x</span><span class="pun">)</span></li></ol></pre>
<p style="text-indent: 2em;">其导数对应于：</p>
<pre class="brush:python;toolbar:false prettyprint linenums"><ol class="linenums"><li class="L0"><span class="kwd">def</span><span class="pln">&nbsp;dsigmoid</span><span class="pun">(</span><span class="pln">y</span><span class="pun">):</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="str">"""</span></li><li class="L2"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;sigmoid&nbsp;函数的导数</span></li><li class="L3"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;:param&nbsp;y:</span></li><li class="L4"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;:return:</span></li><li class="L5"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;"""</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;</span><span class="lit">1.0</span><span class="pln">&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;y&nbsp;</span><span class="pun">**</span><span class="pln">&nbsp;</span><span class="lit">2</span></li></ol></pre>
<h2 id="h2-3">神经网络模型<br></h2>
<p style="text-indent: 2em;">神经网络就是多个神经元的级联，上一级神经元的输出是下一级神经元的输入，而且信号在两级的两个神经元之间传播的时候需要乘上这两个神经元对应的权值。例如，下图就是一个简单的神经网络：</p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsoi5cumdj20m80fo40r.jpg" title="简单的神经网络.png" alt="简单的神经网络.png" width="400" height="282" border="0" vspace="0" style="width: 400px; height: 282px;" data-tag="bdshare"></p>
<p dir="ltr" style="text-indent: 2em;">其中，一共有一个输入层，一个隐藏层和一个输出层。<span style="text-indent: 2em;">输入层有3个输入节点，标注为+1的那个节点是偏置节点，偏置节点不接受输入，输出总是+1。</span></p>
<p dir="ltr" style="text-indent: 2em;">定义上标为层的标号，下标为节点的标号，则本神经网络模型的参数是：<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsosba5igj206x00nweb.jpg" title="神经网络9.png" alt="神经网络9.png" data-tag="bdshare">，其中<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsotb63nhj200z00s3y9.jpg" title="神经网络10.png" alt="神经网络10.png" data-tag="bdshare">是第l层的第j个节点与第l+1层第i个节点之间的连接参数（或称权值）；<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsouvrte5j200n00q0lm.jpg" title="神经网络11.png" alt="神经网络11.png" data-tag="bdshare">表示第l层第i个偏置节点。这些符号在接下来的前向传播将要用到。</p>
<h3 id="h3-4">前向传播</h3>
<p dir="ltr" style="text-indent: 2em;">虽然标题是《（误差）后向传播神经网络入门》，但这并不意味着可以跳过前向传播的学习。因为如果后向传播对应训练的话，那么前向传播就对应预测（分类），并且训练的时候计算误差也要用到预测的输出值来计算误差。</p>
<p dir="ltr" style="text-indent: 2em;">定义<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsp0odbm6j200p00q0l3.jpg" title="神经网络12.png" alt="神经网络12.png" data-tag="bdshare">为第l层第i个节点的激活值（输出值）。当l=1时，<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsp1go028j201z00q741.jpg" title="神经网络13.png" alt="神经网络13.png" data-tag="bdshare">。前向传播的目的就是在给定模型参数<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsp2mk4wlj200y00i0of.jpg" title="神经网络14.png" alt="神经网络14.png" data-tag="bdshare">的情况下，计算l=2,3,4…层的输出值，直到最后一层就得到最终的输出值。具体怎么算呢，以上图的神经网络模型为例：</p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsp4xmrvqj20d603k74q.jpg" title="神经网络15.png" alt="神经网络15.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">这没什么稀奇的，核心思想是这一层的输出乘上相应的权值加上偏置量代入激活函数等于下一层的输入，一句大白话，所谓中文伪码。</p>
<p style="text-indent: 2em;">另外，追求好看的话可以把括号里面那个老长老长的加权和定义为一个参数：<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exspbukq8ej200o00q0k5.jpg" title="2.png" alt="2.png" data-tag="bdshare">表示第l层第i个节点的输入加权和，比如<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exspcjjj7lj205s00tq2r.jpg" title="神经网络16.png" alt="神经网络16.png" data-tag="bdshare">。那么该节点的输出可以写作<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exspdeueyrj202w00q0si.jpg" title="神经网络17.png" alt="神经网络17.png" data-tag="bdshare">。</p>
<p style="text-indent: 2em;">于是就得到一个好看的形式：</p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exspefrfuhj205p03d749.jpg" title="神经网络18.png" alt="神经网络18.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">在这个好看的形式下，前向传播可以简明扼要地表示为：</p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exspfctszuj205101jmx0.jpg" title="神经网络19.png" alt="神经网络19.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">在Python实现中，对应如下方法：</p>
<pre class="brush:python;toolbar:false prettyprint linenums"><ol class="linenums"><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">def</span><span class="pln">&nbsp;runNN</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln">&nbsp;inputs</span><span class="pun">):</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="str">"""</span></li><li class="L2"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前向传播进行分类</span></li><li class="L3"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:param&nbsp;inputs:输入</span></li><li class="L4"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:return:类别</span></li><li class="L5"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">if</span><span class="pln">&nbsp;len</span><span class="pun">(</span><span class="pln">inputs</span><span class="pun">)</span><span class="pln">&nbsp;</span><span class="pun">!=</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ni&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">:</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">print</span><span class="pln">&nbsp;</span><span class="str">'incorrect&nbsp;number&nbsp;of&nbsp;inputs'</span></li><li class="L8"><span class="pln">&nbsp;</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;i&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ni&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">):</span></li><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ai</span><span class="pun">[</span><span class="pln">i</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;inputs</span><span class="pun">[</span><span class="pln">i</span><span class="pun">]</span></li><li class="L1"><span class="pln">&nbsp;</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;j&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">nh</span><span class="pun">):</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sum&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0.0</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;i&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ni</span><span class="pun">):</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sum&nbsp;</span><span class="pun">+=</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ai</span><span class="pun">[</span><span class="pln">i</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">wi</span><span class="pun">[</span><span class="pln">i</span><span class="pun">][</span><span class="pln">j</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">)</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ah</span><span class="pun">[</span><span class="pln">j</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;sigmoid</span><span class="pun">(</span><span class="pln">sum</span><span class="pun">)</span></li><li class="L7"><span class="pln">&nbsp;</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;k&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="kwd">no</span><span class="pun">):</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sum&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0.0</span></li><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;j&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">nh</span><span class="pun">):</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sum&nbsp;</span><span class="pun">+=</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ah</span><span class="pun">[</span><span class="pln">j</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">wo</span><span class="pun">[</span><span class="pln">j</span><span class="pun">][</span><span class="pln">k</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">)</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ao</span><span class="pun">[</span><span class="pln">k</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;sigmoid</span><span class="pun">(</span><span class="pln">sum</span><span class="pun">)</span></li><li class="L3"><span class="pln">&nbsp;</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ao</span></li></ol></pre>
<p style="text-indent: 2em;">其中，ai、ah、ao分别是输入层、隐藏层、输出层，而wi、wo则分别是输入层到隐藏层、隐藏层到输出层的权值矩阵。在本Python实现中，将偏置量一并放入了矩阵，这样进行线性代数运算就会方便一些。</p>
<h2 id="h2-5">后向传播</h2>
<p style="text-indent: 2em;">后向传播指的是在训练的时候，根据最终输出的误差来调整倒数第二层、倒数第三层……第一层的参数的过程。</p>
<h3 id="h3-6"><strong>符号定义</strong></h3>
<p style="text-indent: 2em;">在Ryan的讲义中，符号定义与斯坦福前向传播讲义相似但略有不同：</p>
<p style="text-indent: 2em;"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsq90t0ybj200q00ya9t.jpg" title="神经网络21.png" alt="神经网络21.png" data-tag="bdshare">：第l层第j个节点的输入。</p>
<p style="text-indent: 2em;"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsq9w0bcqj200z010mwx.jpg" title="神经网络22.png" alt="神经网络22.png" data-tag="bdshare">：从第l-1层第i个节点到第l层第j个节点的权值。</p>
<p style="text-indent: 2em;"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsqe99voqj203k012q2r.jpg" title="神经网络23.png" alt="神经网络23.png" data-tag="bdshare">：Sigmoid函数。</p>
<p style="text-indent: 2em;"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsqex83qwj200m00xdfl.jpg" title="神经网络24.png" alt="神经网络24.png" data-tag="bdshare">：第l层第j个节点的偏置。</p>
<p style="text-indent: 2em;"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsqfm5snbj200u00ymwx.jpg" title="神经网络25.png" alt="神经网络25.png" data-tag="bdshare">：<span style="text-indent: 32px;">第l层第j个节点的输出。</span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsqgfwe3yj200k00s0sh.jpg" title="神经网络26.png" alt="神经网络26.png" data-tag="bdshare">：输出层第j个节点的目标值（Target value）。</span></p>
<h3 id="h3-7"><strong><span style="text-indent: 32px;">输出层权值调整</span></strong></h3>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">给定训练集<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsqlx21auj200o00s0sh.jpg" title="神经网络28.png" alt="神经网络28.png" data-tag="bdshare">和模型输出<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsqm82b38j200v00sdfl.jpg" title="神经网络29.png" alt="神经网络29.png" data-tag="bdshare">（这里没有上标l是因为这里在讨论输出层，l是固定的），输出层的输出误差（或称损失函数吧）定义为：<br></span></p>
<p style="text-align:center"><span style="text-indent: 32px;"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsqjlngfcj205v01zwef.jpg" title="神经网络27.png" alt="神经网络27.png" data-tag="bdshare"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">其实就是所有实例对应的误差的平方和的一半，训练的目标就是最小化该误差。怎么最小化呢？看损失函数对参数的导数<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsqpht3ewj2018015gle.jpg" title="神经网络30.png" alt="神经网络30.png" data-tag="bdshare">呗。<br></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">将E的定义代入该导数：<br></span></p>
<p style="text-align:center"><span style="text-indent: 32px;"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsqquiqx6j208e0200st.jpg" title="神经网络31.png" alt="神经网络31.png" data-tag="bdshare"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">无关变量拿出来：<br></span></p>
<p style="text-align:center"><span style="text-indent: 32px;"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsqrn32fyj207a01tjre.jpg" title="神经网络32.png" alt="神经网络32.png" data-tag="bdshare"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">看到这里大概明白为什么非要把误差定义为误差平方和的<strong>一半</strong>了吧，就是为了好看，数学家都是外貌协会的。</span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">将<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr5u28rsj200v00qgld.jpg" title="神经网络37.png" alt="神经网络37.png" data-tag="bdshare">=<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr5xd70dj201f00va9u.jpg" title="神经网络36.png" alt="神经网络36.png" data-tag="bdshare">（输出层的输出等于输入代入Sigmoid函数）这个关系代入有：<br></span></p>
<p style="text-align:center"><span style="text-indent: 32px;"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsqxalhvzj207y01ot8q.jpg" title="神经网络34.png" alt="神经网络34.png" data-tag="bdshare"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">对Sigmoid求导有：</span></p>
<p style="text-align:center"><span style="text-indent: 32px;"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsqy7sxpdj20bb01sjrj.jpg" title="神经网络35.png" alt="神经网络35.png" data-tag="bdshare"></span></p>
<p style="text-indent: 2em;">要开始耍小把戏了，由于输出层第k个节点的输入<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr8ldc7pj200m00l3y9.jpg" title="神经网络38.png" alt="神经网络38.png" data-tag="bdshare">等于上一层第j个节点的输出<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr8zvgs9j200r00udfl.jpg" title="神经网络39.png" alt="神经网络39.png" data-tag="bdshare">乘上<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exybssl393j201000tq2p.jpg" title="神经网络100.png" alt="神经网络100.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">，即<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr8ldc7pj200m00l3y9.jpg" title="神经网络38.png" alt="神经网络38.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">=<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr8zvgs9j200r00udfl.jpg" title="神经网络39.png" alt="神经网络39.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exybssl393j201000tq2p.jpg" title="神经网络100.png" alt="神经网络100.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">，而上一层的输出<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr8zvgs9j200r00udfl.jpg" title="神经网络39.png" alt="神经网络39.png" data-tag="bdshare" style="box-sizing: border-box; border: 0px; vertical-align: middle; max-width: 100%; max-height: 100%; margin: 5px 0px; height: auto; color: rgb(51, 51, 51); font-family: &#39;Microsoft Yahei&#39;; font-size: 15px; line-height: 25px; white-space: normal; text-indent: 32px; background-color: rgb(255, 255, 255);">是与到输出层的权值变量<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exybssl393j201000tq2p.jpg" title="神经网络100.png" alt="神经网络100.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">无关的，可以看做一个常量，是线性关系。所以对<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr8ldc7pj200m00l3y9.jpg" title="神经网络38.png" alt="神经网络38.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">求权值变量<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrbf9eb8j200y00rq2p.jpg" title="神经网络40.png" alt="神经网络40.png" data-tag="bdshare">的偏导数直接等于<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr8zvgs9j200r00udfl.jpg" title="神经网络39.png" alt="神经网络39.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">，也就是说：<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrc0j0sej202301mt8j.jpg" title="神经网络41.png" alt="神经网络41.png" data-tag="bdshare">=<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrck860ij201l01k3yb.jpg" title="神经网络42.png" alt="神经网络42.png" data-tag="bdshare">(<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr8zvgs9j200r00udfl.jpg" title="神经网络39.png" alt="神经网络39.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exybssl393j201000tq2p.jpg" title="神经网络100.png" alt="神经网络100.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">)=<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr8zvgs9j200r00udfl.jpg" title="神经网络39.png" alt="神经网络39.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">。</p>
<p style="text-indent: 2em;">然后将上面用过的<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr5xd70dj201f00va9u.jpg" title="神经网络36.png" alt="神经网络36.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">=<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr5u28rsj200v00qgld.jpg" title="神经网络37.png" alt="神经网络37.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">代进去就得到最终的：</p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrdpbx8wj208n01m74b.jpg" title="神经网络43.png" alt="神经网络43.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">为了表述方便将上式记作：</span></p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrhazaatj203p01pmx1.jpg" title="神经网络44.png" alt="神经网络44.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">其中：</span></p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrhmncrij2072013747.jpg" title="神经网络45.png" alt="神经网络45.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"></span></p>
<h3 id="h3-8"><span style="text-indent: 32px;">隐藏层权值调整</span></h3>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">依然采用类似的方法求导，只不过求的是关于隐藏层和前一层的权值参数的偏导数：</span></p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsriy1k53j207w01st8s.jpg" title="神经网络46.png" alt="神经网络46.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">老样子：</span></p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrkolivjj208301sjrg.jpg" title="神经网络47.png" alt="神经网络47.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">还是老样子：</span></p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrlg3532j208n01xdfx.jpg" title="神经网络48.png" alt="神经网络48.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">还是把Sigmoid弄进去：</span></p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrmd3orij20bo01yjrl.jpg" title="神经网络49.png" alt="神经网络49.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">把<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr5xd70dj201f00va9u.jpg" title="神经网络36.png" alt="神经网络36.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare"><span style="text-indent: 32px;">=</span><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr5u28rsj200v00qgld.jpg" title="神经网络37.png" alt="神经网络37.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">代进去，并且将导数部分拆开：</span></p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsro5phb2j20bx01xglv.jpg" title="神经网络51.png" alt="神经网络51.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">又要耍把戏了，输出层的输入等于上一层的输出乘以相应的权值，亦即<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr8ldc7pj200m00l3y9.jpg" title="神经网络38.png" alt="神经网络38.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">=<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrqg0a66j200x00vgld.jpg" title="神经网络52.png" alt="神经网络52.png" data-tag="bdshare"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrr1tkgbj200n00q741.jpg" title="神经网络53.png" alt="神经网络53.png" data-tag="bdshare">，于是得到：</span></p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrs2dsbyj20bk01tglt.jpg" title="神经网络54.png" alt="神经网络54.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">把最后面的导数挪到前面去，接下来要对它动刀了：</span></p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrtm5d94j20ba021gls.jpg" title="神经网络55.png" alt="神经网络55.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">再次利用<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr5u28rsj200v00qgld.jpg" title="神经网络37.png" alt="神经网络37.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare"><span style="text-indent: 32px;">=</span><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr5xd70dj201f00va9u.jpg" title="神经网络36.png" alt="神经网络36.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">，这对j也成立，代进去：</span></p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrw4irm6j20e401ujro.jpg" title="神经网络56.png" alt="神经网络56.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">再次利用<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsr8ldc7pj200m00l3y9.jpg" title="神经网络38.png" alt="神经网络38.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare"><span style="text-indent: 32px;">=</span><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrqg0a66j200x00vgld.jpg" title="神经网络52.png" alt="神经网络52.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrr1tkgbj200n00q741.jpg" title="神经网络53.png" alt="神经网络53.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">，j换成i，k换成j也成立，代进去：</span></p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrxh5uixj20dc01udg2.jpg" title="神经网络57.png" alt="神经网络57.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">利用刚才定义的<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsry8kcjuj200q00u3y9.jpg" title="神经网络58.png" alt="神经网络58.png" data-tag="bdshare">，最终得到：</span></p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsryo2orzj208i01z0st.jpg" title="神经网络59.png" alt="神经网络59.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"></span></p>
<p style="white-space: normal; text-indent: 2em;">其中：</p>
<p style="white-space: normal; text-align: center;"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrhmncrij2072013747.jpg" title="神经网络45.png" alt="神经网络45.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">我们还可以仿照<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsry8kcjuj200q00u3y9.jpg" title="神经网络58.png" alt="神经网络58.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">的定义来定义一个<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exss0egjsuj200j00z0sh.jpg" title="神经网络60.png" alt="神经网络60.png" data-tag="bdshare">，得到：</p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exss0rhdxhj203c01nglh.jpg" title="神经网络61.png" alt="神经网络61.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">其中</span></p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exstgaf8arj207g01oq2x.jpg" title="神经网络70.png" alt="神经网络70.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"></span></p>
<h3 id="h3-9"><span style="text-indent: 32px;">偏置的调整</span></h3>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">因为没有任何节点的输出流向偏置节点，所以偏置节点不存在上层节点到它所对应的权值参数，也就是说不存在关于权值变量的偏导数。虽然没有流入，但是偏置节点依然有输出（总是+1），该输出到下一层某个节点的时候还是会有权值的，对这个权值依然需要更新。</span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">我们可以直接对偏置求导，发现：</span></p>
<p style="text-align:center"><span style="text-indent: 32px;"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exssgpc4k9j205d01mdfs.jpg" title="神经网络63.png" alt="神经网络63.png" data-tag="bdshare"></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"><span style="text-indent: 32px;">原视频中说</span><span style="text-indent: 32px;">∂O/∂θ=1，这是不对的，作者也在讲义中修正了这个错误，</span></span>∂O/∂θ=O(1–O)。</p>
<p style="text-indent: 2em;">然后再求<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exst2opwizj201001odfm.jpg" title="神经网络66.png" alt="神经网络66.png" data-tag="bdshare">，<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exst9vfd76j208301s74b.jpg" title="神经网络68.png" alt="神经网络68.png" data-tag="bdshare">，后面的导数等于<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exstak8kjuj204d01ia9y.jpg" title="神经网络69.png" alt="神经网络69.png" data-tag="bdshare">，代进去有</p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exssrzue19j202c01lq2r.jpg" title="神经网络65.png" alt="神经网络65.png" data-tag="bdshare"></p>
<p style="text-indent: 2em;">其中，</p>
<p style="text-align:center"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsrhmncrij2072013747.jpg" title="神经网络45.png" alt="神经网络45.png" style="text-align: center; white-space: normal;" data-tag="bdshare"></p>
<p style="text-indent: 2em;">。</p>
<h3 id="h3-10"><span style="text-indent: 32px;">后向传播算法步骤</span></h3>
<ul class=" list-paddingleft-2">
<li>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">随机初始化参数，对输入利用前向传播计算输出。</span></p>
</li>
<li>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">对每个输出节点按照下式计算delta：<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsthnw9ugj2074015747.jpg" title="神经网络71.png" alt="神经网络71.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare"></span></p>
</li>
<li>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">对每个隐藏节点按照下式计算<span style="text-indent: 32px;">delta</span>：<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exstiabhljj207501nmx5.jpg" title="神经网络72.png" alt="神经网络72.png" data-tag="bdshare"></span></p>
</li>
<li>
<p style="text-indent: 2em;"><span style="text-indent: 32px;">计算梯度<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exstiwvh5rj204x01st8n.jpg" title="神经网络73.png" alt="神经网络73.png" data-tag="bdshare">，<span style="text-indent: 32px;">并更新权值参数和偏置参数：<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exstjsxzobj204k01tjrb.jpg" title="神经网络74.png" alt="神经网络74.png" data-tag="bdshare">。这里的<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exstkm7wu0j200d00j0ng.jpg" title="神经网络75.png" alt="神经网络75.png" data-tag="bdshare">是学习率，影响训练速度。</span></span></p>
<p style="text-indent: 2em;"><span style="text-indent: 32px;"><span style="text-indent: 32px;"></span></span></p>
</li>
<h3 id="h3-11">后向传播算法实现<br></h3>
<pre class="brush:python;toolbar:false prettyprint linenums"><ol class="linenums"><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">def</span><span class="pln">&nbsp;backPropagate</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln">&nbsp;targets</span><span class="pun">,</span><span class="pln">&nbsp;N</span><span class="pun">,</span><span class="pln">&nbsp;M</span><span class="pun">):</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="str">"""</span></li><li class="L2"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;后向传播算法</span></li><li class="L3"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:param&nbsp;targets:&nbsp;实例的类别&nbsp;</span></li><li class="L4"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:param&nbsp;N:&nbsp;本次学习率</span></li><li class="L5"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:param&nbsp;M:&nbsp;上次学习率</span></li><li class="L6"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:return:&nbsp;最终的误差平方和的一半</span></li><li class="L7"><span class="str">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;http://www.youtube.com/watch?v=aVId8KMsdUU&amp;feature=BFa&amp;list=LLldMCkmXl4j9_v0HeKdNcRA</span></li><li class="L9"><span class="pln">&nbsp;</span></li><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;计算输出层&nbsp;deltas</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;dE/dw[j][k]&nbsp;=&nbsp;(t[k]&nbsp;-&nbsp;ao[k])&nbsp;*&nbsp;s'(&nbsp;SUM(&nbsp;w[j][k]*ah[j]&nbsp;)&nbsp;)&nbsp;*&nbsp;ah[j]</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_deltas&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="lit">0.0</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="kwd">no</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;k&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="kwd">no</span><span class="pun">):</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;error&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;targets</span><span class="pun">[</span><span class="pln">k</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ao</span><span class="pun">[</span><span class="pln">k</span><span class="pun">]</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_deltas</span><span class="pun">[</span><span class="pln">k</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;error&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;dsigmoid</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ao</span><span class="pun">[</span><span class="pln">k</span><span class="pun">])</span></li><li class="L6"><span class="pln">&nbsp;</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;更新输出层权值</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;j&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">nh</span><span class="pun">):</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;k&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="kwd">no</span><span class="pun">):</span></li><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;output_deltas[k]&nbsp;*&nbsp;self.ah[j]&nbsp;才是&nbsp;dError/dweight[j][k]</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;change&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;output_deltas</span><span class="pun">[</span><span class="pln">k</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ah</span><span class="pun">[</span><span class="pln">j</span><span class="pun">]</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">wo</span><span class="pun">[</span><span class="pln">j</span><span class="pun">][</span><span class="pln">k</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">+=</span><span class="pln">&nbsp;N&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;change&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;M&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">co</span><span class="pun">[</span><span class="pln">j</span><span class="pun">][</span><span class="pln">k</span><span class="pun">]</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">co</span><span class="pun">[</span><span class="pln">j</span><span class="pun">][</span><span class="pln">k</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;change</span></li><li class="L4"><span class="pln">&nbsp;</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;计算隐藏层&nbsp;deltas</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hidden_deltas&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="lit">0.0</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">nh</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;j&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">nh</span><span class="pun">):</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;error&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0.0</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;k&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="kwd">no</span><span class="pun">):</span></li><li class="L0"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;error&nbsp;</span><span class="pun">+=</span><span class="pln">&nbsp;output_deltas</span><span class="pun">[</span><span class="pln">k</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">wo</span><span class="pun">[</span><span class="pln">j</span><span class="pun">][</span><span class="pln">k</span><span class="pun">]</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hidden_deltas</span><span class="pun">[</span><span class="pln">j</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;error&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;dsigmoid</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ah</span><span class="pun">[</span><span class="pln">j</span><span class="pun">])</span></li><li class="L2"><span class="pln">&nbsp;</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;更新输入层权值</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;i&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ni</span><span class="pun">):</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;j&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">nh</span><span class="pun">):</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;change&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;hidden_deltas</span><span class="pun">[</span><span class="pln">j</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ai</span><span class="pun">[</span><span class="pln">i</span><span class="pun">]</span></li><li class="L7"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;print&nbsp;'activation',self.ai[i],'synapse',i,j,'change',change</span></li><li class="L8"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">wi</span><span class="pun">[</span><span class="pln">i</span><span class="pun">][</span><span class="pln">j</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">+=</span><span class="pln">&nbsp;N&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;change&nbsp;</span><span class="pun">+</span><span class="pln">&nbsp;M&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ci</span><span class="pun">[</span><span class="pln">i</span><span class="pun">][</span><span class="pln">j</span><span class="pun">]</span></li><li class="L9"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ci</span><span class="pun">[</span><span class="pln">i</span><span class="pun">][</span><span class="pln">j</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;change</span></li><li class="L0"><span class="pln">&nbsp;</span></li><li class="L1"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;计算误差平方和</span></li><li class="L2"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="com">#&nbsp;1/2&nbsp;是为了好看，**2&nbsp;是平方</span></li><li class="L3"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;error&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0.0</span></li><li class="L4"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">for</span><span class="pln">&nbsp;k&nbsp;</span><span class="kwd">in</span><span class="pln">&nbsp;range</span><span class="pun">(</span><span class="pln">len</span><span class="pun">(</span><span class="pln">targets</span><span class="pun">)):</span></li><li class="L5"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;error&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0.5</span><span class="pln">&nbsp;</span><span class="pun">*</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="pln">targets</span><span class="pun">[</span><span class="pln">k</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">-</span><span class="pln">&nbsp;</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">ao</span><span class="pun">[</span><span class="pln">k</span><span class="pun">])</span><span class="pln">&nbsp;</span><span class="pun">**</span><span class="pln">&nbsp;</span><span class="lit">2</span></li><li class="L6"><span class="pln">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="kwd">return</span><span class="pln">&nbsp;error</span></li></ol></pre>
<p style="text-indent: 2em;">注意不同于上文的单一学习率<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exstkm7wu0j200d00j0ng.jpg" title="神经网络75.png" alt="神经网络75.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">，这里有两个学习率N和M。N相当于上文的<img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exstkm7wu0j200d00j0ng.jpg" title="神经网络75.png" alt="神经网络75.png" style="text-indent: 32px; white-space: normal;" data-tag="bdshare">，而M则是在用上次训练的梯度更新权值时的学习率。这种同时考虑最近两次迭代得到的梯度的方法，可以看做是对单一学习率的改进。</p>
<p style="text-indent: 2em;">另外，这里并没有出现任何更新偏置的操作，为什么？</p>
<p style="text-indent: 2em;">因为这里的偏置是单独作为一个偏置节点放到输入层里的，它的值（输出，没有输入）固定为1，它的权值已经自动包含在上述权值调整中了。</p>
<p style="text-indent: 2em;">如果将偏置作为分别绑定到所有神经元的许多值，那么则需要进行偏置调整，而不需要权值调整（此时没有偏置节点）。</p>
<p style="text-indent: 2em;">哪个方便，当然是前者了，这也导致了大部分神经网络实现都采用前一种做法。</p>
<h2 id="h2-12">完整的实现</h2>
<p style="text-indent: 2em;">已开源到了Github上：<a href="https://github.com/hankcs/neural_net" target="_blank" rel="external nofollow" textvalue="https://github.com/hankcs/neural_net">https://github.com/hankcs/neural_net</a></p>
<p style="text-indent: 2em;">这一模块的原作者是Neil Schemenauer，我做了些注释。</p>
<p style="text-indent: 2em;">直接运行bpnn.py即可得到输出：</p>
<pre class="brush:python;toolbar:false prettyprint linenums"><ol class="linenums"><li class="L0"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.171204877501</span></li><li class="L1"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.190866985872</span></li><li class="L2"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.126126875154</span></li><li class="L3"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.0658488960415</span></li><li class="L4"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.0353249077599</span></li><li class="L5"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.0214428399072</span></li><li class="L6"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.0144886807614</span></li><li class="L7"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.0105787745309</span></li><li class="L8"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.00816264126944</span></li><li class="L9"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.00655731212209</span></li><li class="L0"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.00542964723539</span></li><li class="L1"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.00460235328667</span></li><li class="L2"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.00397407912435</span></li><li class="L3"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.00348339081276</span></li><li class="L4"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.00309120476889</span></li><li class="L5"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.00277163178862</span></li><li class="L6"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.00250692771135</span></li><li class="L7"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.00228457151714</span></li><li class="L8"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.00209550313514</span></li><li class="L9"><span class="typ">Combined</span><span class="pln">&nbsp;error&nbsp;</span><span class="lit">0.00193302192499</span></li><li class="L0"><span class="typ">Inputs</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="lit">0</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">--&gt;</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="lit">0.9982333356008245</span><span class="pun">]</span><span class="pln">&nbsp;	</span><span class="typ">Target</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="lit">1</span><span class="pun">]</span></li><li class="L1"><span class="typ">Inputs</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="lit">0</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">--&gt;</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="lit">0.9647325217906978</span><span class="pun">]</span><span class="pln">&nbsp;	</span><span class="typ">Target</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="lit">1</span><span class="pun">]</span></li><li class="L2"><span class="typ">Inputs</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="lit">1</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">--&gt;</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="lit">0.9627966274767186</span><span class="pun">]</span><span class="pln">&nbsp;	</span><span class="typ">Target</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="lit">1</span><span class="pun">]</span></li><li class="L3"><span class="typ">Inputs</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="lit">1</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">1</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">--&gt;</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="lit">0.05966109502803293</span><span class="pun">]</span><span class="pln">&nbsp;	</span><span class="typ">Target</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="lit">0</span><span class="pun">]</span></li></ol></pre>
<p style="text-indent: 2em;">IBM利用Neil Schemenauer的这一模块（旧版）做了一个识别代码语言的例子，我将其更新到新版，已经整合到了项目中。</p>
<p style="text-indent: 2em;">要运行测试的话，执行命令</p>
<pre class="brush:bash;toolbar:false prettyprint linenums"><ol class="linenums"><li class="L0"><span class="pln">code_recognizer</span><span class="pun">.</span><span class="pln">py&nbsp;testdata</span><span class="pun">.</span><span class="lit">200</span></li></ol></pre>
<p style="text-indent: 2em;">即可得到输出：</p>
<pre class="brush:plain;toolbar:false prettyprint linenums"><ol class="linenums"><li class="L0"><span class="pln">ERROR_CUTOFF&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0.01</span></li><li class="L1"><span class="pln">INPUTS&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">20</span></li><li class="L2"><span class="pln">ITERATIONS&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">1000</span></li><li class="L3"><span class="pln">MOMENTUM&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0.1</span></li><li class="L4"><span class="pln">TESTSIZE&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">500</span></li><li class="L5"><span class="pln">OUTPUTS&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">3</span></li><li class="L6"><span class="pln">TRAINSIZE&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">500</span></li><li class="L7"><span class="pln">LEARNRATE&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">0.5</span></li><li class="L8"><span class="pln">HIDDEN&nbsp;</span><span class="pun">=</span><span class="pln">&nbsp;</span><span class="lit">8</span></li><li class="L9"><span class="typ">Targets</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="pun">[</span><span class="lit">1</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pun">,</span><span class="pln">&nbsp;</span><span class="lit">0</span><span class="pun">]</span><span class="pln">&nbsp;</span><span class="pun">--</span><span class="pln">&nbsp;</span><span class="typ">Errors</span><span class="pun">:</span><span class="pln">&nbsp;</span><span class="pun">(</span><span class="lit">0.000</span><span class="pln">&nbsp;OK</span><span class="pun">)</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="pun">(</span><span class="lit">0.001</span><span class="pln">&nbsp;OK</span><span class="pun">)</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="pun">(</span><span class="lit">0.000</span><span class="pln">&nbsp;OK</span><span class="pun">)</span><span class="pln">&nbsp;&nbsp;&nbsp;</span><span class="pun">--</span><span class="pln">&nbsp;SUCCESS</span><span class="pun">!</span></li></ol></pre>
<p style="text-indent: 2em;">值得一提的是，这里的HIDDEN = 8指的是隐藏层的节点个数，不是层数，层数多了就变成DeepLearning了。</p>
<h2 id="h2-13">Reference</h2>
<p style="text-indent: 2em;"><a href="http://arctrix.com/nas/python/bpnn.py" _src="http://arctrix.com/nas/python/bpnn.py">http://arctrix.com/nas/python/bpnn.py</a></p>
<p style="text-indent: 2em;"><a href="http://code.activestate.com/recipes/578148-simple-back-propagation-neural-network-in-python-s/" _src="http://code.activestate.com/recipes/578148-simple-back-propagation-neural-network-in-python-s/">http://code.activestate.com/recipes/578148-simple-back-propagation-neural-network-in-python-s/</a></p>
<p style="text-indent: 2em;"><a href="https://www.youtube.com/watch?v=aVId8KMsdUU&amp;feature=BFa&amp;list=LLldMCkmXl4j9_v0HeKdNcRA" _src="https://www.youtube.com/watch?v=aVId8KMsdUU&amp;feature=BFa&amp;list=LLldMCkmXl4j9_v0HeKdNcRA">https://www.youtube.com/watch?v=aVId8KMsdUU&amp;feature=BFa&amp;list=LLldMCkmXl4j9_v0HeKdNcRA</a></p>
<p style="line-height: 16px; text-indent: 2em;"><img style="vertical-align: middle; margin-right: 2px;" src="./反向传播神经网络极简入门-码农场_files/icon_pdf.gif" data-tag="bdshare"><a style="font-size:12px; color:#0066cc;" href="http://www.hankcs.com/wp-content/uploads/2015/11/The%20back-propagation%20algorithm.pdf" title="The back-propagation algorithm.pdf">The back-propagation algorithm.pdf</a></p>
<p style="line-height: 16px; text-indent: 2em;"><a href="http://ufldl.stanford.edu/wiki/index.php/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" _src="http://ufldl.stanford.edu/wiki/index.php/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">http://ufldl.stanford.edu/wiki/index.php/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C</a></p>
</ul>
<p class="post-copyright"><a href="http://www.hankcs.com/license/" target="_blank"><img alt="知识共享许可协议" style="border-width: 0px;margin: 0 !important;" src="./反向传播神经网络极简入门-码农场_files/CC-BY-NC-SA-icon-88x31.png" width="88" height="31" border="0" vspace="0" title="知识共享许可协议" data-tag="bdshare"></a>&nbsp;<a href="http://www.hankcs.com/license/" target="_blank" textvalue="知识共享署名-非商业性使用-相同方式共享">知识共享署名-非商业性使用-相同方式共享</a>：<a href="http://www.hankcs.com/">码农场</a> » <a href="http://www.hankcs.com/ml/back-propagation-neural-network.html">反向传播神经网络极简入门</a></p>		</article>
								<div class="action-share bdsharebuttonbox bdshare-button-style0-24" data-bd-bind="1502788217824">
			<span>分享到：</span><a class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博"></a><a class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a><a class="bds_bdhome" data-cmd="bdhome" title="分享到百度新首页"></a><a class="bds_tqf" data-cmd="tqf" title="分享到腾讯朋友"></a><a class="bds_renren" data-cmd="renren" title="分享到人人网"></a><a class="bds_diandian" data-cmd="diandian" title="分享到点点网"></a><a class="bds_youdao" data-cmd="youdao" title="分享到有道云笔记"></a><a class="bds_ty" data-cmd="ty" title="分享到天涯社区"></a><a class="bds_kaixin001" data-cmd="kaixin001" title="分享到开心网"></a><a class="bds_taobao" data-cmd="taobao"></a><a class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a class="bds_mail" data-cmd="mail" title="分享到邮件分享"></a><a class="bds_copy" data-cmd="copy" title="分享到复制网址"></a><a class="bds_more" data-cmd="more">更多</a> <span>(</span><a class="bds_count" data-cmd="count" title="累计分享15次">15</a><span>)</span>		</div>
		<div class="article-tags">继续浏览有关 <a href="http://www.hankcs.com/ml/"><i class="fa fa-folder-open"></i> 机器学习</a> 的文章</div>		<div class="asb asb-post asb-post-02"><script async="" src="./反向传播神经网络极简入门-码农场_files/adsbygoogle.js.下载"></script>
<!-- 文章页正文下 页首横幅 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-1152644711996772" data-ad-slot="2657945648" data-adsbygoogle-status="done"><ins id="aswift_1_expand" style="display:inline-table;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent"><ins id="aswift_1_anchor" style="display:block;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent"><iframe width="728" height="90" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_1" name="aswift_1" style="left:0;position:absolute;top:0;width:728px;height:90px;" src="./反向传播神经网络极简入门-码农场_files/saved_resource(3).html"></iframe></ins></ins></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>		<nav class="article-nav">
			<span class="article-nav-prev">上一篇 <a href="http://www.hankcs.com/ml/lagrange-duality.html" rel="prev">拉格朗日对偶性</a></span>
			<span class="article-nav-next"><a href="http://www.hankcs.com/ml/support-vector-machine.html" rel="next">支持向量机</a> 下一篇</span>
		</nav>
				<div class="asb asb-post asb-post-03"><script async="" src="./反向传播神经网络极简入门-码农场_files/adsbygoogle.js.下载"></script>
<!-- 匹配内容 -->
<ins class="adsbygoogle" style="display: block; height: 466px;" data-ad-client="ca-pub-1152644711996772" data-ad-slot="7343699642" data-ad-format="autorelaxed" data-adsbygoogle-status="done"><ins id="aswift_2_expand" style="display:inline-table;border:none;height:466px;margin:0;padding:0;position:relative;visibility:visible;width:778px;background-color:transparent"><ins id="aswift_2_anchor" style="display:block;border:none;height:466px;margin:0;padding:0;position:relative;visibility:visible;width:778px;background-color:transparent"><iframe width="778" height="466" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_2" name="aswift_2" style="left:0;position:absolute;top:0;width:778px;height:466px;" src="./反向传播神经网络极简入门-码农场_files/saved_resource(4).html"></iframe></ins></ins></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>		<div class="title" id="comments">
	<h3>评论 <b>25</b></h3>
</div>
<div id="respond" class="no_webshot">
		
	<form action="http://www.hankcs.com/wp-comments-post.php" method="post" id="commentform">
		<div class="comt">
			<div class="comt-title">
				<img alt="" data-src="http://1.gravatar.com/avatar/?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://0.gravatar.com/avatar/?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo avatar-default" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/saved_resource" style="display: inline;">				<p><a id="cancel-comment-reply-link" href="javascript:;">取消</a></p>
			</div>
			<div class="comt-box">
				<textarea placeholder="此处不受理任何开源项目问题，请在GitHub上发issue ，大家一起讨论，谢谢。" class="input-block-level comt-area" name="comment" id="comment" cols="100%" rows="3" tabindex="1" onkeydown="if(event.ctrlKey&amp;&amp;event.keyCode==13){document.getElementById(&#39;submit&#39;).click();return false};"></textarea>
				<div class="comt-ctrl">
					<div class="comt-tips"><input type="hidden" name="comment_post_ID" value="6796" id="comment_post_ID">
<input type="hidden" name="comment_parent" id="comment_parent" value="0">
<p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="086f8d603d"></p><label for="comment_mail_notify" class="checkbox inline hide" style="padding-top:0"><input type="checkbox" name="comment_mail_notify" id="comment_mail_notify" value="comment_mail_notify" checked="checked">有人回复时邮件通知我</label><p style="display: none;"></p><div class="comt-tip comt-loading" style="display: none;">评论提交中...</div><div class="comt-tip comt-error" style="display: none;">#</div></div>
					<button type="submit" name="submit" id="submit" tabindex="5">提交评论</button>
					<!-- <span data-type="comment-insert-smilie" class="muted comt-smilie"><i class="icon-thumbs-up icon12"></i> 表情</span> -->
				</div>
			</div>

												<div class="comt-comterinfo" id="comment-author-info">
						<ul>
							<li class="form-inline"><label class="hide" for="author">昵称</label><input class="ipt" type="text" name="author" id="author" value="" tabindex="2" placeholder="昵称"><span class="text-muted">昵称 (必填)</span></li>
							<li class="form-inline"><label class="hide" for="email">邮箱</label><input class="ipt" type="text" name="email" id="email" value="" tabindex="3" placeholder="邮箱"><span class="text-muted">邮箱 (必填)</span></li>
							<li class="form-inline"><label class="hide" for="url">网址</label><input class="ipt" type="text" name="url" id="url" value="" tabindex="4" placeholder="网址"><span class="text-muted">网址</span></li>
						</ul>
					</div>
									</div>

	<input type="hidden" id="ak_js" name="ak_js" value="1502788217370"></form>
	</div>
<div id="postcomments">
	<ol class="commentlist">
		<li class="comment even thread-even depth-1" id="comment-4625"><span class="comt-f">#16</span><div class="comt-avatar"><img alt="" data-src="http://0.gravatar.com/avatar/fea60393c3554f1dcd681e509e699ba6?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://0.gravatar.com/avatar/fea60393c3554f1dcd681e509e699ba6?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/fea60393c3554f1dcd681e509e699ba6" style="display: block;"></div><div class="comt-main" id="div-comment-4625"><p>输出层权值调整中的中有一个公式：无关变量拿出来——&gt;求和的符号没有了！</p>
<div class="comt-meta"><span class="comt-author">憨熊</span>5个月前 (03-22)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-4625&quot;, &quot;4625&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给憨熊">回复</a></div></div></li><!-- #comment-## -->
<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-4472"><span class="comt-f">#15</span><div class="comt-avatar"><img alt="" data-src="http://0.gravatar.com/avatar/?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://1.gravatar.com/avatar/?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo avatar-default" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/saved_resource(1)" style="display: block;"></div><div class="comt-main" id="div-comment-4472"><p>你好，“利用刚才定义的δk，最终得到：”这一步我不太明白是怎么转换的，谢谢！</p>
<div class="comt-meta"><span class="comt-author"><a href="http://t.qq.com/jack798355913" rel="external nofollow" class="url" target="_blank">jack</a></span>7个月前 (01-09)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-4472&quot;, &quot;4472&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给jack">回复</a></div></div></li><!-- #comment-## -->
<li class="comment even thread-even depth-1" id="comment-4301"><span class="comt-f">#14</span><div class="comt-avatar"><img alt="" data-src="http://0.gravatar.com/avatar/c201bc88827634e249acc22b2308a8c9?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://0.gravatar.com/avatar/c201bc88827634e249acc22b2308a8c9?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/c201bc88827634e249acc22b2308a8c9" style="display: block;"></div><div class="comt-main" id="div-comment-4301"><p>“由于输出层第k个节点的输入神经网络Xk=OjWjk”，这个不对啊。应该是Xk=sigma(OjWjk+bj)，但是由于是对Wjk求偏导，所以倒数第一层的隐层其他节点是常数，偏导数为0.</p>
<div class="comt-meta"><span class="comt-author"><a href="http://weibo.com/DjangoP" rel="external nofollow" class="url" target="_blank">PJT</a></span>1年前 (2016-08-13)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-4301&quot;, &quot;4301&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给PJT">回复</a></div></div><ul class="children">
<li class="comment byuser comment-author-hankcs bypostauthor odd alt depth-2" id="comment-4302"><div class="comt-avatar"><img alt="" data-src="http://1.gravatar.com/avatar/de961732ee897fa88c707396fb1d55a4?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://1.gravatar.com/avatar/de961732ee897fa88c707396fb1d55a4?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/de961732ee897fa88c707396fb1d55a4" style="display: block;"></div><div class="comt-main" id="div-comment-4302"><p>输出等于输入代入Sigmoid函数，作为下一级的输入，sigmoid已经代过一次了</p>
<div class="comt-meta"><span class="comt-author"><a href="http://www.hankcs.com/" rel="external nofollow" class="url" target="_blank">hankcs</a></span>1年前 (2016-08-13)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-4302&quot;, &quot;4302&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给hankcs">回复</a></div></div></li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
<li class="comment even thread-odd thread-alt depth-1" id="comment-4209"><span class="comt-f">#13</span><div class="comt-avatar"><img alt="" data-src="http://0.gravatar.com/avatar/6ed291e711442fce63e02ce303245744?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://0.gravatar.com/avatar/6ed291e711442fce63e02ce303245744?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/6ed291e711442fce63e02ce303245744" style="display: block;"></div><div class="comt-main" id="div-comment-4209"><p>结果不对哦；输入为00时，异或结果应该是0；把目标值改成0之后，就不收敛了，无法训练出结果；但是把sigmoid函数改成双曲的就可以，why？</p>
<div class="comt-meta"><span class="comt-author">将中正</span>1年前 (2016-06-12)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-4209&quot;, &quot;4209&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给将中正">回复</a></div></div><ul class="children">
<li class="comment odd alt depth-2" id="comment-4210"><div class="comt-avatar"><img alt="" data-src="http://0.gravatar.com/avatar/6ed291e711442fce63e02ce303245744?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://0.gravatar.com/avatar/6ed291e711442fce63e02ce303245744?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/6ed291e711442fce63e02ce303245744" style="display: block;"></div><div class="comt-main" id="div-comment-4210"><p><a href="https://github.com/hankcs/neural_net" rel="nofollow">https://github.com/hankcs/neural_net</a>  指的这里的bpnn.py代码</p>
<div class="comt-meta"><span class="comt-author">将中正</span>1年前 (2016-06-12)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-4210&quot;, &quot;4210&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给将中正">回复</a></div></div></li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
<li class="comment even thread-even depth-1" id="comment-4159"><span class="comt-f">#12</span><div class="comt-avatar"><img alt="" data-src="http://0.gravatar.com/avatar/?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://0.gravatar.com/avatar/?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo avatar-default" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/saved_resource(1)" style="display: block;"></div><div class="comt-main" id="div-comment-4159"><p>你好，这个链接打不开，能不能重新发一个链接呢</p>
<div class="comt-meta"><span class="comt-author"><a href="http://t.qq.com/s568384420" rel="external nofollow" class="url" target="_blank">想你了</a></span>1年前 (2016-05-13)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-4159&quot;, &quot;4159&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给想你了">回复</a></div></div></li><!-- #comment-## -->
<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-3845"><span class="comt-f">#11</span><div class="comt-avatar"><img alt="" data-src="http://0.gravatar.com/avatar/0744bf6cebd6c6d406a80e2d3b3488c7?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://0.gravatar.com/avatar/0744bf6cebd6c6d406a80e2d3b3488c7?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/0744bf6cebd6c6d406a80e2d3b3488c7" style="display: block;"></div><div class="comt-main" id="div-comment-3845"><p>每个变量解释得很清楚，抛出公式前做好了铺垫，赞。<br>
另外提一个公式表达的问题，文章将权值求和用变量z来代替的部分，用了向量的表示方法，但是没有加粗或者尖头来表明z，W，x，b是变量。</p>
<div class="comt-meta"><span class="comt-author">Jiajun</span>1年前 (2016-04-06)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-3845&quot;, &quot;3845&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给Jiajun">回复</a></div></div><ul class="children">
<li class="comment byuser comment-author-hankcs bypostauthor even depth-2" id="comment-3847"><div class="comt-avatar"><img alt="" data-src="http://1.gravatar.com/avatar/de961732ee897fa88c707396fb1d55a4?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://1.gravatar.com/avatar/de961732ee897fa88c707396fb1d55a4?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/de961732ee897fa88c707396fb1d55a4" style="display: block;"></div><div class="comt-main" id="div-comment-3847"><p>谢谢你的建议，这些公式来自斯坦福的讲义，可能与国内的标准有些不同。</p>
<div class="comt-meta"><span class="comt-author"><a href="http://www.hankcs.com/" rel="external nofollow" class="url" target="_blank">hankcs</a></span>1年前 (2016-04-08)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-3847&quot;, &quot;3847&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给hankcs">回复</a></div></div></li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
<li class="comment odd alt thread-even depth-1" id="comment-3805"><span class="comt-f">#10</span><div class="comt-avatar"><img alt="" data-src="http://2.gravatar.com/avatar/?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://1.gravatar.com/avatar/?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo avatar-default" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/saved_resource(2)" style="display: block;"></div><div class="comt-main" id="div-comment-3805"><p>请问hankcs，神经网络72.png中的delta k 指的是输出层中计算得到的delta k吗？假如我存在两个隐藏层H1与H2，H2的delta j按照神经网络72.png中计算，那么H1的delta的公式应该如何表示呢，我看了之前的推导，神经网络72.png的公式是利用了输出层与隐层是相连接的关系而导出的，有些困惑，期待解答。</p>
<div class="comt-meta"><span class="comt-author">酥酥酥糖</span>1年前 (2016-03-22)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-3805&quot;, &quot;3805&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给酥酥酥糖">回复</a></div></div></li><!-- #comment-## -->
<li class="comment even thread-odd thread-alt depth-1" id="comment-3774"><span class="comt-f">#9</span><div class="comt-avatar"><img alt="" data-src="http://2.gravatar.com/avatar/?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://2.gravatar.com/avatar/?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo avatar-default" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/saved_resource(2)" style="display: block;"></div><div class="comt-main" id="div-comment-3774"><p>应该是“阈”值（Threshold）不是阀值。其它写的很好，受教了 <img draggable="false" class="emoji" alt="🙂" src="./反向传播神经网络极简入门-码农场_files/1f642.svg"></p>
<div class="comt-meta"><span class="comt-author">白杨 baiy.cn</span>1年前 (2016-03-11)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-3774&quot;, &quot;3774&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给白杨 baiy.cn">回复</a></div></div><ul class="children">
<li class="comment byuser comment-author-hankcs bypostauthor odd alt depth-2" id="comment-3775"><div class="comt-avatar"><img alt="" data-src="http://1.gravatar.com/avatar/de961732ee897fa88c707396fb1d55a4?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://1.gravatar.com/avatar/de961732ee897fa88c707396fb1d55a4?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/de961732ee897fa88c707396fb1d55a4" style="display: block;"></div><div class="comt-main" id="div-comment-3775"><p>感谢指正</p>
<div class="comt-meta"><span class="comt-author"><a href="http://www.hankcs.com/" rel="external nofollow" class="url" target="_blank">hankcs</a></span>1年前 (2016-03-12)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-3775&quot;, &quot;3775&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给hankcs">回复</a></div></div></li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
<li class="comment even thread-even depth-1" id="comment-3462"><span class="comt-f">#8</span><div class="comt-avatar"><img alt="" data-src="http://2.gravatar.com/avatar/?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://0.gravatar.com/avatar/?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo avatar-default" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/saved_resource(2)" style="display: block;"></div><div class="comt-main" id="div-comment-3462"><p>你整理的文档非常好，但我觉得一个地方不太妥当：<br>
就是你的代码从隐含层到输出层你并没有加上bias*权重，是不是在初始化时，隐含层的节点数也应该+1？，也就是说从隐含层到输出层漏掉了偏置，与你理论地方不太符合。</p>
<div class="comt-meta"><span class="comt-author"><a href="http://t.qq.com/cqydzang" rel="external nofollow" class="url" target="_blank">臧文华</a></span>2年前 (2015-11-22)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-3462&quot;, &quot;3462&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给臧文华">回复</a></div></div><ul class="children">
<li class="comment byuser comment-author-hankcs bypostauthor odd alt depth-2" id="comment-3469"><div class="comt-avatar"><img alt="" data-src="http://1.gravatar.com/avatar/de961732ee897fa88c707396fb1d55a4?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://1.gravatar.com/avatar/de961732ee897fa88c707396fb1d55a4?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/de961732ee897fa88c707396fb1d55a4" style="display: block;"></div><div class="comt-main" id="div-comment-3469"><p>好问题。</p>
<p>我给隐藏层添加了一个偏置节点，并将其输出固定为+1，代码在：https://gist.github.com/hankcs/53a20ef3cd5f246b9dd1</p>
<p>前后效果如下：<br>
Combined error 0.00193302192499<br>
Combined error 0.00262203286178</p>
<p>在testdata.200测试集上效果不变：<br>
Success rate against test data: 65.20%<br>
Success rate against test data: 65.20%</p>
<p>欢迎进一步探讨。</p>
<div class="comt-meta"><span class="comt-author"><a href="http://www.hankcs.com/" rel="external nofollow" class="url" target="_blank">hankcs</a></span>2年前 (2015-11-23)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-3469&quot;, &quot;3469&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给hankcs">回复</a></div></div></li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
<li class="comment even thread-odd thread-alt depth-1" id="comment-3451"><span class="comt-f">#7</span><div class="comt-avatar"><img alt="" data-src="http://2.gravatar.com/avatar/?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://2.gravatar.com/avatar/?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo avatar-default" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/saved_resource(2)" style="display: block;"></div><div class="comt-main" id="div-comment-3451"><p>请问hankcs，你所说的Ryan讲解反向传播神经网络的视频在哪？恕我无知！</p>
<div class="comt-meta"><span class="comt-author"><a href="http://weibo.com/1995474551" rel="external nofollow" class="url" target="_blank">O_Ramon</a></span>2年前 (2015-11-19)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-3451&quot;, &quot;3451&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给O_Ramon">回复</a></div></div><ul class="children">
<li class="comment byuser comment-author-hankcs bypostauthor odd alt depth-2" id="comment-3452"><div class="comt-avatar"><img alt="" data-src="http://1.gravatar.com/avatar/de961732ee897fa88c707396fb1d55a4?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://1.gravatar.com/avatar/de961732ee897fa88c707396fb1d55a4?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/de961732ee897fa88c707396fb1d55a4" style="display: block;"></div><div class="comt-main" id="div-comment-3452"><p>请参考Reference</p>
<div class="comt-meta"><span class="comt-author"><a href="http://www.hankcs.com/" rel="external nofollow" class="url" target="_blank">hankcs</a></span>2年前 (2015-11-19)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-3452&quot;, &quot;3452&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给hankcs">回复</a></div></div></li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
<li class="comment even thread-even depth-1" id="comment-3408"><span class="comt-f">#6</span><div class="comt-avatar"><img alt="" data-src="http://0.gravatar.com/avatar/?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://2.gravatar.com/avatar/?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo avatar-default" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/saved_resource(1)" style="display: block;"></div><div class="comt-main" id="div-comment-3408"><p>正好我也在看BP的推导[嘻嘻]</p>
<div class="comt-meta"><span class="comt-author"><a href="http://weibo.com/1923852757" rel="external nofollow" class="url" target="_blank">抬起頭向朝陽浪跡天涯</a></span>2年前 (2015-11-12)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-3408&quot;, &quot;3408&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给抬起頭向朝陽浪跡天涯">回复</a></div></div></li><!-- #comment-## -->
<li class="comment byuser comment-author-hankcs bypostauthor odd alt thread-odd thread-alt depth-1" id="comment-3407"><span class="comt-f">#5</span><div class="comt-avatar"><img alt="" data-src="http://1.gravatar.com/avatar/de961732ee897fa88c707396fb1d55a4?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://1.gravatar.com/avatar/de961732ee897fa88c707396fb1d55a4?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/de961732ee897fa88c707396fb1d55a4" style="display: block;"></div><div class="comt-main" id="div-comment-3407"><p>感谢指正，的确是x_k=w_jk*O_j，线性相关求导后得到O_j，这样两个不对的地方都解决了。</p>
<div class="comt-meta"><span class="comt-author"><a href="http://www.hankcs.com/" rel="external nofollow" class="url" target="_blank">hankcs</a></span>2年前 (2015-11-12)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-3407&quot;, &quot;3407&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给hankcs">回复</a></div></div></li><!-- #comment-## -->
<li class="comment byuser comment-author-hankcs bypostauthor even thread-even depth-1" id="comment-3406"><span class="comt-f">#4</span><div class="comt-avatar"><img alt="" data-src="http://1.gravatar.com/avatar/de961732ee897fa88c707396fb1d55a4?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://1.gravatar.com/avatar/de961732ee897fa88c707396fb1d55a4?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/de961732ee897fa88c707396fb1d55a4" style="display: block;"></div><div class="comt-main" id="div-comment-3406"><p>感谢指正，的确是x_k=w_jk*O_j。</p>
<div class="comt-meta"><span class="comt-author"><a href="http://www.hankcs.com/" rel="external nofollow" class="url" target="_blank">hankcs</a></span>2年前 (2015-11-12)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-3406&quot;, &quot;3406&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给hankcs">回复</a></div></div></li><!-- #comment-## -->
<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-3405"><span class="comt-f">#3</span><div class="comt-avatar"><img alt="" data-src="http://0.gravatar.com/avatar/?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://2.gravatar.com/avatar/?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo avatar-default" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/saved_resource(1)" style="display: block;"></div><div class="comt-main" id="div-comment-3405"><p>大赞！写得非常棒！有个小疑问呢，就在后向传播中耍把戏那里：<br>
1）输出层第k个节点的输入神经网络38.png等于上一层第j个节点的输出神经网络39.png，这是为什么呢？输出层第K个节点的输入不是等与上一层所有节点的输出和相应权重的加权和吗？<br>
2）文中说「上一层的输出神经网络39.png是与到输出层的权值变量无关的，所以对神经网络39.png求权值变量神经网络40.png的偏导数直接等于其本身」我想问的是既然无关了，求导不就是0吗？线性相关的才会保持本身吧？</p>
<div class="comt-meta"><span class="comt-author"><a href="http://weibo.com/1923852757" rel="external nofollow" class="url" target="_blank">抬起頭向朝陽浪跡天涯</a></span>2年前 (2015-11-12)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-3405&quot;, &quot;3405&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给抬起頭向朝陽浪跡天涯">回复</a></div></div></li><!-- #comment-## -->
<li class="comment even thread-even depth-1" id="comment-3404"><span class="comt-f">#2</span><div class="comt-avatar"><img alt="" data-src="http://2.gravatar.com/avatar/8570bd9295e414bb43f05a49b3b5d832?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://2.gravatar.com/avatar/8570bd9295e414bb43f05a49b3b5d832?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/8570bd9295e414bb43f05a49b3b5d832" style="display: block;"></div><div class="comt-main" id="div-comment-3404"><p>大赞！写得非常棒！有个小疑问呢，就在耍把戏那里，输出层第k个节点的输入神经网络38.png等于上一层第j个节点的输出神经网络39.png，这是为什么呢？输出层第K个节点的输入不是等与上一层所有节点的输出和相应权重的加权和吗？</p>
<div class="comt-meta"><span class="comt-author">Hugo101</span>2年前 (2015-11-12)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-3404&quot;, &quot;3404&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给Hugo101">回复</a></div></div></li><!-- #comment-## -->
<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-3403"><span class="comt-f">#1</span><div class="comt-avatar"><img alt="" data-src="http://2.gravatar.com/avatar/b4d2138cb6bb5ad470102efdd7bad089?s=50&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g" srcset="http://2.gravatar.com/avatar/b4d2138cb6bb5ad470102efdd7bad089?s=100&amp;d=http%3A%2F%2Fwww.hankcs.com%2Fwp-content%2Fthemes%2Fdux%2Fimg%2Favatar-default.png&amp;r=g 2x" class="avatar avatar-50 photo" height="50" width="50" src="./反向传播神经网络极简入门-码农场_files/b4d2138cb6bb5ad470102efdd7bad089" style="display: block;"></div><div class="comt-main" id="div-comment-3403"><p>Java smile 这个工具可以关注下   蛮强大的</p>
<div class="comt-meta"><span class="comt-author">burnes</span>2年前 (2015-11-12)<a rel="nofollow" class="comment-reply-link" href="javascript:;" onclick="return addComment.moveForm( &quot;div-comment-3403&quot;, &quot;3403&quot;, &quot;respond&quot;, &quot;6796&quot; )" aria-label="回复给burnes">回复</a></div></div></li><!-- #comment-## -->
	</ol>
	<div class="pagenav">
			</div>
</div>
	</div>
	</div>
	<aside class="sidebar">
<div class="widget widget_categories affix" style="top: 15px;"><h3>栏目分类</h3><label class="screen-reader-text" for="cat">栏目分类</label><select name="cat" id="cat" class="postform">
	<option value="-1">选择分类目录</option>
	<option class="level-0" value="18">ACG&nbsp;&nbsp;(7)</option>
	<option class="level-1" value="117">&nbsp;&nbsp;&nbsp;游戏&nbsp;&nbsp;(5)</option>
	<option class="level-0" value="7">Web开发&nbsp;&nbsp;(80)</option>
	<option class="level-1" value="64">&nbsp;&nbsp;&nbsp;BAE&nbsp;&nbsp;(13)</option>
	<option class="level-1" value="11">&nbsp;&nbsp;&nbsp;Linux相关&nbsp;&nbsp;(9)</option>
	<option class="level-1" value="54">&nbsp;&nbsp;&nbsp;Mac OS&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="27">&nbsp;&nbsp;&nbsp;WordPress&nbsp;&nbsp;(8)</option>
	<option class="level-1" value="65">&nbsp;&nbsp;&nbsp;Yii&nbsp;&nbsp;(17)</option>
	<option class="level-1" value="2">&nbsp;&nbsp;&nbsp;主机域名&nbsp;&nbsp;(26)</option>
	<option class="level-1" value="66">&nbsp;&nbsp;&nbsp;数据库&nbsp;&nbsp;(4)</option>
	<option class="level-0" value="140">信息安全&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="1">其他类别&nbsp;&nbsp;(184)</option>
	<option class="level-1" value="78">&nbsp;&nbsp;&nbsp;心情&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="15">&nbsp;&nbsp;&nbsp;旧的博文&nbsp;&nbsp;(170)</option>
	<option class="level-0" value="87">操作系统&nbsp;&nbsp;(3)</option>
	<option class="level-1" value="88">&nbsp;&nbsp;&nbsp;Windows&nbsp;&nbsp;(2)</option>
	<option class="level-0" value="81">数学基礎&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="4">日语教程&nbsp;&nbsp;(120)</option>
	<option class="level-1" value="96">&nbsp;&nbsp;&nbsp;口译&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="59">&nbsp;&nbsp;&nbsp;新编日语商务贸易会话&nbsp;&nbsp;(14)</option>
	<option class="level-1" value="19">&nbsp;&nbsp;&nbsp;新编日语阅读文选&nbsp;&nbsp;(34)</option>
	<option class="level-2" value="44">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一册&nbsp;&nbsp;(20)</option>
	<option class="level-2" value="61">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第三册&nbsp;&nbsp;(2)</option>
	<option class="level-2" value="20">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第二册&nbsp;&nbsp;(10)</option>
	<option class="level-1" value="46">&nbsp;&nbsp;&nbsp;日语入门&nbsp;&nbsp;(2)</option>
	<option class="level-1" value="62">&nbsp;&nbsp;&nbsp;日语听力&nbsp;&nbsp;(2)</option>
	<option class="level-1" value="5">&nbsp;&nbsp;&nbsp;日语综合教程&nbsp;&nbsp;(64)</option>
	<option class="level-2" value="120">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第七册&nbsp;&nbsp;(14)</option>
	<option class="level-2" value="50">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第三册&nbsp;&nbsp;(7)</option>
	<option class="level-2" value="73">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第五册&nbsp;&nbsp;(12)</option>
	<option class="level-2" value="98">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第六册&nbsp;&nbsp;(18)</option>
	<option class="level-2" value="6">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第四册&nbsp;&nbsp;(12)</option>
	<option class="level-1" value="86">&nbsp;&nbsp;&nbsp;月の珊瑚&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="131">机器学习&nbsp;&nbsp;(57)</option>
	<option class="level-0" value="16">经济人文&nbsp;&nbsp;(19)</option>
	<option class="level-1" value="17">&nbsp;&nbsp;&nbsp;国际贸易理论与政策&nbsp;&nbsp;(9)</option>
	<option class="level-1" value="30">&nbsp;&nbsp;&nbsp;当代世界经济与政治&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="9">编程开发&nbsp;&nbsp;(556)</option>
	<option class="level-1" value="8">&nbsp;&nbsp;&nbsp;Android&nbsp;&nbsp;(30)</option>
	<option class="level-1" value="13">&nbsp;&nbsp;&nbsp;C++&nbsp;&nbsp;(237)</option>
	<option class="level-1" value="25">&nbsp;&nbsp;&nbsp;Drupal&nbsp;&nbsp;(23)</option>
	<option class="level-1" value="10">&nbsp;&nbsp;&nbsp;Java&nbsp;&nbsp;(69)</option>
	<option class="level-1" value="123">&nbsp;&nbsp;&nbsp;Javascript&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="24">&nbsp;&nbsp;&nbsp;PHP&nbsp;&nbsp;(57)</option>
	<option class="level-1" value="94">&nbsp;&nbsp;&nbsp;Python&nbsp;&nbsp;(8)</option>
	<option class="level-1" value="14">&nbsp;&nbsp;&nbsp;汇编逆向&nbsp;&nbsp;(12)</option>
	<option class="level-1" value="70">&nbsp;&nbsp;&nbsp;算法&nbsp;&nbsp;(235)</option>
	<option class="level-1" value="121">&nbsp;&nbsp;&nbsp;网络&nbsp;&nbsp;(6)</option>
	<option class="level-0" value="104">自然语言处理&nbsp;&nbsp;(91)</option>
	<option class="level-1" value="109">&nbsp;&nbsp;&nbsp;中文分词&nbsp;&nbsp;(11)</option>
	<option class="level-1" value="128">&nbsp;&nbsp;&nbsp;句法分析&nbsp;&nbsp;(6)</option>
	<option class="level-1" value="127">&nbsp;&nbsp;&nbsp;命名实体识别&nbsp;&nbsp;(7)</option>
	<option class="level-1" value="105">&nbsp;&nbsp;&nbsp;语料库&nbsp;&nbsp;(4)</option>
	<option class="level-0" value="12">软件发布&nbsp;&nbsp;(9)</option>
</select>

<script type="text/javascript">
/* <![CDATA[ */
(function() {
	var dropdown = document.getElementById( "cat" );
	function onCatChange() {
		if ( dropdown.options[ dropdown.selectedIndex ].value > 0 ) {
			location.href = "http://www.hankcs.com/?cat=" + dropdown.options[ dropdown.selectedIndex ].value;
		}
	}
	dropdown.onchange = onCatChange;
})();
/* ]]> */
</script>

</div><div class="widget widget_archive affix" style="top: 123px;"><h3>文章归档</h3>		<label class="screen-reader-text" for="archives-dropdown-5">文章归档</label>
		<select id="archives-dropdown-5" name="archive-dropdown" onchange="document.location.href=this.options[this.selectedIndex].value;">
			
			<option value="">选择月份</option>
				<option value="http://www.hankcs.com/2017/08/"> 2017年八月 &nbsp;(1)</option>
	<option value="http://www.hankcs.com/2017/07/"> 2017年七月 &nbsp;(14)</option>
	<option value="http://www.hankcs.com/2017/06/"> 2017年六月 &nbsp;(28)</option>
	<option value="http://www.hankcs.com/2017/05/"> 2017年五月 &nbsp;(8)</option>
	<option value="http://www.hankcs.com/2017/03/"> 2017年三月 &nbsp;(12)</option>
	<option value="http://www.hankcs.com/2017/02/"> 2017年二月 &nbsp;(13)</option>
	<option value="http://www.hankcs.com/2017/01/"> 2017年一月 &nbsp;(22)</option>
	<option value="http://www.hankcs.com/2016/12/"> 2016年十二月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2016/11/"> 2016年十一月 &nbsp;(15)</option>
	<option value="http://www.hankcs.com/2016/10/"> 2016年十月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2016/09/"> 2016年九月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2016/08/"> 2016年八月 &nbsp;(7)</option>
	<option value="http://www.hankcs.com/2016/07/"> 2016年七月 &nbsp;(1)</option>
	<option value="http://www.hankcs.com/2016/06/"> 2016年六月 &nbsp;(1)</option>
	<option value="http://www.hankcs.com/2016/05/"> 2016年五月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2016/04/"> 2016年四月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2016/03/"> 2016年三月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2016/02/"> 2016年二月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2015/12/"> 2015年十二月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2015/11/"> 2015年十一月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2015/10/"> 2015年十月 &nbsp;(4)</option>
	<option value="http://www.hankcs.com/2015/09/"> 2015年九月 &nbsp;(4)</option>
	<option value="http://www.hankcs.com/2015/08/"> 2015年八月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2015/07/"> 2015年七月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2015/05/"> 2015年五月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2015/04/"> 2015年四月 &nbsp;(5)</option>
	<option value="http://www.hankcs.com/2015/03/"> 2015年三月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2015/02/"> 2015年二月 &nbsp;(22)</option>
	<option value="http://www.hankcs.com/2015/01/"> 2015年一月 &nbsp;(14)</option>
	<option value="http://www.hankcs.com/2014/12/"> 2014年十二月 &nbsp;(10)</option>
	<option value="http://www.hankcs.com/2014/11/"> 2014年十一月 &nbsp;(21)</option>
	<option value="http://www.hankcs.com/2014/10/"> 2014年十月 &nbsp;(14)</option>
	<option value="http://www.hankcs.com/2014/09/"> 2014年九月 &nbsp;(16)</option>
	<option value="http://www.hankcs.com/2014/08/"> 2014年八月 &nbsp;(11)</option>
	<option value="http://www.hankcs.com/2014/07/"> 2014年七月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2014/06/"> 2014年六月 &nbsp;(13)</option>
	<option value="http://www.hankcs.com/2014/05/"> 2014年五月 &nbsp;(28)</option>
	<option value="http://www.hankcs.com/2014/04/"> 2014年四月 &nbsp;(41)</option>
	<option value="http://www.hankcs.com/2014/03/"> 2014年三月 &nbsp;(26)</option>
	<option value="http://www.hankcs.com/2014/02/"> 2014年二月 &nbsp;(52)</option>
	<option value="http://www.hankcs.com/2014/01/"> 2014年一月 &nbsp;(28)</option>
	<option value="http://www.hankcs.com/2013/12/"> 2013年十二月 &nbsp;(29)</option>
	<option value="http://www.hankcs.com/2013/11/"> 2013年十一月 &nbsp;(21)</option>
	<option value="http://www.hankcs.com/2013/10/"> 2013年十月 &nbsp;(11)</option>
	<option value="http://www.hankcs.com/2013/09/"> 2013年九月 &nbsp;(19)</option>
	<option value="http://www.hankcs.com/2013/08/"> 2013年八月 &nbsp;(22)</option>
	<option value="http://www.hankcs.com/2013/07/"> 2013年七月 &nbsp;(36)</option>
	<option value="http://www.hankcs.com/2013/06/"> 2013年六月 &nbsp;(24)</option>
	<option value="http://www.hankcs.com/2013/05/"> 2013年五月 &nbsp;(36)</option>
	<option value="http://www.hankcs.com/2013/04/"> 2013年四月 &nbsp;(29)</option>
	<option value="http://www.hankcs.com/2013/03/"> 2013年三月 &nbsp;(46)</option>
	<option value="http://www.hankcs.com/2013/02/"> 2013年二月 &nbsp;(5)</option>
	<option value="http://www.hankcs.com/2012/05/"> 2012年五月 &nbsp;(2)</option>
	<option value="http://www.hankcs.com/2012/04/"> 2012年四月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2010/12/"> 2010年十二月 &nbsp;(5)</option>
	<option value="http://www.hankcs.com/2010/11/"> 2010年十一月 &nbsp;(10)</option>
	<option value="http://www.hankcs.com/2010/10/"> 2010年十月 &nbsp;(13)</option>
	<option value="http://www.hankcs.com/2010/09/"> 2010年九月 &nbsp;(6)</option>
	<option value="http://www.hankcs.com/2010/08/"> 2010年八月 &nbsp;(5)</option>
	<option value="http://www.hankcs.com/2010/07/"> 2010年七月 &nbsp;(3)</option>
	<option value="http://www.hankcs.com/2010/06/"> 2010年六月 &nbsp;(12)</option>
	<option value="http://www.hankcs.com/2010/05/"> 2010年五月 &nbsp;(14)</option>
	<option value="http://www.hankcs.com/2010/04/"> 2010年四月 &nbsp;(8)</option>
	<option value="http://www.hankcs.com/2010/03/"> 2010年三月 &nbsp;(16)</option>
	<option value="http://www.hankcs.com/2010/01/"> 2010年一月 &nbsp;(16)</option>
	<option value="http://www.hankcs.com/2009/12/"> 2009年十二月 &nbsp;(33)</option>
	<option value="http://www.hankcs.com/2009/11/"> 2009年十一月 &nbsp;(26)</option>
	<option value="http://www.hankcs.com/2009/09/"> 2009年九月 &nbsp;(2)</option>

		</select>
		</div><div class="widget widget_ui_posts"><h3>热门文章</h3><ul>		<li><a target="_blank" href="http://www.hankcs.com/ml/machine-learning-entry-list.html"><span class="thumbnail"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1ew7s3qoi2uj20h30meaco.jpg" class="thumb" alt="机器学习入门书单" title="机器学习入门书单"></span><span class="text">机器学习入门书单</span><span class="muted">2015-02-04</span><span class="muted">评论(27)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/back-propagation-neural-network.html"><span class="thumbnail"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1exsm4yho09j208c044t8o(1).jpg" class="thumb" alt="反向传播神经网络极简入门" title="反向传播神经网络极简入门"></span><span class="text">反向传播神经网络极简入门</span><span class="muted">2015-11-08</span><span class="muted">评论(25)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/k-nearest-neighbor-method.html"><span class="thumbnail"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645jw1eoxj45stqqg20m80godki.gif" class="thumb" alt="k近邻法" title="k近邻法"></span><span class="text">k近邻法</span><span class="muted">2015-02-06</span><span class="muted">评论(12)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/naive-bayesian-method.html"><span class="thumbnail"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645jw1eozn0y3spcj20mn0h3q3a.jpg" class="thumb" alt="朴素贝叶斯法" title="朴素贝叶斯法"></span><span class="text">朴素贝叶斯法</span><span class="muted">2015-02-09</span><span class="muted">评论(11)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/em-algorithm-and-its-generalization.html"><span class="thumbnail"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1f4b95mzejvj20p60t0qab.jpg" class="thumb" alt="EM算法及其推广" title="EM算法及其推广"></span><span class="text">EM算法及其推广</span><span class="muted">2016-05-30</span><span class="muted">评论(8)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/understanding-the-convolution-in-deep-learning.html"><span class="thumbnail"><img src="./反向传播神经网络极简入门-码农场_files/006Fmjmcly1fdwjpji6qtj30dw05d0t8.jpg" class="thumb" alt="理解深度学习中的卷积" title="理解深度学习中的卷积"></span><span class="text">理解深度学习中的卷积</span><span class="muted">2017-03-24</span><span class="muted">评论(8)</span></a></li>
</ul></div><div class="widget widget_ui_posts affix" style="top: 622px;"><h3>最新文章</h3><ul>		<li><a target="_blank" href="http://www.hankcs.com/ml/compile-and-install-tensorflow-from-source.html"><span class="thumbnail"><img src="./反向传播神经网络极简入门-码农场_files/6cbb8645gw1f9ty3e8d82j20ju0g6jsn.jpg" class="thumb" alt="从源码编译安装TensorFlow" title="从源码编译安装TensorFlow"></span><span class="text">从源码编译安装TensorFlow</span><span class="muted">2017-06-26</span><span class="muted">评论(0)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/hinton-recent-applications-of-deep-neural-nets.html"><span class="thumbnail"><img src="./反向传播神经网络极简入门-码农场_files/006Fmjmcly1fga3c5iit4j30n40hg0tw.jpg" class="thumb" alt="Hinton神经网络公开课16 Recent applications of deep neural nets" title="Hinton神经网络公开课16 Recent applications of deep neural nets"></span><span class="text">Hinton神经网络公开课16 Recent applications of deep neural nets</span><span class="muted">2017-06-05</span><span class="muted">评论(0)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/hinton-modeling-hierarchical-structure-with-neural-nets.html"><span class="thumbnail"><img src="./反向传播神经网络极简入门-码农场_files/006Fmjmcly1fg9g1g542jj30z60n07t8.jpg" class="thumb" alt="Hinton神经网络公开课15 Modeling hierarchical structure with neural nets" title="Hinton神经网络公开课15 Modeling hierarchical structure with neural nets"></span><span class="text">Hinton神经网络公开课15 Modeling hierarchical structure with neural nets</span><span class="muted">2017-06-04</span><span class="muted">评论(0)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/nnml-rbm.html"><span class="thumbnail"><img src="./反向传播神经网络极简入门-码农场_files/006Fmjmcly1fg8ao6kthsj31kw11x48q.jpg" class="thumb" alt="Hinton神经网络公开课编程练习4 Restricted Boltzmann Machines" title="Hinton神经网络公开课编程练习4 Restricted Boltzmann Machines"></span><span class="text">Hinton神经网络公开课编程练习4 Restricted Boltzmann Machines</span><span class="muted">2017-06-03</span><span class="muted">评论(0)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/hinton-deep-neural-nets-with-generative-pre-training.html"><span class="thumbnail"><img src="./反向传播神经网络极简入门-码农场_files/006Fmjmcly1fg61rho89qj30p00e6gsm.jpg" class="thumb" alt="Hinton神经网络公开课14 Deep neural nets with generative pre-training" title="Hinton神经网络公开课14 Deep neural nets with generative pre-training"></span><span class="text">Hinton神经网络公开课14 Deep neural nets with generative pre-training</span><span class="muted">2017-06-02</span><span class="muted">评论(0)</span></a></li>
		<li><a target="_blank" href="http://www.hankcs.com/ml/hinton-stacking-rbms-to-make-deep-belief-nets.html"><span class="thumbnail"><img src="./反向传播神经网络极简入门-码农场_files/006Fmjmcly1fg4sosou7jj30zk0aqac0.jpg" class="thumb" alt="Hinton神经网络公开课13 Stacking RBMs to make Deep Belief Nets" title="Hinton神经网络公开课13 Stacking RBMs to make Deep Belief Nets"></span><span class="text">Hinton神经网络公开课13 Stacking RBMs to make Deep Belief Nets</span><span class="muted">2017-05-31</span><span class="muted">评论(0)</span></a></li>
</ul></div><div class="widget widget_text"><h3>订阅关注</h3>			<div class="textwidget"><iframe width="100%" height="400" class="share_self" frameborder="0" scrolling="no" src="./反向传播神经网络极简入门-码农场_files/index.html"></iframe></div>
		</div><div class="widget widget_ui_tags affix" style="top: 231px;"><h3>热门标签</h3><div class="d_tags"><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%8c%91%e6%88%98%e7%a8%8b%e5%ba%8f%e8%ae%be%e8%ae%a1%e7%ab%9e%e8%b5%9b%e7%ac%ac2%e7%89%88%e3%80%8b/">《挑战程序设计竞赛(第2版)》 (184)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%97%a5%e8%af%ad%e7%bb%bc%e5%90%88%e6%95%99%e7%a8%8b%e3%80%8b/">《日语综合教程》 (57)</a><a href="http://www.hankcs.com/tag/cs224n/">CS224n (36)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%96%b0%e7%bc%96%e6%97%a5%e8%af%ad%e9%98%85%e8%af%bb%e6%96%87%e9%80%89%e3%80%8b/">《新编日语阅读文选》 (34)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%99%ba%e8%83%bdweb%e7%ae%97%e6%b3%95%e3%80%8b/">《智能Web算法》 (20)</a><a href="http://www.hankcs.com/tag/neural-networks-for-machine-learning/">Neural Networks for Machine Learning (19)</a><a href="http://www.hankcs.com/tag/%e4%b8%ad%e6%96%87%e5%88%86%e8%af%8d/">中文分词 (18)</a><a href="http://www.hankcs.com/tag/wordpress/">WordPress (17)</a><a href="http://www.hankcs.com/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0/">深度学习 (16)</a><a href="http://www.hankcs.com/tag/lucene/">Lucene (15)</a><a href="http://www.hankcs.com/tag/%e7%bb%b4%e7%89%b9%e6%af%94%e7%ae%97%e6%b3%95/">维特比算法 (15)</a><a href="http://www.hankcs.com/tag/%e6%96%b0%e7%bc%96%e6%97%a5%e8%af%ad%e5%95%86%e5%8a%a1%e8%b4%b8%e6%98%93%e4%bc%9a%e8%af%9d/">新编日语商务贸易会话 (14)</a><a href="http://www.hankcs.com/tag/intellij-idea/">IntelliJ IDEA (13)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e7%bb%9f%e8%ae%a1%e5%ad%a6%e4%b9%a0%e6%96%b9%e6%b3%95%e3%80%8b/">《统计学习方法》 (12)</a><a href="http://www.hankcs.com/tag/uva/">UVa (11)</a><a href="http://www.hankcs.com/tag/drupal7%e4%b8%93%e4%b8%9a%e5%bc%80%e5%8f%91%e6%8c%87%e5%8d%97-%e7%ac%ac%e4%b8%89%e7%89%88/">Drupal7专业开发指南 第三版 (10)</a><a href="http://www.hankcs.com/tag/%e3%80%8a%e6%8c%91%e6%88%98%e7%bc%96%e7%a8%8b-%e7%a8%8b%e5%ba%8f%e8%ae%be%e8%ae%a1%e7%ab%9e%e8%b5%9b%e8%ae%ad%e7%bb%83%e6%89%8b%e5%86%8c%e3%80%8b/">《挑战编程-程序设计竞赛训练手册》 (10)</a><a href="http://www.hankcs.com/tag/hmm/">HMM (10)</a><a href="http://www.hankcs.com/tag/matlab/">matlab (9)</a><a href="http://www.hankcs.com/tag/tensorflow/">TensorFlow (9)</a><a href="http://www.hankcs.com/tag/cs229/">CS229 (8)</a><a href="http://www.hankcs.com/tag/word2vec/">word2vec (8)</a><a href="http://www.hankcs.com/tag/google-code-jam/">Google code jam (7)</a><a href="http://www.hankcs.com/tag/%e3%80%8ac%e6%a0%87%e5%87%86%e7%a8%8b%e5%ba%8f%e5%ba%93-%e8%87%aa%e4%bf%ae%e6%95%99%e7%a8%8b%e4%b8%8e%e5%8f%82%e8%80%83%e6%89%8b%e5%86%8c%e3%80%8b/">《C++标准程序库—自修教程与参考手册》 (7)</a><a href="http://www.hankcs.com/tag/crf/">CRF (7)</a><a href="http://www.hankcs.com/tag/yii/">Yii (6)</a><a href="http://www.hankcs.com/tag/cnn/">CNN (6)</a><a href="http://www.hankcs.com/tag/webrtc/">WebRTC (5)</a><a href="http://www.hankcs.com/tag/cocos2d-x/">Cocos2d-x (5)</a><a href="http://www.hankcs.com/tag/rnn/">RNN (5)</a></div></div></aside></section>

<div class="branding branding-black">
	<div class="container">
		<h2>我的开源项目</h2>
		<a target="blank" class="btn btn-lg" href="https://github.com/hankcs/HanLP">HanLP自然语言处理包</a><a target="blank" class="btn btn-lg" href="https://github.com/hankcs/AhoCorasickDoubleArrayTrie">基于DoubleArrayTrie的Aho Corasick自动机</a>	</div>
</div>
<footer class="footer">
	<div class="container">
		<div class="fcode">
					</div>
		<p>© 2017 <a href="http://www.hankcs.com/">码农场</a> &nbsp; <a href="http://www.hankcs.com/sitemap.xml">网站地图</a> &nbsp; <a href="http://www.miitbeian.gov.cn/" target="_blank">沪ICP备14002007号-1</a></p>
		<div style="display:none">
<script language="javascript" type="text/javascript" src="./反向传播神经网络极简入门-码农场_files/trace.js.下载"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-47205472-1', 'auto');
  ga('send', 'pageview');

</script>
<script language="javascript" type="text/javascript" src="./反向传播神经网络极简入门-码农场_files/15590612.js.下载"></script><a href="https://www.51.la/?15590612" target="_blank" title="51.La 网站流量统计系统"><img alt="51.La 网站流量统计系统" src="./反向传播神经网络极简入门-码农场_files/icon_2.gif" style="border:none"></a>

<noscript>&lt;a href="//www.51.la/?15590612" target="_blank"&gt;&lt;img alt="&amp;#x6211;&amp;#x8981;&amp;#x5566;&amp;#x514D;&amp;#x8D39;&amp;#x7EDF;&amp;#x8BA1;" src="//img.users.51.la/15590612.asp" style="border:none" /&gt;&lt;/a&gt;</noscript>
</div>	</div>
</footer>

<script>
window.jsui={
    www: 'http://www.hankcs.com',
    uri: 'http://www.hankcs.com/wp-content/themes/dux',
    ver: '1.3',
	roll: ["1","2","6","4"],
    ajaxpager: '500',
    url_rp: 'http://www.hankcs.com/about/'
};
</script>
<script type="text/javascript" src="./反向传播神经网络极简入门-码农场_files/bootstrap.min.js.下载"></script>
<script type="text/javascript" src="./反向传播神经网络极简入门-码农场_files/loader.js.下载"></script>
<script type="text/javascript" src="./反向传播神经网络极简入门-码农场_files/wp-embed.min.js.下载"></script>
<script type="text/javascript" src="./反向传播神经网络极简入门-码农场_files/form.js.下载"></script>

    <div class="m-mask"></div>    <div class="rollbar" style="display: block;"><ul><li><a href="javascript:(scrollTo());"><i class="fa fa-angle-up"></i></a><h6>去顶部<i></i></h6></li><li><a href="javascript:(on_click_toc_button());"><i class="fa fa-list post_open_icon"></i></a><h6 id="toc_label">打开目录</h6></li><li><a href="javascript:(scrollTo(&#39;#comments&#39;,-15));"><i class="fa fa-comments"></i></a><h6>去评论<i></i></h6></li></ul></div><ul class="m-navbar">
			<li id="menu-item-1834" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1834"><a href="http://www.hankcs.com/program/cpp/">C++</a></li>
<li id="menu-item-1835" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1835"><a href="http://www.hankcs.com/program/java/">Java</a></li>
<li id="menu-item-5754" class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent menu-item-5754"><a href="http://www.hankcs.com/ml/">机器学习</a></li>
<li id="menu-item-2954" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-2954"><a href="http://www.hankcs.com/nlp/">NLP</a>
<ul class="sub-menu">
	<li id="menu-item-4344" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4344"><a href="http://www.hankcs.com/nlp/corpus/">语料库</a></li>
	<li id="menu-item-4342" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4342"><a href="http://www.hankcs.com/nlp/segment/">中文分词</a></li>
	<li id="menu-item-4343" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4343"><a href="http://www.hankcs.com/nlp/ner/">命名实体识别</a></li>
	<li id="menu-item-4479" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-4479"><a href="http://www.hankcs.com/nlp/parsing/">句法分析</a></li>
</ul>
</li>
<li id="menu-item-1837" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1837"><a href="http://www.hankcs.com/program/algorithm/">算法</a></li>
<li id="menu-item-1839" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1839"><a href="http://www.hankcs.com/software/">软件</a></li>
<li id="menu-item-1838" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1838"><a href="http://www.hankcs.com/nihongonote/">日语</a>
<ul class="sub-menu">
	<li id="menu-item-1860" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1860"><a href="http://www.hankcs.com/nihongonote/riyurimen/">日语入门</a></li>
	<li id="menu-item-1861" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1861"><a href="http://www.hankcs.com/nihongonote/listening/">日语听力</a></li>
	<li id="menu-item-1863" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1863"><a href="http://www.hankcs.com/nihongonote/tekusuto/">日语综合教程</a>
	<ul class="sub-menu">
		<li id="menu-item-2190" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2190"><a href="http://www.hankcs.com/nihongonote/tekusuto/disance/">第三册</a></li>
		<li id="menu-item-2192" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2192"><a href="http://www.hankcs.com/nihongonote/tekusuto/daiyonnsatu/">第四册</a></li>
		<li id="menu-item-2191" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2191"><a href="http://www.hankcs.com/nihongonote/tekusuto/5/">第五册</a></li>
		<li id="menu-item-2702" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2702"><a href="http://www.hankcs.com/nihongonote/tekusuto/%e7%ac%ac%e5%85%ad%e5%86%8c/">第六册</a></li>
		<li id="menu-item-3604" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3604"><a href="http://www.hankcs.com/nihongonote/tekusuto/%e7%ac%ac%e4%b8%83%e5%86%8c/">第七册</a></li>
	</ul>
</li>
	<li id="menu-item-1859" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-1859"><a href="http://www.hankcs.com/nihongonote/fd2/">新编日语阅读文选</a>
	<ul class="sub-menu">
		<li id="menu-item-2187" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2187"><a href="http://www.hankcs.com/nihongonote/fd2/c1/">第一册</a></li>
		<li id="menu-item-2189" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2189"><a href="http://www.hankcs.com/nihongonote/fd2/c2/">第二册</a></li>
		<li id="menu-item-2188" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-2188"><a href="http://www.hankcs.com/nihongonote/fd2/c3/">第三册</a></li>
	</ul>
</li>
	<li id="menu-item-1858" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1858"><a href="http://www.hankcs.com/nihongonote/jpkaiwa/">日语商务贸易会话</a></li>
</ul>
</li>
<li id="menu-item-1843" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1843"><a href="http://www.hankcs.com/about/">关于</a></li>
							<li class="navto-search"><a href="javascript:;" class="search-show active"><i class="fa fa-search"></i></a></li>
					</ul>			<div class="sign">			    <div class="sign-mask"></div>			    <div class="container">			        <a href="http://www.hankcs.com/ml/back-propagation-neural-network.html#" class="close-link signclose-loader"><i class="fa fa-close"></i></a>			        <div class="sign-tips"></div>			        <form id="sign-in">  			            <h3><small class="signup-loader">切换注册</small>登录</h3>			            <h6>			                <label for="inputEmail">用户名或邮箱</label>			                <input type="text" name="username" class="form-control" id="inputEmail" placeholder="用户名或邮箱">			            </h6>			            <h6>			                <label for="inputPassword">密码</label>			                <input type="password" name="password" class="form-control" id="inputPassword" placeholder="登录密码">			            </h6>			            <div class="sign-submit">			                <input type="button" class="btn btn-primary signsubmit-loader" name="submit" value="登录">  			                <input type="hidden" name="action" value="signin">			                <label><input type="checkbox" checked="checked" name="remember" value="forever">记住我</label>			            </div><div class="sign-info"><a href="http://www.hankcs.com/about/">找回密码？</a></div></form>			        <form id="sign-up"> 			            <h3><small class="signin-loader">切换登录</small>注册</h3>			            <h6>			                <label for="inputName">昵称</label>			                <input type="text" name="name" class="form-control" id="inputName" placeholder="设置昵称">			            </h6>			            <h6>			                <label for="inputEmail">邮箱</label>			                <input type="email" name="email" class="form-control" id="inputEmail" placeholder="邮箱">			            </h6>			            <div class="sign-submit">			                <input type="button" class="btn btn-primary btn-block signsubmit-loader" name="submit" value="快速注册">  			                <input type="hidden" name="action" value="signup">  			            </div>			        </form>			    </div>			</div>		</body></html>